{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import json\n",
    "from typing import List, Dict, Tuple, Set, Any\n",
    "\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Constants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "### Constants                                      ###\n",
    "######################################################\n",
    "# Base Paths\n",
    "INPUT_PATH = \"./data\"\n",
    "MODEL_PATH = \"./model\"\n",
    "OUTPUT_PATH = \"./output\"\n",
    "\n",
    "# Model File names\n",
    "VANILLA_MODEL_FILENAME = \"vanillamodel.txt\"\n",
    "AVERAGED_MODEL_FILENAME = \"averagedmodel.txt\"\n",
    "OUTPUT_FILENAME = \"output.txt\"\n",
    "\n",
    "# Class Identifiers\n",
    "TRUTHFUL = \"True\"\n",
    "DECEPTIVE = \"Fake\"\n",
    "POSITIVE = \"Pos\"\n",
    "NEGATIVE = \"Neg\"\n",
    "\n",
    "SENTIMENT_CLASS_DICT = {1: \"Pos\", -1: \"Neg\"}\n",
    "\n",
    "TRUTHFULNESS_CLASS_DICT = {1: \"True\", -1: \"Fake\"}\n",
    "\n",
    "# File paths\n",
    "TRAIN_FILE_PATH = f\"{INPUT_PATH}/train-labeled.txt\"\n",
    "CLEANED_DATA_FILE_PATH = f\"{INPUT_PATH}/cleaned-data.txt\"\n",
    "PREPROCESSED_DATA_FILE_PATH = f\"{INPUT_PATH}/preprocessed-data.txt\"\n",
    "\n",
    "VANILLA_MODEL_FILE_PATH = f\"{MODEL_PATH}/{VANILLA_MODEL_FILENAME}\"\n",
    "AVERAGED_MODEL_FILE_PATH = f\"{MODEL_PATH}/{AVERAGED_MODEL_FILENAME}\"\n",
    "\n",
    "OUTPUT_FILE_PATH = f\"{OUTPUT_PATH}/{OUTPUT_FILENAME}\"\n",
    "\n",
    "DEV_DATA_FILE_PATH = f\"{INPUT_PATH}/dev-text.txt\"\n",
    "DEV_KEY_FILE_PATH = f\"{INPUT_PATH}/dev-key.txt\"\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "DATA_COL = 3\n",
    "SENTIMENT_TARGET_COL = 2\n",
    "TRUTHFULNESS_TARGET_COL = 1\n",
    "TEST_SIZE = 0.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(seed=RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = \"07Zfn0z Fake Pos If you're looking for an elegant hotel in downtown Chicago, you have to stay here. The Ambassador East Hotel has very comfortable and beautiful large rooms and is like a home away from home. The perfect place for a business person, and if you have a small pet you can bring them too! I would give this place four stars and would definitely stay here again.\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_regex = re.compile(\"(\\w*) (\\w*) (\\w*) (.*)\\n?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('07Zfn0z',\n",
       " 'Fake',\n",
       " 'Pos',\n",
       " \"If you're looking for an elegant hotel in downtown Chicago, you have to stay here. The Ambassador East Hotel has very comfortable and beautiful large rooms and is like a home away from home. The perfect place for a business person, and if you have a small pet you can bring them too! I would give this place four stars and would definitely stay here again.\")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.match(input_regex, line).groups()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('07Zfn0z',\n",
       "  'Fake',\n",
       "  'Pos',\n",
       "  \"If you're looking for an elegant hotel in downtown Chicago, you have to stay here. The Ambassador East Hotel has very comfortable and beautiful large rooms and is like a home away from home. The perfect place for a business person, and if you have a small pet you can bring them too! I would give this place four stars and would definitely stay here again.\")]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "data.append(re.match(input_regex, line).groups())\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Fake'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(data)[0, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "def load_data(input_file_path: str, type: str = \"TRAIN\") -> npt.NDArray:\n",
    "    input_data = list()\n",
    "    regex = \"(\\w*) (\\w*) (\\w*) (.*)\\n?\"\n",
    "    if type == \"DEV\":\n",
    "        regex = \"(\\w*) (.*)\\n?\"\n",
    "    elif type == \"KEY\":\n",
    "        regex = \"(\\w*) (\\w*) (\\w*)\\n?\"\n",
    "    input_regex = re.compile(regex)\n",
    "    with open(input_file_path, mode=\"r\") as input_file:\n",
    "        for line in input_file:\n",
    "            input_data.append(re.match(input_regex, line).groups())\n",
    "    return np.array(input_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store Data\n",
    "def store_data(date_file_path: str, data: npt.NDArray) -> None:\n",
    "    with open(date_file_path, mode=\"w\") as data_file:\n",
    "        for row in data:\n",
    "            data_file.write(f\"{row[0]} {row[1]} {row[2]} {row[3]}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store Model\n",
    "def store_model(model_file_path: str, model_data: Any) -> None:\n",
    "    with open(model_file_path, mode=\"w\") as model_file:\n",
    "        json.dump(model_data, model_file, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model\n",
    "def load_model(model_file_path: str) -> npt.NDArray:\n",
    "    with open(model_file_path, mode=\"r\") as model_file:\n",
    "        model_data = json.load(model_file)\n",
    "\n",
    "    return (model_data[\"tf_idf_model\"], model_data[\"sentiment_classifier\"], model_data[\"truthfulness_classifier\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store Predictions\n",
    "def store_predictions(output_file_path: str, predictions: List[Tuple[str, str, str]]) -> None:\n",
    "    with open(output_file_path, mode=\"w\") as output_file:\n",
    "        for prediction in predictions:\n",
    "            output_file.write(f\"{prediction[0]} {prediction[1]} {prediction[2]}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy_score(y_true: npt.NDArray, y_pred: npt.NDArray):\n",
    "    return (y_true == y_pred).sum() / y_true.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_f1_score(y_true: npt.NDArray, y_pred: npt.NDArray, type: str = \"macro\"):\n",
    "    def calculate_f1(y_true, y_pred, label):\n",
    "        tp = np.sum((y_true == label) & (y_pred == label))\n",
    "        fp = np.sum((y_true != label) & (y_pred == label))\n",
    "        fn = np.sum((y_pred != label) & (y_true == label))\n",
    "\n",
    "        precision = tp / (tp + fp)\n",
    "        recall = tp / (tp + fn)\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "        return f1\n",
    "\n",
    "    def macro_f1(y_true, y_pred):\n",
    "        return np.mean([calculate_f1(y_true, y_pred, label) for label in np.unique(y_true)])\n",
    "\n",
    "    def micro_f1(y_true, y_pred):\n",
    "        return {label: calculate_f1(y_true, y_pred, label) for label in np.unique(y_true)}\n",
    "\n",
    "    if type == \"macro\":\n",
    "        return macro_f1(y_true, y_pred)\n",
    "    elif type == \"micro\":\n",
    "        return micro_f1(y_true, y_pred)\n",
    "    else:\n",
    "        return {\"micro\": micro_f1(y_true, y_pred), \"macro\": macro_f1(y_true, y_pred)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Scores\n",
    "def calculate_scores(y_true, y_pred, title: str):\n",
    "    from sklearn.metrics import classification_report\n",
    "\n",
    "    print(f\"------------------------ {title} ------------------------\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(\"---------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data(TRAIN_FILE_PATH, type=\"TRAIN\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all reviews to lower case (optional according to study)\n",
    "def to_lower(data: npt.NDArray):\n",
    "    n_data = data.shape[0]\n",
    "    result = data.copy()\n",
    "    for i in range(n_data):\n",
    "        result[i] = result[i].lower()\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_encodings(data: npt.NDArray):\n",
    "    import re\n",
    "\n",
    "    n_data = data.shape[0]\n",
    "    result = data.copy()\n",
    "    for i in range(n_data):\n",
    "        result[i] = re.sub(r\"&#\\d+;\", \" \", result[i])\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_tags(data: npt.NDArray):\n",
    "    import re\n",
    "\n",
    "    n_data = data.shape[0]\n",
    "    result = data.copy()\n",
    "    for i in range(n_data):\n",
    "        result[i] = re.sub(r\"<[a-zA-Z]+\\s?/?>\", \"\", result[i])\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(data: npt.NDArray):\n",
    "    import re\n",
    "\n",
    "    n_data = data.shape[0]\n",
    "    result = data.copy()\n",
    "    for i in range(n_data):\n",
    "        result[i] = re.sub(r\"https?://([\\w\\-\\._]+){2,}/[\\w\\-\\.\\-/=\\+_\\?]+\", \"\", result[i])\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_and_url(data):\n",
    "    \"\"\"Function to remove\n",
    "             1. HTML encodings\n",
    "             2. HTML tags (both closed and open)\n",
    "             3. URLs\n",
    "\n",
    "    Args:\n",
    "        data (npt.NDArray): A Numpy Array of type string\n",
    "\n",
    "    Returns:\n",
    "        _type_: npt.NDArray\n",
    "    \"\"\"\n",
    "    import re\n",
    "\n",
    "    n_data = data.shape[0]\n",
    "    result = data.copy()\n",
    "    for i in range(n_data):\n",
    "        # Remove HTML encodings\n",
    "        result[i] = re.sub(r\"&#\\d+;\", \"\", result[i])\n",
    "\n",
    "        # Remove HTML tags (both open and closed)\n",
    "        result[i] = re.sub(r\"<[a-zA-Z]+\\s?/?>\", \"\", result[i])\n",
    "\n",
    "        # Remove URLs\n",
    "        result[i] = re.sub(r\"https?://([\\w\\-\\._]+){2,}/[\\w\\-\\.\\-/=\\+_\\?]+\", \"\", result[i])\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_digits_with_tag(data: npt.NDArray):\n",
    "    import re\n",
    "\n",
    "    n_data = data.shape[0]\n",
    "    result = data.copy()\n",
    "    for i in range(n_data):\n",
    "        result[i] = re.sub(r\"\\d+\", \" NUM \", result[i])\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove non-alphabetical characters\n",
    "def remove_non_alpha_characters(data: npt.NDArray):\n",
    "    import re\n",
    "\n",
    "    n_data = data.shape[0]\n",
    "    result = data.copy()\n",
    "    for i in range(n_data):\n",
    "        result[i] = re.sub(r\"_+|\\\\|[^a-zA-Z0-9\\s]\", \" \", result[i])\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove extra spaces\n",
    "def remove_extra_spaces(data: npt.NDArray):\n",
    "    import re\n",
    "\n",
    "    n_data = data.shape[0]\n",
    "    result = data.copy()\n",
    "    for i in range(n_data):\n",
    "        result[i] = re.sub(r\"^\\s*|\\s\\s*\", \" \", result[i])\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expanding contractions\n",
    "def fix_contractions(data: npt.NDArray):\n",
    "    from contractions import fix\n",
    "\n",
    "    def contraction_fixer(txt: str):\n",
    "        return \" \".join([fix(word) for word in txt.split()])\n",
    "\n",
    "    n_data = data.shape[0]\n",
    "    result = data.copy()\n",
    "    for i in range(n_data):\n",
    "        result[i] = contraction_fixer(result[i])\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting: to_lower\n",
      "Ended: to_lower\n",
      "Starting: remove_html_encodings\n",
      "Ended: remove_html_encodings\n",
      "Starting: remove_html_tags\n",
      "Ended: remove_html_tags\n",
      "Starting: remove_url\n",
      "Ended: remove_url\n",
      "Starting: fix_contractions\n",
      "Ended: fix_contractions\n",
      "Starting: remove_non_alpha_characters\n",
      "Ended: remove_non_alpha_characters\n",
      "Starting: remove_extra_spaces\n",
      "Ended: remove_extra_spaces\n"
     ]
    }
   ],
   "source": [
    "# A dictionary containing the columns and a list of functions to perform on it in order\n",
    "data_cleaning_pipeline = {\n",
    "    DATA_COL: [\n",
    "        to_lower,\n",
    "        remove_html_encodings,\n",
    "        remove_html_tags,\n",
    "        remove_url,\n",
    "        fix_contractions,\n",
    "        remove_non_alpha_characters,\n",
    "        remove_extra_spaces,\n",
    "    ]\n",
    "}\n",
    "\n",
    "cleaned_data = data.copy()\n",
    "\n",
    "# Process all the cleaning instructions\n",
    "for col, pipeline in data_cleaning_pipeline.items():\n",
    "    # Get the column to perform cleaning on\n",
    "    temp_data = cleaned_data[:, col].copy()\n",
    "\n",
    "    # Perform all the cleaning functions sequencially\n",
    "    for func in pipeline:\n",
    "        print(f\"Starting: {func.__name__}\")\n",
    "        temp_data = func(temp_data)\n",
    "        print(f\"Ended: {func.__name__}\")\n",
    "\n",
    "    # Replace the old column with cleaned one.\n",
    "    cleaned_data[:, col] = temp_data.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_data(CLEANED_DATA_FILE_PATH, cleaned_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not Applicable since everything has to be implemented from scratch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfIdf:\n",
    "    def __init__(self) -> None:\n",
    "        self.n_docs: int = None\n",
    "        self.vocab: List = list()\n",
    "        self.vocab_size: int = None\n",
    "        self.vocab_index: Dict[str, int] = dict()\n",
    "        self.word_document_count: Dict[str, int] = dict()\n",
    "\n",
    "    def __create_vocab__(self, documents: npt.NDArray) -> Set:\n",
    "        vocab = set()\n",
    "\n",
    "        for document in documents:\n",
    "            for word in document:\n",
    "                vocab.add(word)\n",
    "\n",
    "        return list(vocab)\n",
    "\n",
    "    def __get_word_document_count__(self, documents: npt.NDArray):\n",
    "        word_document_count = dict()\n",
    "\n",
    "        for document in documents:\n",
    "            for word in document:\n",
    "                if word in self.vocab:\n",
    "                    if word not in word_document_count:\n",
    "                        word_document_count[word] = 1\n",
    "                    else:\n",
    "                        word_document_count[word] += 1\n",
    "\n",
    "        return word_document_count\n",
    "\n",
    "    def __term_frequency__(self, word: str, document: npt.NDArray):\n",
    "        word_occurences = (document == word).sum()\n",
    "        return word_occurences / self.n_docs\n",
    "\n",
    "    def __inverse_document_frequency__(self, word: str):\n",
    "        word_occurrences = 1\n",
    "\n",
    "        if word in self.word_document_count:\n",
    "            word_occurrences += self.word_document_count[word]\n",
    "\n",
    "        return np.log(self.n_docs / word_occurrences)\n",
    "\n",
    "    def __tf_idf__(self, document: npt.NDArray):\n",
    "        tf_idf_vector = np.zeros(shape=(self.vocab_size,))\n",
    "        for word in document:\n",
    "            # ignore word not in vocab\n",
    "            if word in self.vocab:\n",
    "                tf = self.__term_frequency__(word, document)\n",
    "                idf = self.__inverse_document_frequency__(word)\n",
    "\n",
    "                tf_idf_vector[self.vocab_index[word]] = tf * idf\n",
    "        return tf_idf_vector\n",
    "\n",
    "    def fit(self, documents: npt.NDArray):\n",
    "        self.n_docs = documents.shape[0]\n",
    "        self.vocab = self.__create_vocab__(documents)\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.vocab_index = {word: idx for idx, word in enumerate(self.vocab)}\n",
    "        self.word_document_count = self.__get_word_document_count__(documents)\n",
    "\n",
    "    def transform(self, documents: npt.NDArray):\n",
    "        tf_idf_vectors = list()\n",
    "        for document in documents:\n",
    "            tf_idf_vectors.append(self.__tf_idf__(document))\n",
    "        return np.array(tf_idf_vectors)\n",
    "\n",
    "    def export(self):\n",
    "        return {\n",
    "            \"n_docs\": self.n_docs,\n",
    "            \"vocab_size\": self.vocab_size,\n",
    "            \"vocab\": self.vocab,\n",
    "            \"vocab_size\": self.vocab_size,\n",
    "            \"vocab_index\": self.vocab_index,\n",
    "            \"word_document_count\": self.word_document_count,\n",
    "        }\n",
    "\n",
    "    def load(self, tf_idf_model_data):\n",
    "        self.n_docs = tf_idf_model_data[\"n_docs\"]\n",
    "        self.vocab_size = tf_idf_model_data[\"vocab_size\"]\n",
    "        self.vocab = tf_idf_model_data[\"vocab\"]\n",
    "        self.vocab_size = tf_idf_model_data[\"vocab_size\"]\n",
    "        self.vocab_index = tf_idf_model_data[\"vocab_index\"]\n",
    "        self.word_document_count = tf_idf_model_data[\"word_document_count\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = load_data(CLEANED_DATA_FILE_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(data: npt.NDArray):\n",
    "    tokenized_documents = list()\n",
    "    for document in data:\n",
    "        tokenized_documents.append(np.array(document.split()))\n",
    "    return np.array(tokenized_documents, dtype=object)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['if', 'you', 'are', 'looking', 'for', 'an', 'elegant', 'hotel',\n",
       "       'in', 'downtown', 'chicago', 'you', 'have', 'to', 'stay', 'here',\n",
       "       'the', 'ambassador', 'east', 'hotel', 'has', 'very', 'comfortable',\n",
       "       'and', 'beautiful', 'large', 'rooms', 'and', 'is', 'like', 'a',\n",
       "       'home', 'away', 'from', 'home', 'the', 'perfect', 'place', 'for',\n",
       "       'a', 'business', 'person', 'and', 'if', 'you', 'have', 'a',\n",
       "       'small', 'pet', 'you', 'can', 'bring', 'them', 'too', 'i', 'would',\n",
       "       'give', 'this', 'place', 'four', 'stars', 'and', 'would',\n",
       "       'definitely', 'stay', 'here', 'again'], dtype='<U11')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_documents = tokenize(final_data[:, DATA_COL])\n",
    "tokenized_documents[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Invoke TF-IDF\n",
    "tf_idf_model = TfIdf()\n",
    "tf_idf_model.fit(tokenized_documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(960, 7675)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_vectors = tf_idf_model.transform(tokenized_documents)\n",
    "tf_idf_vectors.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_model_data = tf_idf_model.export()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_labels = np.where(final_data[:, SENTIMENT_TARGET_COL] == POSITIVE, 1, -1)\n",
    "truthfulness_labels = np.where(final_data[:, TRUTHFULNESS_TARGET_COL] == TRUTHFUL, 1, -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(\n",
    "    data: npt.NDArray, sentiment_labels: npt.NDArray, truthfulness_labels: npt.NDArray, test_size: float = 0.2\n",
    "):\n",
    "    n_max = data.shape[0]\n",
    "    sample = int((1 - test_size) * n_max)\n",
    "\n",
    "    # Shuffle the data\n",
    "    all_idx = np.random.permutation(n_max)\n",
    "    train_idx, test_idx = all_idx[:sample], all_idx[sample:]\n",
    "\n",
    "    X_train, X_test, y_train_sentiment, y_train_truthfulness, y_test_sentiment, y_test_truthfulness = (\n",
    "        data[train_idx],\n",
    "        data[test_idx],\n",
    "        sentiment_labels[train_idx],\n",
    "        truthfulness_labels[train_idx],\n",
    "        sentiment_labels[test_idx],\n",
    "        truthfulness_labels[test_idx],\n",
    "    )\n",
    "\n",
    "    return X_train, X_test, y_train_sentiment, y_train_truthfulness, y_test_sentiment, y_test_truthfulness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train_sentiment, y_train_truthfulness, y_test_sentiment, y_test_truthfulness = train_test_split(\n",
    "    tf_idf_vectors, sentiment_labels, truthfulness_labels, TEST_SIZE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla Perceptron\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaPerceptron:\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        X: npt.NDArray,\n",
    "        y: npt.NDArray,\n",
    "        max_iterations: int,\n",
    "        learning_rate: float = 1e-2,\n",
    "        tolerance: float = 1e-2,\n",
    "        shuffle: bool = True,\n",
    "        class_weights: dict = None,\n",
    "    ):\n",
    "        n_epoch = 0\n",
    "\n",
    "        self.weights: npt.NDArray = np.zeros(shape=(X.shape[-1],))\n",
    "        self.bias: float = 0\n",
    "\n",
    "        for epoch in range(max_iterations):\n",
    "            n_epoch = epoch\n",
    "\n",
    "            if shuffle:\n",
    "                idxs = np.random.permutation(X.shape[0])\n",
    "                X = X[idxs]\n",
    "                y = y[idxs]\n",
    "\n",
    "            for x, y_true in zip(X, y):\n",
    "\n",
    "                a = np.dot(self.weights, x) + self.bias\n",
    "                if y_true * a <= 0:\n",
    "                    if class_weights is None:\n",
    "                        self.weights = self.weights + y_true * x\n",
    "                    else:\n",
    "                        self.weights = self.weights + y_true * x * class_weights[y_true]\n",
    "                    self.bias = self.bias + y_true\n",
    "\n",
    "    def predict(self, X: npt.NDArray):\n",
    "        predictions = list()\n",
    "        for x in X:\n",
    "            pred = np.sign(np.dot(self.weights, x) + self.bias)\n",
    "            predictions.append(pred)\n",
    "        return np.array(predictions)\n",
    "\n",
    "    def export(\n",
    "        self,\n",
    "    ):\n",
    "        return {\"weights\": self.weights.tolist(), \"bias\": self.bias}\n",
    "\n",
    "    def load(self, perceptron_data: Dict[str, Any]):\n",
    "        self.weights = np.array(perceptron_data[\"weights\"])\n",
    "        self.bias = perceptron_data[\"bias\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 100  Train:  0.7195467422096318  Test:  0.7279411764705882\n",
      "Epoch # 150  Train:  0.6455026455026455  Test:  0.557142857142857\n",
      "Epoch # 200  Train:  0.7094972067039106  Test:  0.7148014440433214\n",
      "Epoch # 250  Train:  0.7175141242937852  Test:  0.7252747252747253\n",
      "Epoch # 300  Train:  0.8751835535976504  Test:  0.7261146496815287\n",
      "Epoch # 350  Train:  0.936111111111111  Test:  0.7341772151898734\n",
      "Epoch # 400  Train:  0.9578231292517007  Test:  0.7804878048780488\n",
      "Epoch # 450  Train:  0.962059620596206  Test:  0.7654320987654321\n",
      "Epoch # 500  Train:  0.971736204576043  Test:  0.7577639751552795\n",
      "Epoch # 550  Train:  0.9758713136729223  Test:  0.7654320987654321\n",
      "Epoch # 600  Train:  0.9359331476323121  Test:  0.7577639751552795\n",
      "Epoch # 650  Train:  0.9853917662682602  Test:  0.7499999999999999\n",
      "Epoch # 700  Train:  0.9894179894179893  Test:  0.7499999999999999\n",
      "Epoch # 750  Train:  0.9894179894179893  Test:  0.7499999999999999\n",
      "Epoch # 800  Train:  0.9907529722589168  Test:  0.7499999999999999\n",
      "Epoch # 850  Train:  0.9947368421052631  Test:  0.7577639751552795\n",
      "Epoch # 900  Train:  0.9947368421052631  Test:  0.7577639751552795\n",
      "Epoch # 950  Train:  0.9973821989528796  Test:  0.7804878048780488\n",
      "Epoch # 1000  Train:  0.998689384010485  Test:  0.7878787878787878\n"
     ]
    }
   ],
   "source": [
    "epochs = [100, 150, 200, 250, 300, 350, 400, 450, 500, 550, 600, 650, 700, 750, 800, 850, 900, 950, 1000]\n",
    "\n",
    "for epoch in epochs:\n",
    "    vanilla_perceptron_sentiment = VanillaPerceptron()\n",
    "    vanilla_perceptron_sentiment.fit(\n",
    "        X_train, y_train_sentiment, max_iterations=epoch, learning_rate=1e-2, tolerance=1e-8, shuffle=True\n",
    "    )\n",
    "\n",
    "    y_pred_train_sentiment = vanilla_perceptron_sentiment.predict(X_train)\n",
    "    y_pred_sentiment = vanilla_perceptron_sentiment.predict(X_test)\n",
    "    \n",
    "    print(\"Epoch #\", epoch, \" Train: \", f1_score(y_train_sentiment, y_pred_train_sentiment), \" Test: \", f1_score(y_test_sentiment, y_pred_sentiment))\n",
    "\n",
    "    del vanilla_perceptron_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_raw_data = load_data(DEV_DATA_FILE_PATH, type=\"DEV\")\n",
    "dev_key_data = load_data(DEV_KEY_FILE_PATH, type=\"KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting: to_lower\n",
      "Ended: to_lower\n",
      "Starting: remove_html_encodings\n",
      "Ended: remove_html_encodings\n",
      "Starting: remove_html_tags\n",
      "Ended: remove_html_tags\n",
      "Starting: remove_url\n",
      "Ended: remove_url\n",
      "Starting: fix_contractions\n",
      "Ended: fix_contractions\n",
      "Starting: remove_non_alpha_characters\n",
      "Ended: remove_non_alpha_characters\n",
      "Starting: remove_extra_spaces\n",
      "Ended: remove_extra_spaces\n"
     ]
    }
   ],
   "source": [
    "# A dictionary containing the columns and a list of functions to perform on it in order\n",
    "data_cleaning_pipeline = {\n",
    "    1: [\n",
    "        to_lower,\n",
    "        remove_html_encodings,\n",
    "        remove_html_tags,\n",
    "        remove_url,\n",
    "        fix_contractions,\n",
    "        remove_non_alpha_characters,\n",
    "        remove_extra_spaces,\n",
    "    ]\n",
    "}\n",
    "\n",
    "dev_cleaned_data = dev_raw_data.copy()\n",
    "\n",
    "# Process all the cleaning instructions\n",
    "for col, pipeline in data_cleaning_pipeline.items():\n",
    "    # Get the column to perform cleaning on\n",
    "    temp_data = dev_cleaned_data[:, col].copy()\n",
    "\n",
    "    # Perform all the cleaning functions sequencially\n",
    "    for func in pipeline:\n",
    "        print(f\"Starting: {func.__name__}\")\n",
    "        temp_data = func(temp_data)\n",
    "        print(f\"Ended: {func.__name__}\")\n",
    "\n",
    "    # Replace the old column with cleaned one.\n",
    "    dev_cleaned_data[:, col] = temp_data.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dev_documents = tokenize(dev_cleaned_data[:,1])\n",
    "\n",
    "dev_tf_idf_vectors = tf_idf_model.transform(tokenized_dev_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_perceptron_sentiment = VanillaPerceptron()\n",
    "vanilla_perceptron_sentiment.fit(\n",
    "    tf_idf_vectors, sentiment_labels, max_iterations=900, learning_rate=1e-2, tolerance=1e-8, shuffle=True\n",
    ")\n",
    "\n",
    "vanilla_perceptron_sentiment_data = vanilla_perceptron_sentiment.export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_sentiment_pred = vanilla_perceptron_sentiment.predict(dev_tf_idf_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------ Train ------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Neg       1.00      0.99      1.00       480\n",
      "         Pos       0.99      1.00      1.00       480\n",
      "\n",
      "    accuracy                           1.00       960\n",
      "   macro avg       1.00      1.00      1.00       960\n",
      "weighted avg       1.00      1.00      1.00       960\n",
      "\n",
      "---------------------------------------------------------\n",
      "------------------------ Dev ------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Neg       0.78      0.99      0.87       160\n",
      "         Pos       0.99      0.72      0.83       160\n",
      "\n",
      "    accuracy                           0.86       320\n",
      "   macro avg       0.89      0.86      0.85       320\n",
      "weighted avg       0.89      0.86      0.85       320\n",
      "\n",
      "---------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "calculate_scores(final_data[:, SENTIMENT_TARGET_COL], np.where(vanilla_perceptron_sentiment.predict(tf_idf_vectors) == -1, \"Neg\", \"Pos\"), title=\"Train\")\n",
    "calculate_scores(dev_key_data[:, 2], np.where(dev_sentiment_pred == -1, \"Neg\", \"Pos\"), title=\"Dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_f1_score = f1_score(np.where(dev_key_data[:, 2] == \"Neg\", -1, 1), dev_sentiment_pred, pos_label=-1)\n",
    "pos_f1_score = f1_score(np.where(dev_key_data[:, 2] == \"Neg\", -1, 1), dev_sentiment_pred, pos_label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 100  Train:  0.7277936962750716  Test:  0.7279411764705882\n",
      "Epoch # 150  Train:  0.6234234234234234  Test:  0.5255474452554744\n",
      "Epoch # 200  Train:  0.7068645640074211  Test:  0.7148014440433214\n",
      "Epoch # 250  Train:  0.8651851851851852  Test:  0.7051282051282051\n",
      "Epoch # 300  Train:  0.9211267605633803  Test:  0.7577639751552795\n",
      "Epoch # 350  Train:  0.7270992366412213  Test:  0.7252747252747253\n",
      "Epoch # 400  Train:  0.9119318181818181  Test:  0.7499999999999999\n",
      "Epoch # 450  Train:  0.9432918395573997  Test:  0.7654320987654321\n",
      "Epoch # 500  Train:  0.7426900584795322  Test:  0.7279411764705882\n",
      "Epoch # 550  Train:  0.9813333333333333  Test:  0.7577639751552795\n",
      "Epoch # 600  Train:  0.9853917662682602  Test:  0.7577639751552795\n",
      "Epoch # 650  Train:  0.9867374005305041  Test:  0.7499999999999999\n",
      "Epoch # 700  Train:  0.9813333333333333  Test:  0.7499999999999999\n",
      "Epoch # 750  Train:  0.9534246575342467  Test:  0.7499999999999999\n",
      "Epoch # 800  Train:  0.992084432717678  Test:  0.7577639751552795\n",
      "Epoch # 850  Train:  0.9947368421052631  Test:  0.7577639751552795\n",
      "Epoch # 900  Train:  0.9947368421052631  Test:  0.7577639751552795\n",
      "Epoch # 950  Train:  0.9973821989528796  Test:  0.7804878048780488\n",
      "Epoch # 1000  Train:  0.9973821989528796  Test:  0.7951807228915662\n"
     ]
    }
   ],
   "source": [
    "epochs = [100, 150, 200, 250, 300, 350, 400, 450, 500, 550, 600, 650, 700, 750, 800, 850, 900, 950, 1000]\n",
    "\n",
    "for epoch in epochs:\n",
    "    vanilla_perceptron_truthfulness = VanillaPerceptron()\n",
    "    vanilla_perceptron_truthfulness.fit(\n",
    "        X_train, y_train_sentiment, max_iterations=epoch, learning_rate=1e-2, tolerance=1e-8, shuffle=True\n",
    "    )\n",
    "\n",
    "    y_pred_train_truthfulness = vanilla_perceptron_truthfulness.predict(X_train)\n",
    "    y_pred_truthfulness = vanilla_perceptron_truthfulness.predict(X_test)\n",
    "    \n",
    "    print(\"Epoch #\", epoch, \" Train: \", f1_score(y_train_sentiment, y_pred_train_truthfulness), \" Test: \", f1_score(y_test_sentiment, y_pred_truthfulness))\n",
    "\n",
    "    del vanilla_perceptron_truthfulness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_perceptron_truthfulness = VanillaPerceptron()\n",
    "vanilla_perceptron_truthfulness.fit(\n",
    "    tf_idf_vectors, truthfulness_labels, max_iterations=800, learning_rate=1e-2, tolerance=1e-8, shuffle=True, class_weights={\n",
    "        -1: 1.0275, 1: 1.0\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_truthfulness_pred = vanilla_perceptron_truthfulness.predict(dev_tf_idf_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------ Train ------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fake       1.00      1.00      1.00       480\n",
      "        True       1.00      1.00      1.00       480\n",
      "\n",
      "    accuracy                           1.00       960\n",
      "   macro avg       1.00      1.00      1.00       960\n",
      "weighted avg       1.00      1.00      1.00       960\n",
      "\n",
      "---------------------------------------------------------\n",
      "------------------------ Dev ------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fake       0.98      0.29      0.45       160\n",
      "        True       0.58      0.99      0.74       160\n",
      "\n",
      "    accuracy                           0.64       320\n",
      "   macro avg       0.78      0.64      0.59       320\n",
      "weighted avg       0.78      0.64      0.59       320\n",
      "\n",
      "---------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "calculate_scores(final_data[:, TRUTHFULNESS_TARGET_COL], np.where(vanilla_perceptron_truthfulness.predict(tf_idf_vectors) == -1, \"Fake\", \"True\"), title=\"Train\")\n",
    "calculate_scores(dev_key_data[:, 1], np.where(dev_truthfulness_pred == -1, \"Fake\", \"True\"), title=\"Dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_f1_score = f1_score(np.where(dev_key_data[:, 1] == \"Fake\", -1, 1), dev_truthfulness_pred, pos_label=-1)\n",
    "truth_f1_score = f1_score(np.where(dev_key_data[:, 1] == \"Fake\", -1, 1), dev_truthfulness_pred, pos_label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8736263736263736,\n",
       " 0.8333333333333334,\n",
       " 0.4519230769230769,\n",
       " 0.7361111111111112)"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_f1_score, pos_f1_score, fake_f1_score, truth_f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7237484737484738"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([neg_f1_score, pos_f1_score, fake_f1_score, truth_f1_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classify\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_sentiment = vanilla_perceptron_truthfulness.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------ Train ------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Neg       1.00      0.99      1.00       387\n",
      "         Pos       0.99      1.00      1.00       381\n",
      "\n",
      "    accuracy                           1.00       768\n",
      "   macro avg       1.00      1.00      1.00       768\n",
      "weighted avg       1.00      1.00      1.00       768\n",
      "\n",
      "---------------------------------------------------------\n",
      "------------------------ Test ------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Neg       0.76      0.99      0.86        93\n",
      "         Pos       0.99      0.71      0.82        99\n",
      "\n",
      "    accuracy                           0.84       192\n",
      "   macro avg       0.87      0.85      0.84       192\n",
      "weighted avg       0.88      0.84      0.84       192\n",
      "\n",
      "---------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "calculate_scores(\n",
    "    np.where(y_train_sentiment == -1, \"Neg\", \"Pos\"),\n",
    "    np.where(vanilla_perceptron_truthfulness.predict(X_train) == -1, \"Neg\", \"Pos\"),\n",
    "    title=\"Train\",\n",
    ")\n",
    "calculate_scores(\n",
    "    np.where(y_test_sentiment == -1, \"Neg\", \"Pos\"), np.where(y_pred_sentiment == -1, \"Neg\", \"Pos\"), title=\"Test\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Truthfulness Classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16\n"
     ]
    }
   ],
   "source": [
    "vanilla_perceptron_truthfulness = VanillaPerceptron()\n",
    "vanilla_perceptron_truthfulness.fit(\n",
    "    X_train, y_train_truthfulness, max_iterations=1000, learning_rate=1e-12, shuffle=True\n",
    ")\n",
    "\n",
    "vanilla_perceptron_truthfulness_data = vanilla_perceptron_truthfulness.export()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classify\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_truthfulness = vanilla_perceptron_truthfulness.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------ Train ------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fake       1.00      1.00      1.00       383\n",
      "        True       1.00      1.00      1.00       385\n",
      "\n",
      "    accuracy                           1.00       768\n",
      "   macro avg       1.00      1.00      1.00       768\n",
      "weighted avg       1.00      1.00      1.00       768\n",
      "\n",
      "---------------------------------------------------------\n",
      "------------------------ Test ------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fake       0.98      0.43      0.60        97\n",
      "        True       0.63      0.99      0.77        95\n",
      "\n",
      "    accuracy                           0.71       192\n",
      "   macro avg       0.80      0.71      0.69       192\n",
      "weighted avg       0.81      0.71      0.68       192\n",
      "\n",
      "---------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "calculate_scores(\n",
    "    np.where(y_train_truthfulness == 0, \"Fake\", \"True\"),\n",
    "    np.where(vanilla_perceptron_truthfulness.predict(X_train) == 0, \"Fake\", \"True\"),\n",
    "    title=\"Train\",\n",
    ")\n",
    "calculate_scores(\n",
    "    np.where(y_test_truthfulness == 0, \"Fake\", \"True\"),\n",
    "    np.where(y_pred_truthfulness == 0, \"Fake\", \"True\"),\n",
    "    title=\"Test\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write Vanilla Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_model_file_data = {\n",
    "    \"tf_idf_model\": tf_idf_model_data,\n",
    "    \"sentiment_classifier\": vanilla_perceptron_sentiment_data,\n",
    "    \"truthfulness_classifier\": vanilla_perceptron_truthfulness_data,\n",
    "}\n",
    "\n",
    "store_model(VANILLA_MODEL_FILE_PATH, vanilla_model_file_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Vanilla Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_model_data, vanilla_perceptron_sentiment_data, vanilla_perceptron_truthfulness_data = load_model(\n",
    "    VANILLA_MODEL_FILE_PATH\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_model = TfIdf()\n",
    "tf_idf_model.load(tf_idf_model_data)\n",
    "\n",
    "vanilla_perceptron_sentiment = VanillaPerceptron()\n",
    "vanilla_perceptron_sentiment.load(vanilla_perceptron_sentiment_data)\n",
    "\n",
    "vanilla_perceptron_truthfulness = VanillaPerceptron()\n",
    "vanilla_perceptron_truthfulness.load(vanilla_perceptron_truthfulness_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test Loaded Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(960, 7655)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_vectors = tf_idf_model.transform(tokenized_documents)\n",
    "tf_idf_vectors.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sentiment, X_test_sentiment, y_train_sentiment, y_test_sentiment = train_test_split(\n",
    "    tf_idf_vectors, sentiment_labels, TEST_SIZE\n",
    ")\n",
    "X_train_truthfulness, X_test_truthfulness, y_train_truthfulness, y_test_truthfulness = train_test_split(\n",
    "    tf_idf_vectors, truthfulness_labels, TEST_SIZE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_sentiment = vanilla_perceptron_sentiment.predict(X_test_sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------ Train ------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Neg       0.72      1.00      0.84       381\n",
      "         Pos       1.00      0.62      0.76       387\n",
      "\n",
      "    accuracy                           0.81       768\n",
      "   macro avg       0.86      0.81      0.80       768\n",
      "weighted avg       0.86      0.81      0.80       768\n",
      "\n",
      "---------------------------------------------------------\n",
      "------------------------ Test ------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Neg       0.74      0.99      0.85        99\n",
      "         Pos       0.98      0.63      0.77        93\n",
      "\n",
      "    accuracy                           0.82       192\n",
      "   macro avg       0.86      0.81      0.81       192\n",
      "weighted avg       0.86      0.82      0.81       192\n",
      "\n",
      "---------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "calculate_scores(\n",
    "    np.where(y_train_sentiment == -1, \"Neg\", \"Pos\"),\n",
    "    np.where(vanilla_perceptron_sentiment.predict(X_train_sentiment) == -1, \"Neg\", \"Pos\"),\n",
    "    title=\"Train\",\n",
    ")\n",
    "calculate_scores(\n",
    "    np.where(y_test_sentiment == -1, \"Neg\", \"Pos\"), np.where(y_pred_sentiment == -1, \"Neg\", \"Pos\"), title=\"Test\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_truthfulness = vanilla_perceptron_truthfulness.predict(X_test_truthfulness)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------ Train ------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fake       0.99      0.95      0.97       385\n",
      "        True       0.95      0.99      0.97       383\n",
      "\n",
      "    accuracy                           0.97       768\n",
      "   macro avg       0.97      0.97      0.97       768\n",
      "weighted avg       0.97      0.97      0.97       768\n",
      "\n",
      "---------------------------------------------------------\n",
      "------------------------ Test ------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fake       0.99      0.89      0.94        95\n",
      "        True       0.91      0.99      0.95        97\n",
      "\n",
      "    accuracy                           0.94       192\n",
      "   macro avg       0.95      0.94      0.94       192\n",
      "weighted avg       0.95      0.94      0.94       192\n",
      "\n",
      "---------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "calculate_scores(\n",
    "    np.where(y_train_truthfulness == -1, \"Fake\", \"True\"),\n",
    "    np.where(vanilla_perceptron_truthfulness.predict(X_train_truthfulness) == -1, \"Fake\", \"True\"),\n",
    "    title=\"Train\",\n",
    ")\n",
    "calculate_scores(\n",
    "    np.where(y_test_truthfulness == -1, \"Fake\", \"True\"),\n",
    "    np.where(y_pred_truthfulness == -1, \"Fake\", \"True\"),\n",
    "    title=\"Test\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------- ********\\*\\*********\\*\\*\\*********\\*\\********* -------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Averaged Perceptron\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AveragedPerceptron:\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    # TODO: Can implement this ... tolerance: float, early_stopping: bool = True\n",
    "    def fit(\n",
    "        self,\n",
    "        X: npt.NDArray,\n",
    "        y: npt.NDArray,\n",
    "        max_iterations: int,\n",
    "        learning_rate: float = 1e-2,\n",
    "        tolerance: float = 1e-2,\n",
    "        shuffle: bool = True,\n",
    "        class_weights: dict = None,\n",
    "    ):\n",
    "        misclassified = 0\n",
    "        n_epoch = 0\n",
    "\n",
    "        self.weights = np.zeros(shape=(X.shape[-1],))\n",
    "        self.bias = 0.0\n",
    "        self.cache = {\"weights\": np.zeros(shape=(X.shape[-1],)), \"bias\": 0.0}\n",
    "\n",
    "        c = 1\n",
    "        for epoch in range(max_iterations):\n",
    "            n_epoch = epoch\n",
    "\n",
    "            if shuffle:\n",
    "                idxs = np.random.permutation(X.shape[0])\n",
    "                X = X[idxs]\n",
    "                y = y[idxs]\n",
    "\n",
    "            for x, y_true in zip(X, y):\n",
    "\n",
    "                a = np.dot(self.weights, x) + self.bias\n",
    "                if y_true * a <= 0:\n",
    "                    if class_weights is None:\n",
    "                        self.weights = self.weights + y_true * x\n",
    "                    else:\n",
    "                        self.weights = self.weights + y_true * x * class_weights[y_true]\n",
    "\n",
    "                    self.bias = self.bias + y_true\n",
    "\n",
    "                    self.cache[\"weights\"] = self.cache[\"weights\"] + y_true * c * x\n",
    "                    self.cache[\"bias\"] = self.cache[\"bias\"] + y_true * c\n",
    "\n",
    "                c += 1\n",
    "\n",
    "        self.weights = self.weights - ((1 / c) * self.cache[\"weights\"])\n",
    "        self.bias = self.bias - ((1 / c) * self.cache[\"bias\"])\n",
    "\n",
    "    def predict(self, X: npt.NDArray):\n",
    "        predictions = list()\n",
    "        for x in X:\n",
    "            pred = np.sign(np.dot(self.weights, x) + self.bias)\n",
    "            predictions.append(pred)\n",
    "        return np.array(predictions)\n",
    "\n",
    "    def export(\n",
    "        self,\n",
    "    ):\n",
    "        return {\"weights\": self.weights.tolist(), \"biases\": self.bias}\n",
    "\n",
    "    def load(self, perceptron_data: Dict[str, Any]):\n",
    "        self.weights = perceptron_data[\"weights\"]\n",
    "        self.bias = perceptron_data[\"bias\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 100  Train:  0.7685290763968073  Test:  0.7381974248927038\n",
      "Epoch # 150  Train:  0.8041237113402061  Test:  0.7631578947368421\n",
      "Epoch # 200  Train:  0.8370457209847597  Test:  0.7892376681614349\n",
      "Epoch # 250  Train:  0.8619447779111644  Test:  0.8054298642533937\n",
      "Epoch # 300  Train:  0.8812729498164015  Test:  0.8240740740740741\n",
      "Epoch # 350  Train:  0.9007444168734491  Test:  0.827906976744186\n",
      "Epoch # 400  Train:  0.9166666666666666  Test:  0.8380952380952381\n",
      "Epoch # 450  Train:  0.9273885350318471  Test:  0.8421052631578948\n",
      "Epoch # 500  Train:  0.9346991037131882  Test:  0.8557692307692307\n",
      "Epoch # 550  Train:  0.9435897435897436  Test:  0.859903381642512\n",
      "Epoch # 600  Train:  0.9472329472329473  Test:  0.8627450980392156\n",
      "Epoch # 650  Train:  0.9521345407503234  Test:  0.8613861386138614\n",
      "Epoch # 700  Train:  0.9547218628719275  Test:  0.8542713567839195\n",
      "Epoch # 750  Train:  0.9571984435797665  Test:  0.8615384615384615\n",
      "Epoch # 800  Train:  0.9622886866059819  Test:  0.865979381443299\n",
      "Epoch # 850  Train:  0.9661458333333334  Test:  0.875\n",
      "Epoch # 900  Train:  0.9673202614379085  Test:  0.8736842105263157\n",
      "Epoch # 950  Train:  0.9659685863874344  Test:  0.8663101604278074\n",
      "Epoch # 1000  Train:  0.9750982961992137  Test:  0.8602150537634408\n"
     ]
    }
   ],
   "source": [
    "epochs = [100, 150, 200, 250, 300, 350, 400, 450, 500, 550, 600, 650, 700, 750, 800, 850, 900, 950, 1000]\n",
    "\n",
    "for epoch in epochs:\n",
    "    averaged_perceptron_sentiment = AveragedPerceptron()\n",
    "    averaged_perceptron_sentiment.fit(\n",
    "        X_train, y_train_sentiment, max_iterations=epoch, learning_rate=1e-2, tolerance=1e-8, shuffle=True\n",
    "    )\n",
    "\n",
    "    y_pred_train_sentiment = averaged_perceptron_sentiment.predict(X_train)\n",
    "    y_pred_sentiment = averaged_perceptron_sentiment.predict(X_test)\n",
    "    \n",
    "    print(\"Epoch #\", epoch, \" Train: \", f1_score(y_train_sentiment, y_pred_train_sentiment), \" Test: \", f1_score(y_test_sentiment, y_pred_sentiment))\n",
    "\n",
    "    del averaged_perceptron_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "averaged_perceptron_sentiment = AveragedPerceptron()\n",
    "averaged_perceptron_sentiment.fit(\n",
    "    tf_idf_vectors, sentiment_labels, max_iterations=800, learning_rate=1e-2, tolerance=1e-8, shuffle=True\n",
    ")\n",
    "\n",
    "# averaged_perceptron_sentiment_data = averaged_perceptron_sentiment.export()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_sentiment_pred = averaged_perceptron_sentiment.predict(dev_tf_idf_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------ Train ------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Neg       0.97      0.97      0.97       480\n",
      "         Pos       0.97      0.97      0.97       480\n",
      "\n",
      "    accuracy                           0.97       960\n",
      "   macro avg       0.97      0.97      0.97       960\n",
      "weighted avg       0.97      0.97      0.97       960\n",
      "\n",
      "---------------------------------------------------------\n",
      "------------------------ Dev ------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Neg       0.88      0.95      0.92       160\n",
      "         Pos       0.95      0.88      0.91       160\n",
      "\n",
      "    accuracy                           0.91       320\n",
      "   macro avg       0.91      0.91      0.91       320\n",
      "weighted avg       0.91      0.91      0.91       320\n",
      "\n",
      "---------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "calculate_scores(final_data[:, SENTIMENT_TARGET_COL], np.where(averaged_perceptron_sentiment.predict(tf_idf_vectors) == -1, \"Neg\", \"Pos\"), title=\"Train\")\n",
    "calculate_scores(dev_key_data[:, 2], np.where(dev_sentiment_pred == -1, \"Neg\", \"Pos\"), title=\"Dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_f1_score = f1_score(np.where(dev_key_data[:, 2] == \"Neg\", -1, 1), dev_sentiment_pred, pos_label=-1)\n",
    "pos_f1_score = f1_score(np.where(dev_key_data[:, 2] == \"Neg\", -1, 1), dev_sentiment_pred, pos_label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 100  Train:  0.35887096774193544  Test:  0.17391304347826084\n",
      "Epoch # 150  Train:  0.4362934362934363  Test:  0.21052631578947367\n",
      "Epoch # 200  Train:  0.5046382189239331  Test:  0.22916666666666663\n",
      "Epoch # 250  Train:  0.5898778359511343  Test:  0.28\n",
      "Epoch # 300  Train:  0.6338983050847457  Test:  0.3364485981308411\n",
      "Epoch # 350  Train:  0.7266982622432858  Test:  0.4347826086956521\n",
      "Epoch # 400  Train:  0.7687595712098009  Test:  0.46153846153846156\n",
      "Epoch # 450  Train:  0.7909774436090227  Test:  0.5396825396825395\n",
      "Epoch # 500  Train:  0.8228404099560761  Test:  0.5625\n",
      "Epoch # 550  Train:  0.8579545454545454  Test:  0.5925925925925926\n",
      "Epoch # 600  Train:  0.8739495798319328  Test:  0.5985401459854014\n",
      "Epoch # 650  Train:  0.897119341563786  Test:  0.647887323943662\n",
      "Epoch # 700  Train:  0.9105691056910569  Test:  0.6887417218543046\n",
      "Epoch # 750  Train:  0.9249329758713137  Test:  0.6883116883116883\n",
      "Epoch # 800  Train:  0.929427430093209  Test:  0.6962025316455697\n",
      "Epoch # 850  Train:  0.9407114624505929  Test:  0.7073170731707318\n",
      "Epoch # 900  Train:  0.9490196078431372  Test:  0.7151515151515152\n",
      "Epoch # 950  Train:  0.9571984435797665  Test:  0.7142857142857143\n",
      "Epoch # 1000  Train:  0.9638242894056848  Test:  0.7471264367816093\n"
     ]
    }
   ],
   "source": [
    "epochs = [100, 150, 200, 250, 300, 350, 400, 450, 500, 550, 600, 650, 700, 750, 800, 850, 900, 950, 1000]\n",
    "\n",
    "for epoch in epochs:\n",
    "    averaged_perceptron_truthfulness = AveragedPerceptron()\n",
    "    averaged_perceptron_truthfulness.fit(\n",
    "        X_train, y_train_truthfulness, max_iterations=epoch, learning_rate=1e-2, tolerance=1e-8, shuffle=True\n",
    "    )\n",
    "\n",
    "    y_pred_train_truthfulness = averaged_perceptron_truthfulness.predict(X_train)\n",
    "    y_pred_truthfulness = averaged_perceptron_truthfulness.predict(X_test)\n",
    "    \n",
    "    print(\"Epoch #\", epoch, \" Train: \", f1_score(y_train_truthfulness, y_pred_train_truthfulness), \" Test: \", f1_score(y_test_truthfulness, y_pred_truthfulness))\n",
    "\n",
    "    del averaged_perceptron_truthfulness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "averaged_perceptron_truthfulness = AveragedPerceptron()\n",
    "averaged_perceptron_truthfulness.fit(\n",
    "    tf_idf_vectors, truthfulness_labels, max_iterations=1000, learning_rate=1e-2, tolerance=1e-8, shuffle=True, class_weights={\n",
    "        -1: 1.0275, 1: 1.0\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_truthfulness_pred = averaged_perceptron_truthfulness.predict(dev_tf_idf_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------ Train ------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fake       0.61      1.00      0.76       480\n",
      "        True       1.00      0.37      0.54       480\n",
      "\n",
      "    accuracy                           0.68       960\n",
      "   macro avg       0.81      0.68      0.65       960\n",
      "weighted avg       0.81      0.68      0.65       960\n",
      "\n",
      "---------------------------------------------------------\n",
      "------------------------ Dev ------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fake       0.54      1.00      0.70       160\n",
      "        True       1.00      0.14      0.24       160\n",
      "\n",
      "    accuracy                           0.57       320\n",
      "   macro avg       0.77      0.57      0.47       320\n",
      "weighted avg       0.77      0.57      0.47       320\n",
      "\n",
      "---------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "calculate_scores(final_data[:, TRUTHFULNESS_TARGET_COL], np.where(averaged_perceptron_truthfulness.predict(tf_idf_vectors) == -1, \"Fake\", \"True\"), title=\"Train\")\n",
    "calculate_scores(dev_key_data[:, 1], np.where(dev_truthfulness_pred == -1, \"Fake\", \"True\"), title=\"Dev\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_f1_score = f1_score(np.where(dev_key_data[:, 1] == \"Fake\", -1, 1), dev_truthfulness_pred, pos_label=-1)\n",
    "truth_f1_score = f1_score(np.where(dev_key_data[:, 1] == \"Fake\", -1, 1), dev_truthfulness_pred, pos_label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([neg_f1_score, pos_f1_score, fake_f1_score, truth_f1_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = list()\n",
    "for (id, truthfulness, sentiment) in zip(dev_raw_data[:,0], np.where(dev_truthfulness_pred == -1, \"Fake\", \"True\"), np.where(dev_sentiment_pred == -1, \"Neg\", \"Pos\")):\n",
    "    output.append((id, truthfulness, sentiment))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_predictions(OUTPUT_FILE_PATH, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "averaged_perceptron_sentiment = AveragedPerceptron()\n",
    "averaged_perceptron_sentiment.fit(X_train, y_train_sentiment, max_iterations=epoch, learning_rate=1e-2, tolerance=1e-8, shuffle=True)\n",
    "\n",
    "averaged_perceptron_sentiment_data = averaged_perceptron_sentiment.export()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classify\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_sentiment = averaged_perceptron_sentiment.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------ Train ------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Neg       0.85      0.55      0.66       387\n",
      "         Pos       0.66      0.90      0.76       381\n",
      "\n",
      "    accuracy                           0.72       768\n",
      "   macro avg       0.76      0.72      0.71       768\n",
      "weighted avg       0.76      0.72      0.71       768\n",
      "\n",
      "---------------------------------------------------------\n",
      "------------------------ Test ------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Neg       0.78      0.45      0.57        93\n",
      "         Pos       0.63      0.88      0.73        99\n",
      "\n",
      "    accuracy                           0.67       192\n",
      "   macro avg       0.70      0.67      0.65       192\n",
      "weighted avg       0.70      0.67      0.66       192\n",
      "\n",
      "---------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "calculate_scores(\n",
    "    np.where(y_train_sentiment == -1, \"Neg\", \"Pos\"),\n",
    "    np.where(averaged_perceptron_sentiment.predict(X_train) == -1, \"Neg\", \"Pos\"),\n",
    "    title=\"Train\",\n",
    ")\n",
    "calculate_scores(\n",
    "    np.where(y_test_sentiment == -1, \"Neg\", \"Pos\"), np.where(y_pred_sentiment == -1, \"Neg\", \"Pos\"), title=\"Test\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Truthfulness Classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #: 1\n",
      "Epoch #: 2\n",
      "Epoch #: 3\n",
      "Epoch #: 4\n",
      "Epoch #: 5\n",
      "Epoch #: 6\n",
      "Epoch #: 7\n",
      "Epoch #: 8\n",
      "Epoch #: 9\n",
      "Epoch #: 10\n",
      "Epoch #: 11\n",
      "Epoch #: 12\n",
      "Epoch #: 13\n",
      "Epoch #: 14\n",
      "Epoch #: 15\n",
      "Epoch #: 16\n",
      "Epoch #: 17\n",
      "Epoch #: 18\n",
      "Epoch #: 19\n"
     ]
    }
   ],
   "source": [
    "averaged_perceptron_truthfulness = AveragedPerceptron()\n",
    "averaged_perceptron_truthfulness.fit(X_train_truthfulness, y_train_truthfulness, max_iterations=100)\n",
    "\n",
    "averaged_perceptron_truthfulness_data = averaged_perceptron_truthfulness.export()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classify\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_truthfulness = averaged_perceptron_truthfulness.predict(X_test_truthfulness)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------ Train ------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fake       0.50      1.00      0.67       385\n",
      "        True       0.00      0.00      0.00       383\n",
      "\n",
      "    accuracy                           0.50       768\n",
      "   macro avg       0.25      0.50      0.33       768\n",
      "weighted avg       0.25      0.50      0.33       768\n",
      "\n",
      "---------------------------------------------------------\n",
      "------------------------ Test ------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fake       0.49      1.00      0.66        95\n",
      "        True       0.00      0.00      0.00        97\n",
      "\n",
      "    accuracy                           0.49       192\n",
      "   macro avg       0.25      0.50      0.33       192\n",
      "weighted avg       0.24      0.49      0.33       192\n",
      "\n",
      "---------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aditya/.pyenv/versions/3.10.6/envs/csci544/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/aditya/.pyenv/versions/3.10.6/envs/csci544/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/aditya/.pyenv/versions/3.10.6/envs/csci544/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/aditya/.pyenv/versions/3.10.6/envs/csci544/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/aditya/.pyenv/versions/3.10.6/envs/csci544/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/aditya/.pyenv/versions/3.10.6/envs/csci544/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "calculate_scores(\n",
    "    np.where(y_train_truthfulness == -1, \"Fake\", \"True\"),\n",
    "    np.where(averaged_perceptron_truthfulness.predict(X_train_truthfulness) == -1, \"Fake\", \"True\"),\n",
    "    title=\"Train\",\n",
    ")\n",
    "calculate_scores(\n",
    "    np.where(y_test_truthfulness == -1, \"Fake\", \"True\"),\n",
    "    np.where(y_pred_truthfulness == -1, \"Fake\", \"True\"),\n",
    "    title=\"Test\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write Averaged Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "averaged_model_file_data = {\n",
    "    \"tf_idf_model\": tf_idf_model_data,\n",
    "    \"sentiment_classifier\": averaged_perceptron_sentiment_data,\n",
    "    \"truthfulness_classifier\": averaged_perceptron_truthfulness_data,\n",
    "}\n",
    "\n",
    "store_model(AVERAGED_MODEL_FILE_PATH, averaged_model_file_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Averaged Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf_idf_model, averaged_perceptron_sentiment, averaged_perceptron_truthfulness = load_model(AVERAGED_MODEL_FILE_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(960, 7655)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_vectors = tf_idf_model.transform(tokenized_documents)\n",
    "tf_idf_vectors.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test Loaded Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sentiment, X_test_sentiment, y_train_sentiment, y_test_sentiment = train_test_split(\n",
    "    tf_idf_vectors, sentiment_labels, TEST_SIZE\n",
    ")\n",
    "X_train_truthfulness, X_test_truthfulness, y_train_truthfulness, y_test_truthfulness = train_test_split(\n",
    "    tf_idf_vectors, truthfulness_labels, TEST_SIZE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_sentiment = averaged_perceptron_sentiment.predict(X_test_sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------ Train ------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Neg       0.95      0.99      0.97       380\n",
      "         Pos       0.99      0.95      0.97       388\n",
      "\n",
      "    accuracy                           0.97       768\n",
      "   macro avg       0.97      0.97      0.97       768\n",
      "weighted avg       0.97      0.97      0.97       768\n",
      "\n",
      "---------------------------------------------------------\n",
      "------------------------ Test ------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Neg       0.98      0.99      0.99       100\n",
      "         Pos       0.99      0.98      0.98        92\n",
      "\n",
      "    accuracy                           0.98       192\n",
      "   macro avg       0.98      0.98      0.98       192\n",
      "weighted avg       0.98      0.98      0.98       192\n",
      "\n",
      "---------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "calculate_scores(\n",
    "    np.where(y_train_sentiment == -1, \"Neg\", \"Pos\"),\n",
    "    np.where(averaged_perceptron_sentiment.predict(X_train_sentiment) == -1, \"Neg\", \"Pos\"),\n",
    "    title=\"Train\",\n",
    ")\n",
    "calculate_scores(\n",
    "    np.where(y_test_sentiment == -1, \"Neg\", \"Pos\"), np.where(y_pred_sentiment == -1, \"Neg\", \"Pos\"), title=\"Test\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_truthfulness = averaged_perceptron_truthfulness.predict(X_test_truthfulness)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------ Train ------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fake       0.51      1.00      0.68       392\n",
      "        True       0.00      0.00      0.00       376\n",
      "\n",
      "    accuracy                           0.51       768\n",
      "   macro avg       0.26      0.50      0.34       768\n",
      "weighted avg       0.26      0.51      0.34       768\n",
      "\n",
      "---------------------------------------------------------\n",
      "------------------------ Test ------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fake       0.46      1.00      0.63        88\n",
      "        True       0.00      0.00      0.00       104\n",
      "\n",
      "    accuracy                           0.46       192\n",
      "   macro avg       0.23      0.50      0.31       192\n",
      "weighted avg       0.21      0.46      0.29       192\n",
      "\n",
      "---------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aditya/.pyenv/versions/3.10.6/envs/csci544/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/aditya/.pyenv/versions/3.10.6/envs/csci544/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/aditya/.pyenv/versions/3.10.6/envs/csci544/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/aditya/.pyenv/versions/3.10.6/envs/csci544/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/aditya/.pyenv/versions/3.10.6/envs/csci544/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/aditya/.pyenv/versions/3.10.6/envs/csci544/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "calculate_scores(\n",
    "    np.where(y_train_truthfulness == -1, \"Fake\", \"True\"),\n",
    "    np.where(averaged_perceptron_truthfulness.predict(X_train_truthfulness) == -1, \"Fake\", \"True\"),\n",
    "    title=\"Train\",\n",
    ")\n",
    "calculate_scores(\n",
    "    np.where(y_test_truthfulness == -1, \"Fake\", \"True\"),\n",
    "    np.where(y_pred_truthfulness == -1, \"Fake\", \"True\"),\n",
    "    title=\"Test\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit ('csci544')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e78917e7f90f3892e7e12462ef46781cf5994bd706032ea53be00d0b1f29dcb9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
