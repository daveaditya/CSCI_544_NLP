{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import json\n",
    "from typing import List, Dict, Tuple, Set, Any\n",
    "\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Constants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "### Constants                                      ###\n",
    "######################################################\n",
    "# Base Paths\n",
    "INPUT_PATH = \"./data\"\n",
    "MODEL_PATH = \"./model\"\n",
    "\n",
    "# Model File names\n",
    "VANILLA_MODEL_FILENAME = \"vanillamodel.txt\"\n",
    "AVERAGED_MODEL_FILENAME = \"averagedmodel.txt\"\n",
    "\n",
    "# Class Identifiers\n",
    "TRUTHFUL = \"True\"\n",
    "DECEPTIVE = \"Fake\"\n",
    "POSITIVE = \"Pos\"\n",
    "NEGATIVE = \"Neg\"\n",
    "\n",
    "# File paths\n",
    "TRAIN_FILE_PATH = f\"{INPUT_PATH}/train-labeled.txt\"\n",
    "CLEANED_DATA_FILE_PATH = f\"{INPUT_PATH}/cleaned-data.txt\"\n",
    "PREPROCESSED_DATA_FILE_PATH = f\"{INPUT_PATH}/preprocessed-data.txt\"\n",
    "\n",
    "VANILLA_MODEL_FILE_PATH = f\"{MODEL_PATH}/{VANILLA_MODEL_FILENAME}\"\n",
    "AVERAGED_MODEL_FILE_PATH = f\"{MODEL_PATH}/{AVERAGED_MODEL_FILENAME}\"\n",
    "\n",
    "DEV_FILE_PATH = f\"{INPUT_PATH}/dev-text.txt\"\n",
    "DEV_TAGGED_FILE_PATH = f\"{INPUT_PATH}/dev-key.txt\"\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "DATA_COL = 3\n",
    "SENTIMENT_TARGET_COL = 2\n",
    "TRUTHFULNESS_TARGET_COL = 1\n",
    "TEST_SIZE = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(seed=RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = \"07Zfn0z Fake Pos If you're looking for an elegant hotel in downtown Chicago, you have to stay here. The Ambassador East Hotel has very comfortable and beautiful large rooms and is like a home away from home. The perfect place for a business person, and if you have a small pet you can bring them too! I would give this place four stars and would definitely stay here again.\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_regex = re.compile(\"(\\w*) (\\w*) (\\w*) (.*)\\n?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('07Zfn0z',\n",
       " 'Fake',\n",
       " 'Pos',\n",
       " \"If you're looking for an elegant hotel in downtown Chicago, you have to stay here. The Ambassador East Hotel has very comfortable and beautiful large rooms and is like a home away from home. The perfect place for a business person, and if you have a small pet you can bring them too! I would give this place four stars and would definitely stay here again.\")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.match(input_regex, line).groups()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('07Zfn0z',\n",
       "  'Fake',\n",
       "  'Pos',\n",
       "  \"If you're looking for an elegant hotel in downtown Chicago, you have to stay here. The Ambassador East Hotel has very comfortable and beautiful large rooms and is like a home away from home. The perfect place for a business person, and if you have a small pet you can bring them too! I would give this place four stars and would definitely stay here again.\")]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "data.append(re.match(input_regex, line).groups())\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Fake'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(data)[0, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "def load_data(input_file_path: str, type: str = \"TRAIN\") -> npt.NDArray:\n",
    "    input_data = list()\n",
    "    regex = \"(\\w*) (\\w*) (\\w*) (.*)\\n?\"\n",
    "    if type == \"TEST\":\n",
    "        regex = \"(\\w*) (.*)\\n?\"\n",
    "    elif type == \"LABELS\":\n",
    "        regex = \"(\\w*) (\\w*) (\\w*)\\n?\"\n",
    "    input_regex = re.compile(regex)\n",
    "    with open(input_file_path, mode=\"r\") as input_file:\n",
    "        for line in input_file:\n",
    "            input_data.append(re.match(input_regex, line).groups())\n",
    "    return np.array(input_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store Data\n",
    "def store_data(date_file_path: str, data: npt.NDArray) -> None:\n",
    "    with open(date_file_path, mode=\"w\") as data_file:\n",
    "        for row in data:\n",
    "            data_file.write(f\"{row[0]} {row[1]} {row[2]} {row[3]}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store Model\n",
    "def store_model(model_file_path: str, model_data: Any) -> None:\n",
    "    # TODO: Need to check model_data\n",
    "    with open(model_file_path, mode=\"w\") as model_file:\n",
    "        json.dump(model_data, model_file, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model\n",
    "def load_model(model_file_path: str) -> npt.NDArray:\n",
    "    # TODO: Extract data from JSON and return that only\n",
    "    with open(model_file_path, mode=\"r\") as model_file:\n",
    "        model_data = json.load(model_file)\n",
    "    return model_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store Predictions\n",
    "def store_predictions(output_file_path: str, predictions: List[Tuple[str, str, str]]) -> None:\n",
    "    with open(output_file_path, mode=\"w\") as output_file:\n",
    "        for prediction in predictions:\n",
    "            output_file.write(f\"{prediction[0]} {prediction[1]} {prediction[2]}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Scores\n",
    "def calculate_scores(y_true, y_pred, title: str):\n",
    "    from sklearn.metrics import classification_report\n",
    "\n",
    "    print(f\"------------------------ {title} ------------------------\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(\"---------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data(TRAIN_FILE_PATH, type=\"TRAIN\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all reviews to lower case (optional according to study)\n",
    "def to_lower(data: npt.NDArray):\n",
    "    n_data = data.shape[0]\n",
    "    result = data.copy()\n",
    "    for i in range(n_data):\n",
    "        result[i] = result[i].lower()\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_encodings(data: npt.NDArray):\n",
    "    import re\n",
    "\n",
    "    n_data = data.shape[0]\n",
    "    result = data.copy()\n",
    "    for i in range(n_data):\n",
    "        result[i] = re.sub(r\"&#\\d+;\", \" \", result[i])\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_tags(data: npt.NDArray):\n",
    "    import re\n",
    "\n",
    "    n_data = data.shape[0]\n",
    "    result = data.copy()\n",
    "    for i in range(n_data):\n",
    "        result[i] = re.sub(r\"<[a-zA-Z]+\\s?/?>\", \"\", result[i])\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(data: npt.NDArray):\n",
    "    import re\n",
    "\n",
    "    n_data = data.shape[0]\n",
    "    result = data.copy()\n",
    "    for i in range(n_data):\n",
    "        result[i] = re.sub(r\"https?://([\\w\\-\\._]+){2,}/[\\w\\-\\.\\-/=\\+_\\?]+\", \"\", result[i])\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_and_url(data):\n",
    "    \"\"\"Function to remove\n",
    "             1. HTML encodings\n",
    "             2. HTML tags (both closed and open)\n",
    "             3. URLs\n",
    "\n",
    "    Args:\n",
    "        data (npt.NDArray): A Numpy Array of type string\n",
    "\n",
    "    Returns:\n",
    "        _type_: npt.NDArray\n",
    "    \"\"\"\n",
    "    import re\n",
    "\n",
    "    n_data = data.shape[0]\n",
    "    result = data.copy()\n",
    "    for i in range(n_data):\n",
    "        # Remove HTML encodings\n",
    "        result[i] = re.sub(r\"&#\\d+;\", \"\", result[i])\n",
    "\n",
    "        # Remove HTML tags (both open and closed)\n",
    "        result[i] = re.sub(r\"<[a-zA-Z]+\\s?/?>\", \"\", result[i])\n",
    "\n",
    "        # Remove URLs\n",
    "        result[i] = re.sub(r\"https?://([\\w\\-\\._]+){2,}/[\\w\\-\\.\\-/=\\+_\\?]+\", \"\", result[i])\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_digits_with_tag(data: npt.NDArray):\n",
    "    import re\n",
    "\n",
    "    n_data = data.shape[0]\n",
    "    result = data.copy()\n",
    "    for i in range(n_data):\n",
    "        result[i] = re.sub(r\"\\d+\", \" NUM \", result[i])\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove non-alphabetical characters\n",
    "def remove_non_alpha_characters(data: npt.NDArray):\n",
    "    import re\n",
    "\n",
    "    n_data = data.shape[0]\n",
    "    result = data.copy()\n",
    "    for i in range(n_data):\n",
    "        result[i] = re.sub(r\"_+|\\\\|[^a-zA-Z0-9\\s]\", \" \", result[i])\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove extra spaces\n",
    "def remove_extra_spaces(data: npt.NDArray):\n",
    "    import re\n",
    "\n",
    "    n_data = data.shape[0]\n",
    "    result = data.copy()\n",
    "    for i in range(n_data):\n",
    "        result[i] = re.sub(r\"^\\s*|\\s\\s*\", \" \", result[i])\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expanding contractions\n",
    "def fix_contractions(data: npt.NDArray):\n",
    "    # TODO: Replace with custom implementation\n",
    "    import contractions\n",
    "\n",
    "    def contraction_fixer(txt: str):\n",
    "        return \" \".join([contractions.fix(word) for word in txt.split()])\n",
    "\n",
    "    n_data = data.shape[0]\n",
    "    result = data.copy()\n",
    "    for i in range(n_data):\n",
    "        result[i] = contraction_fixer(result[i])\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting: to_lower\n",
      "Ended: to_lower\n",
      "Starting: remove_html_encodings\n",
      "Ended: remove_html_encodings\n",
      "Starting: remove_html_tags\n",
      "Ended: remove_html_tags\n",
      "Starting: remove_url\n",
      "Ended: remove_url\n",
      "Starting: fix_contractions\n",
      "Ended: fix_contractions\n",
      "Starting: remove_non_alpha_characters\n",
      "Ended: remove_non_alpha_characters\n",
      "Starting: remove_extra_spaces\n",
      "Ended: remove_extra_spaces\n"
     ]
    }
   ],
   "source": [
    "# A dictionary containing the columns and a list of functions to perform on it in order\n",
    "data_cleaning_pipeline = {\n",
    "    DATA_COL: [\n",
    "        to_lower,\n",
    "        remove_html_encodings,\n",
    "        remove_html_tags,\n",
    "        remove_url,\n",
    "        fix_contractions,\n",
    "        remove_non_alpha_characters,\n",
    "        remove_extra_spaces,\n",
    "    ]\n",
    "}\n",
    "\n",
    "cleaned_data = data.copy()\n",
    "\n",
    "# Process all the cleaning instructions\n",
    "for col, pipeline in data_cleaning_pipeline.items():\n",
    "    # Get the column to perform cleaning on\n",
    "    temp_data = cleaned_data[:, col].copy()\n",
    "\n",
    "    # Perform all the cleaning functions sequencially\n",
    "    for func in pipeline:\n",
    "        print(f\"Starting: {func.__name__}\")\n",
    "        temp_data = func(temp_data)\n",
    "        print(f\"Ended: {func.__name__}\")\n",
    "\n",
    "    # Replace the old column with cleaned one.\n",
    "    cleaned_data[:, col] = temp_data.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_data(CLEANED_DATA_FILE_PATH, cleaned_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize(data: pd.Series):\n",
    "#     from nltk.tokenize import word_tokenize\n",
    "\n",
    "#     nltk.download(\"punkt\")\n",
    "\n",
    "#     return data.apply(word_tokenize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_stopwords(data: pd.Series):\n",
    "#     \"\"\"Remove stop words using the NLTK stopwords dictionary\n",
    "\n",
    "#     Args:\n",
    "#         string (str): a document\n",
    "\n",
    "#     Returns:\n",
    "#         str: a document with stopwords removed\n",
    "#     \"\"\"\n",
    "#     from nltk.corpus import stopwords\n",
    "\n",
    "#     nltk.download(\"stopwords\")\n",
    "\n",
    "#     stopwords = set(stopwords.words())\n",
    "\n",
    "#     def remover(word_list: List[str], stopwords: Set[str]):\n",
    "#         return [word for word in word_list if not word in stopwords]\n",
    "\n",
    "#     return data.apply(lambda word_list: remover(word_list, stopwords))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def lemmatize(data: pd.Series, consider_pos_tag: bool = True):\n",
    "#     from nltk.corpus import wordnet\n",
    "#     from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#     nltk.download(\"omw-1.4\")\n",
    "\n",
    "#     # POS tagging\n",
    "#     def perform_nltk_pos_tag(data: pd.Series):\n",
    "#         from nltk import pos_tag\n",
    "\n",
    "#         nltk.download(\"averaged_perceptron_tagger\")\n",
    "\n",
    "#         return data.apply(pos_tag)\n",
    "\n",
    "#     # Convert POS tag to wordnet pos tags\n",
    "#     def wordnet_pos_tagger(tag: str):\n",
    "#         if tag.startswith(\"J\"):\n",
    "#             return wordnet.ADJ\n",
    "#         elif tag.startswith(\"V\"):\n",
    "#             return wordnet.VERB\n",
    "#         elif tag.startswith(\"N\"):\n",
    "#             return wordnet.NOUN\n",
    "#         elif tag.startswith(\"R\"):\n",
    "#             return wordnet.ADV\n",
    "#         else:\n",
    "#             return None\n",
    "\n",
    "#     lemmatizer = WordNetLemmatizer()\n",
    "#     lemmatized = list()\n",
    "\n",
    "#     if consider_pos_tag:\n",
    "#         pos_tagged_data = data.copy()\n",
    "#         pos_tagged_data = perform_nltk_pos_tag(data)\n",
    "\n",
    "#         for row in pos_tagged_data:\n",
    "\n",
    "#             lemmatized_row = list()\n",
    "\n",
    "#             if consider_pos_tag:\n",
    "#                 for word, tag in row:\n",
    "#                     wordnet_pos_tag = wordnet_pos_tagger(tag)\n",
    "\n",
    "#                     if wordnet_pos_tag is None:\n",
    "#                         lemmatized_row.append(word)\n",
    "#                     else:\n",
    "#                         result = lemmatizer.lemmatize(word, wordnet_pos_tag)\n",
    "#                         lemmatized_row.append(lemmatizer.lemmatize(word, wordnet_pos_tag))\n",
    "\n",
    "#             lemmatized.append(lemmatized_row)\n",
    "#     else:\n",
    "#         for row in data:\n",
    "#             lemmatized_row = list()\n",
    "\n",
    "#             for word in row:\n",
    "#                 lemmatized_row.append(lemmatizer.lemmatize(word))\n",
    "\n",
    "#             lemmatized.append(lemmatized_row)\n",
    "\n",
    "#     return pd.Series(lemmatized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate lemmatized sentences back into one sentence\n",
    "# def concatenate(data: pd.Series):\n",
    "#     return data.apply(lambda words: \" \".join(words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing_pipeline = {DATA_COL: [tokenize, lemmatize, concatenate]}\n",
    "\n",
    "# # Run the pipeline\n",
    "# preprocessed_data = cleaned_data.copy()\n",
    "\n",
    "# # Process all the cleaning instructions\n",
    "# for col, pipeline in preprocessing_pipeline.items():\n",
    "#     # Get the column to perform cleaning on\n",
    "#     temp_data = preprocessed_data[col].copy()\n",
    "\n",
    "#     # Perform all the cleaning functions sequencially\n",
    "#     for func in pipeline:\n",
    "#         print(f\"Starting: {func.__name__}\")\n",
    "\n",
    "#         if func.__name__ == \"lemmatize\":\n",
    "#             temp_data = func(temp_data, consider_pos_tag=True)\n",
    "#         else:\n",
    "#             temp_data = func(temp_data)\n",
    "\n",
    "#         print(f\"Ended: {func.__name__}\")\n",
    "\n",
    "#     # Replace the old column with cleaned one.\n",
    "#     preprocessed_data[col] = temp_data.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfIdf:\n",
    "    # TODO: Implement\n",
    "    def __init__(self) -> None:\n",
    "        self.n_docs = None\n",
    "        self.vocab = set()\n",
    "        self.vocab_size = None\n",
    "        self.vocab_index = dict()\n",
    "        self.word_document_count = dict()\n",
    "\n",
    "    def __create_vocab__(self, documents: npt.NDArray) -> Set:\n",
    "        vocab = set()\n",
    "\n",
    "        for document in documents:\n",
    "            for word in document:\n",
    "                vocab.add(word)\n",
    "\n",
    "        return vocab\n",
    "\n",
    "    def __get_word_document_count__(self, documents: npt.NDArray):\n",
    "        word_document_count = dict()\n",
    "\n",
    "        for word in self.vocab:\n",
    "            word_document_count[word] = 0\n",
    "            for document in documents:\n",
    "                if word in document:\n",
    "                    word_document_count[word] += 1\n",
    "\n",
    "        return word_document_count\n",
    "\n",
    "    def __term_frequency__(self, word: str, document: npt.NDArray):\n",
    "        word_occurences = (document == word).sum()\n",
    "        return word_occurences / self.n_docs\n",
    "\n",
    "    def __inverse_document_frequency__(self, word: str):\n",
    "        word_occurrences = 1\n",
    "\n",
    "        if word in self.word_document_count:\n",
    "            word_occurrences += self.word_document_count[word]\n",
    "\n",
    "        return np.log(self.n_docs / word_occurrences)\n",
    "\n",
    "    def __tf_idf__(self, document: npt.NDArray):\n",
    "        tf_idf_vector = np.zeros(shape=(self.vocab_size, ))\n",
    "        for word in document:\n",
    "            tf = self.__term_frequency__(word, document)\n",
    "            idf = self.__inverse_document_frequency__(word)\n",
    "\n",
    "            tf_idf_vector[self.vocab_index[word]] = tf * idf\n",
    "        return tf_idf_vector\n",
    "\n",
    "    def fit(self, documents: npt.NDArray):\n",
    "        self.n_docs = documents.shape[0]\n",
    "        self.vocab = self.__create_vocab__(documents)\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.vocab_index = {word: idx for idx, word in enumerate(self.vocab)}\n",
    "        self.word_document_count = self.__get_word_document_count__(documents)\n",
    "\n",
    "    def transform(self, documents: npt.NDArray):\n",
    "        tf_idf_vectors = list()\n",
    "        for document in documents:\n",
    "            tf_idf_vectors.append(self.__tf_idf__(document))\n",
    "        return np.array(tf_idf_vectors)\n",
    "\n",
    "    def export(self):\n",
    "        return {\n",
    "            \"n_docs\": self.n_docs,\n",
    "            \"vocab_size\": self.vocab_size,\n",
    "            \"vocab\": self.vocab,\n",
    "            \"vocab_size\": self.vocab_size,\n",
    "            \"vocab_index\": self.vocab_index,\n",
    "            \"self.word_document_count\": self.word_document_count\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = load_data(CLEANED_DATA_FILE_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(data: npt.NDArray):\n",
    "    tokenized_documents = list()\n",
    "    for document in data:\n",
    "        tokenized_documents.append(np.array(document.split()))\n",
    "    return np.array(tokenized_documents, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['if', 'you', 'are', 'looking', 'for', 'an', 'elegant', 'hotel',\n",
       "       'in', 'downtown', 'chicago', 'you', 'have', 'to', 'stay', 'here',\n",
       "       'the', 'ambassador', 'east', 'hotel', 'has', 'very', 'comfortable',\n",
       "       'and', 'beautiful', 'large', 'rooms', 'and', 'is', 'like', 'a',\n",
       "       'home', 'away', 'from', 'home', 'the', 'perfect', 'place', 'for',\n",
       "       'a', 'business', 'person', 'and', 'if', 'you', 'have', 'a',\n",
       "       'small', 'pet', 'you', 'can', 'bring', 'them', 'too', 'i', 'would',\n",
       "       'give', 'this', 'place', 'four', 'stars', 'and', 'would',\n",
       "       'definitely', 'stay', 'here', 'again'], dtype='<U11')"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_documents = tokenize(final_data[:, DATA_COL])\n",
    "tokenized_documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Invoke TF-IDF\n",
    "tf_idf = TfIdf()\n",
    "tf_idf.fit(tokenized_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(960, 7655)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_vectors = tf_idf.transform(tokenized_documents)\n",
    "tf_idf_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([706, 730, 688, 754, 492])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rng.integers(tokenized_documents.shape[0], size=(5,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8, 1, 5, 0, 7, 2, 9, 4, 3, 6])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.permutation(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(data: npt.NDArray, labels: npt.NDArray, test_size: float = 0.2):\n",
    "    n_max = data.shape[0]\n",
    "    sample = int((1 - test_size) * n_max)\n",
    "\n",
    "    all_idx = np.random.permutation(n_max)\n",
    "    train_idx, test_idx = all_idx[:sample], all_idx[sample:]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = data[train_idx], data[test_idx], labels[train_idx], labels[test_idx]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tf_idf_vectors, final_data[:, SENTIMENT_TARGET_COL], TEST_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((768, 7655), (768,))"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((192, 7655), (192,))"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    # TODO: Implement\n",
    "    def __init__(self, n_input, n_features) -> None:\n",
    "        self.weights = np.zeroes(shape=(n_input, n_features))\n",
    "        self.biases = np.zeroes(shape=(n_input,))\n",
    "\n",
    "    def fit(self, X: npt.NDArray, y: npt.NDArray, n_iterations: int, tolerance: float, early_stopping: bool = True):\n",
    "        pass\n",
    "\n",
    "    def predict():\n",
    "        pass\n",
    "\n",
    "    def export(\n",
    "        self,\n",
    "    ):\n",
    "        return {\"weights\": self.weights, \"biases\": self.biases}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AveragePerceptron:\n",
    "    # TODO: Implement\n",
    "    def __init__(self, n_input, n_features) -> None:\n",
    "        self.weights = np.zeroes(shape=(n_input, n_features))\n",
    "        self.biases = np.zeroes(shape=(n_input,))\n",
    "        self.cache = {\"weights\": np.zeroes(shape=(n_input, n_features)), \"biases\": np.zeroes(shape=(n_input,))}\n",
    "\n",
    "    def fit(self, n_iterations: int, tolerance: float, early_stopping: bool = True):\n",
    "        pass\n",
    "\n",
    "    def predict():\n",
    "        pass\n",
    "\n",
    "    def export(\n",
    "        self,\n",
    "    ):\n",
    "        return {\"weights\": self.weights, \"biases\": self.biases}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit ('csci544')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e78917e7f90f3892e7e12462ef46781cf5994bd706032ea53be00d0b1f29dcb9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
