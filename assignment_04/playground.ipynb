{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import json\n",
    "from typing import List, Dict, Tuple, Any\n",
    "\n",
    "import numpy as np\n",
    "import numpy.typing as npt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "### Constants                                      ###\n",
    "######################################################\n",
    "# Base Paths\n",
    "INPUT_PATH = \"./data\"\n",
    "MODEL_PATH = \"./model\"\n",
    "\n",
    "# Model File names\n",
    "VANILLA_MODEL_FILENAME = \"vanillamodel.txt\"\n",
    "AVERAGED_MODEL_FILENAME = \"averagedmodel.txt\"\n",
    "\n",
    "# Class Identifiers\n",
    "TRUTHFUL = \"True\"\n",
    "DECEPTIVE = \"Fake\"\n",
    "POSITIVE = \"Pos\"\n",
    "NEGATIVE = \"Neg\"\n",
    "\n",
    "# File paths\n",
    "TRAIN_FILE_PATH = f\"{INPUT_PATH}./train-labeled.txt\"\n",
    "\n",
    "VANILLA_MODEL_FILE_PATH = f\"{MODEL_PATH}/{VANILLA_MODEL_FILENAME}\"\n",
    "AVERAGED_MODEL_FILE_PATH = f\"{MODEL_PATH}/{AVERAGED_MODEL_FILENAME}\"\n",
    "\n",
    "DEV_FILE_PATH = f\"{INPUT_PATH}/dev-text.txt\"\n",
    "DEV_TAGGED_FILE_PATH = f\"{INPUT_PATH}/dev-key.txt\"\n",
    "\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(seed=RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = \"07Zfn0z Fake Pos If you're looking for an elegant hotel in downtown Chicago, you have to stay here. The Ambassador East Hotel has very comfortable and beautiful large rooms and is like a home away from home. The perfect place for a business person, and if you have a small pet you can bring them too! I would give this place four stars and would definitely stay here again.\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_regex = re.compile(\"(\\w*) (\\w*) (\\w*) (.*)\\n?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('07Zfn0z',\n",
       " 'Fake',\n",
       " 'Pos',\n",
       " \"If you're looking for an elegant hotel in downtown Chicago, you have to stay here. The Ambassador East Hotel has very comfortable and beautiful large rooms and is like a home away from home. The perfect place for a business person, and if you have a small pet you can bring them too! I would give this place four stars and would definitely stay here again.\")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.match(input_regex, line).groups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('07Zfn0z',\n",
       "  'Fake',\n",
       "  \"Pos If you're looking for an elegant hotel in downtown Chicago, you have to stay here. The Ambassador East Hotel has very comfortable and beautiful large rooms and is like a home away from home. The perfect place for a business person, and if you have a small pet you can bring them too! I would give this place four stars and would definitely stay here again.\")]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "data.append(re.match(input_regex, line).groups())\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Fake'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(data)[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "def load_data(input_file_path: str, type: str = \"TRAIN\") -> npt.NDArray:\n",
    "    input_data = list()\n",
    "    regex = \"(\\w*) (\\w*) (\\w*) (.*)\\n?\"\n",
    "    if type == \"TEST\":\n",
    "        regex = \"(\\w*) (.*)\\n?\"\n",
    "    elif type == \"LABELS\":\n",
    "        regex = \"(\\w*) (\\w*) (\\w*)\\n?\"\n",
    "    input_regex = re.compile(regex)\n",
    "    with open(input_file_path, mode=\"r\") as input_file:\n",
    "        for line in input_file:\n",
    "            input_data.append(re.match(input_regex, line).groups())\n",
    "    return np.array(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store Model\n",
    "def store_model(model_file_path: str, model_data: Any) -> None:\n",
    "    # TODO: Need to check model_data\n",
    "    with open(model_file_path, mode=\"w\") as model_file:\n",
    "        json.dump(model_data, model_file, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model\n",
    "def load_model(model_file_path: str) -> npt.NDArray:\n",
    "    # TODO: Extract data from JSON and return that only\n",
    "    with open(model_file_path, mode=\"r\") as model_file:\n",
    "        model_data = json.load(model_file)\n",
    "    return model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store Predictions\n",
    "def store_predictions(output_file_path: str, predictions: List[Tuple[str, str, str]]) -> None:\n",
    "    with open(output_file_path, mode=\"w\") as output_file:\n",
    "        for prediction in predictions:\n",
    "            output_file.write(f\"{prediction[0]} {prediction[1]} {prediction[2]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Scores\n",
    "def calculate_scores(y_true, y_pred, title: str):\n",
    "    from sklearn.metrics import classification_report\n",
    "\n",
    "    print(f\"------------------------ {title} ------------------------\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(\"---------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data(TRAIN_FILE_PATH, type=\"TRAIN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all reviews to lower case (optional according to study)\n",
    "# def to_lower(data: pd.Series):\n",
    "#     return data.str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_html_encodings(data):\n",
    "#   return data.str.replace(r\"&#\\d+;\", \" \", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_html_tags(data):\n",
    "#   return data.str.replace(r\"<[a-zA-Z]+\\s?/?>\", \" \", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_url(data):\n",
    "#   return data.str.replace(r\"https?://([\\w\\-\\._]+){2,}/[\\w\\-\\.\\-/=\\+_\\?]+\", \" \", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_html_and_url(data):\n",
    "#     \"\"\"Function to remove\n",
    "#              1. HTML encodings\n",
    "#              2. HTML tags (both closed and open)\n",
    "#              3. URLs\n",
    "\n",
    "#     Args:\n",
    "#         data (pd.Series): A Pandas series of type string\n",
    "\n",
    "#     Returns:\n",
    "#         _type_: pd.Series\n",
    "#     \"\"\"\n",
    "#     # Remove HTML encodings\n",
    "#     data.str.replace(r\"&#\\d+;\", \" \", regex=True)\n",
    "\n",
    "#     # Remove HTML tags (both open and closed)\n",
    "#     data.str.replace(r\"<[a-zA-Z]+\\s?/?>\", \" \", regex=True)\n",
    "\n",
    "#     # Remove URLs\n",
    "#     data.str.replace(r\"https?://([\\w\\-\\._]+){2,}/[\\w\\-\\.\\-/=\\+_\\?]+\", \" \", regex=True)\n",
    "\n",
    "#     return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def replace_digits_with_tag(data):  \n",
    "#   return data.str.replace(r\"\\d+\", \" NUM \", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove non-alphabetical characters\n",
    "# def remove_non_alpha_characters(data):\n",
    "#     return data.str.replace(r\"_+|\\\\|[^a-zA-Z0-9\\s]\", \" \", regex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove extra spaces\n",
    "# def remove_extra_spaces(data: pd.Series):\n",
    "#     return data.str.replace(r\"^\\s*|\\s\\s*\", \" \", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install contractions package, if you don't have it\n",
    "# ! pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expanding contractions\n",
    "# def fix_contractions(data: pd.Series):\n",
    "#     import contractions\n",
    "\n",
    "#     def contraction_fixer(txt: str):\n",
    "#         return \" \".join([contractions.fix(word) for word in txt.split()])\n",
    "\n",
    "#     return data.apply(contraction_fixer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dictionary containing the columns and a list of functions to perform on it in order\n",
    "# data_cleaning_pipeline = {\n",
    "#     DATA_COL: [\n",
    "#         to_lower,\n",
    "#         remove_html_encodings,\n",
    "#         remove_html_tags,\n",
    "#         remove_url,\n",
    "#         # fix_contractions,\n",
    "#         remove_non_alpha_characters,\n",
    "#         remove_extra_spaces,\n",
    "#     ]\n",
    "# }\n",
    "\n",
    "# cleaned_data = sampled_data.copy()\n",
    "\n",
    "# # Process all the cleaning instructions\n",
    "# for col, pipeline in data_cleaning_pipeline.items():\n",
    "#     # Get the column to perform cleaning on\n",
    "#     temp_data = cleaned_data[col].copy()\n",
    "\n",
    "#     # Perform all the cleaning functions sequencially\n",
    "#     for func in pipeline:\n",
    "#         print(f\"Starting: {func.__name__}\")\n",
    "#         temp_data = func(temp_data)\n",
    "#         print(f\"Ended: {func.__name__}\")\n",
    "\n",
    "#     # Replace the old column with cleaned one.\n",
    "#     cleaned_data[col] = temp_data.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize(data: pd.Series):\n",
    "#     from nltk.tokenize import word_tokenize\n",
    "\n",
    "#     nltk.download(\"punkt\")\n",
    "\n",
    "#     return data.apply(word_tokenize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_stopwords(data: pd.Series):\n",
    "#     \"\"\"Remove stop words using the NLTK stopwords dictionary\n",
    "\n",
    "#     Args:\n",
    "#         string (str): a document\n",
    "\n",
    "#     Returns:\n",
    "#         str: a document with stopwords removed\n",
    "#     \"\"\"\n",
    "#     from nltk.corpus import stopwords\n",
    "\n",
    "#     nltk.download(\"stopwords\")\n",
    "\n",
    "#     stopwords = set(stopwords.words())\n",
    "\n",
    "#     def remover(word_list: List[str], stopwords: Set[str]):\n",
    "#         return [word for word in word_list if not word in stopwords]\n",
    "\n",
    "#     return data.apply(lambda word_list: remover(word_list, stopwords))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def lemmatize(data: pd.Series, consider_pos_tag: bool = True):\n",
    "#     from nltk.corpus import wordnet\n",
    "#     from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#     nltk.download(\"omw-1.4\")\n",
    "\n",
    "#     # POS tagging\n",
    "#     def perform_nltk_pos_tag(data: pd.Series):\n",
    "#         from nltk import pos_tag\n",
    "\n",
    "#         nltk.download(\"averaged_perceptron_tagger\")\n",
    "\n",
    "#         return data.apply(pos_tag)\n",
    "\n",
    "#     # Convert POS tag to wordnet pos tags\n",
    "#     def wordnet_pos_tagger(tag: str):\n",
    "#         if tag.startswith(\"J\"):\n",
    "#             return wordnet.ADJ\n",
    "#         elif tag.startswith(\"V\"):\n",
    "#             return wordnet.VERB\n",
    "#         elif tag.startswith(\"N\"):\n",
    "#             return wordnet.NOUN\n",
    "#         elif tag.startswith(\"R\"):\n",
    "#             return wordnet.ADV\n",
    "#         else:\n",
    "#             return None\n",
    "\n",
    "#     lemmatizer = WordNetLemmatizer()\n",
    "#     lemmatized = list()\n",
    "\n",
    "#     if consider_pos_tag:\n",
    "#         pos_tagged_data = data.copy()\n",
    "#         pos_tagged_data = perform_nltk_pos_tag(data)\n",
    "\n",
    "#         for row in pos_tagged_data:\n",
    "\n",
    "#             lemmatized_row = list()\n",
    "\n",
    "#             if consider_pos_tag:\n",
    "#                 for word, tag in row:\n",
    "#                     wordnet_pos_tag = wordnet_pos_tagger(tag)\n",
    "\n",
    "#                     if wordnet_pos_tag is None:\n",
    "#                         lemmatized_row.append(word)\n",
    "#                     else:\n",
    "#                         result = lemmatizer.lemmatize(word, wordnet_pos_tag)\n",
    "#                         lemmatized_row.append(lemmatizer.lemmatize(word, wordnet_pos_tag))\n",
    "\n",
    "#             lemmatized.append(lemmatized_row)\n",
    "#     else:\n",
    "#         for row in data:\n",
    "#             lemmatized_row = list()\n",
    "\n",
    "#             for word in row:\n",
    "#                 lemmatized_row.append(lemmatizer.lemmatize(word))\n",
    "\n",
    "#             lemmatized.append(lemmatized_row)\n",
    "\n",
    "#     return pd.Series(lemmatized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate lemmatized sentences back into one sentence\n",
    "# def concatenate(data: pd.Series):\n",
    "#     return data.apply(lambda words: \" \".join(words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing_pipeline = {DATA_COL: [tokenize, lemmatize, concatenate]}\n",
    "\n",
    "# # Run the pipeline\n",
    "# preprocessed_data = cleaned_data.copy()\n",
    "\n",
    "# # Process all the cleaning instructions\n",
    "# for col, pipeline in preprocessing_pipeline.items():\n",
    "#     # Get the column to perform cleaning on\n",
    "#     temp_data = preprocessed_data[col].copy()\n",
    "\n",
    "#     # Perform all the cleaning functions sequencially\n",
    "#     for func in pipeline:\n",
    "#         print(f\"Starting: {func.__name__}\")\n",
    "\n",
    "#         if func.__name__ == \"lemmatize\":\n",
    "#             temp_data = func(temp_data, consider_pos_tag=True)\n",
    "#         else:\n",
    "#             temp_data = func(temp_data)\n",
    "\n",
    "#         print(f\"Ended: {func.__name__}\")\n",
    "\n",
    "#     # Replace the old column with cleaned one.\n",
    "#     preprocessed_data[col] = temp_data.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 64-bit ('CSCI544_assignment_03')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "99319869c2b84476c56e7423ed21631b8ddb5a8a9b4475198cf8370d36903f81"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
