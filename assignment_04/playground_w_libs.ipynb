{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import json\n",
    "from typing import List, Dict, Tuple, Set, Any\n",
    "\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Constants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "### Constants                                      ###\n",
    "######################################################\n",
    "# Base Paths\n",
    "INPUT_PATH = \"./data\"\n",
    "MODEL_PATH = \"./model\"\n",
    "\n",
    "# Model File names\n",
    "VANILLA_MODEL_FILENAME = \"vanillamodel.txt\"\n",
    "AVERAGED_MODEL_FILENAME = \"averagedmodel.txt\"\n",
    "\n",
    "# Class Identifiers\n",
    "TRUTHFUL = \"True\"\n",
    "DECEPTIVE = \"Fake\"\n",
    "POSITIVE = \"Pos\"\n",
    "NEGATIVE = \"Neg\"\n",
    "\n",
    "# File paths\n",
    "TRAIN_FILE_PATH = f\"{INPUT_PATH}/train-labeled.txt\"\n",
    "CLEANED_DATA_FILE_PATH = f\"{INPUT_PATH}/cleaned-data.txt\"\n",
    "PREPROCESSED_DATA_FILE_PATH = f\"{INPUT_PATH}/preprocessed-data.txt\"\n",
    "\n",
    "VANILLA_MODEL_FILE_PATH = f\"{MODEL_PATH}/{VANILLA_MODEL_FILENAME}\"\n",
    "AVERAGED_MODEL_FILE_PATH = f\"{MODEL_PATH}/{AVERAGED_MODEL_FILENAME}\"\n",
    "\n",
    "DEV_FILE_PATH = f\"{INPUT_PATH}/dev-text.txt\"\n",
    "DEV_TAGGED_FILE_PATH = f\"{INPUT_PATH}/dev-key.txt\"\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "DATA_COL = 3\n",
    "SENTIMENT_TARGET_COL = 2\n",
    "TRUTHFULNESS_TARGET_COL = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(seed=RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = \"07Zfn0z Fake Pos If you're looking for an elegant hotel in downtown Chicago, you have to stay here. The Ambassador East Hotel has very comfortable and beautiful large rooms and is like a home away from home. The perfect place for a business person, and if you have a small pet you can bring them too! I would give this place four stars and would definitely stay here again.\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_regex = re.compile(\"(\\w*) (\\w*) (\\w*) (.*)\\n?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('07Zfn0z',\n",
       " 'Fake',\n",
       " 'Pos',\n",
       " \"If you're looking for an elegant hotel in downtown Chicago, you have to stay here. The Ambassador East Hotel has very comfortable and beautiful large rooms and is like a home away from home. The perfect place for a business person, and if you have a small pet you can bring them too! I would give this place four stars and would definitely stay here again.\")"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.match(input_regex, line).groups()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('07Zfn0z',\n",
       "  'Fake',\n",
       "  'Pos',\n",
       "  \"If you're looking for an elegant hotel in downtown Chicago, you have to stay here. The Ambassador East Hotel has very comfortable and beautiful large rooms and is like a home away from home. The perfect place for a business person, and if you have a small pet you can bring them too! I would give this place four stars and would definitely stay here again.\")]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "data.append(re.match(input_regex, line).groups())\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Fake'"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(data)[0, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "def load_data(input_file_path: str, type: str = \"TRAIN\") -> npt.NDArray:\n",
    "    input_data = list()\n",
    "    regex = \"(\\w*) (\\w*) (\\w*) (.*)\\n?\"\n",
    "    if type == \"TEST\":\n",
    "        regex = \"(\\w*) (.*)\\n?\"\n",
    "    elif type == \"LABELS\":\n",
    "        regex = \"(\\w*) (\\w*) (\\w*)\\n?\"\n",
    "    input_regex = re.compile(regex)\n",
    "    with open(input_file_path, mode=\"r\") as input_file:\n",
    "        for line in input_file:\n",
    "            input_data.append(re.match(input_regex, line).groups())\n",
    "    return np.array(input_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store Data\n",
    "def store_data(date_file_path: str, data: npt.NDArray) -> None:\n",
    "    with open(date_file_path, mode=\"w\") as data_file:\n",
    "        for row in data:\n",
    "            data_file.write(f\"{row[0]} {row[1]} {row[2]} {row[3]}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store Model\n",
    "def store_model(model_file_path: str, model_data: Any) -> None:\n",
    "    # TODO: Need to check model_data\n",
    "    with open(model_file_path, mode=\"w\") as model_file:\n",
    "        json.dump(model_data, model_file, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model\n",
    "def load_model(model_file_path: str) -> npt.NDArray:\n",
    "    # TODO: Extract data from JSON and return that only\n",
    "    with open(model_file_path, mode=\"r\") as model_file:\n",
    "        model_data = json.load(model_file)\n",
    "    return model_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store Predictions\n",
    "def store_predictions(output_file_path: str, predictions: List[Tuple[str, str, str]]) -> None:\n",
    "    with open(output_file_path, mode=\"w\") as output_file:\n",
    "        for prediction in predictions:\n",
    "            output_file.write(f\"{prediction[0]} {prediction[1]} {prediction[2]}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Scores\n",
    "def calculate_scores(y_true, y_pred, title: str):\n",
    "    from sklearn.metrics import classification_report\n",
    "\n",
    "    print(f\"------------------------ {title} ------------------------\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(\"--------------------------------------------------------------------------------\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data(TRAIN_FILE_PATH, type=\"TRAIN\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(960,)"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:, 3].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all reviews to lower case (optional according to study)\n",
    "def to_lower(data: npt.NDArray):\n",
    "    n_data = data.shape[0]\n",
    "    result = data.copy()\n",
    "    for i in range(n_data):\n",
    "        result[i] = result[i].lower()\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_encodings(data: npt.NDArray):\n",
    "    import re\n",
    "\n",
    "    n_data = data.shape[0]\n",
    "    result = data.copy()\n",
    "    for i in range(n_data):\n",
    "        result[i] = re.sub(r\"&#\\d+;\", \" \", result[i])\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_tags(data: npt.NDArray):\n",
    "    import re\n",
    "\n",
    "    n_data = data.shape[0]\n",
    "    result = data.copy()\n",
    "    for i in range(n_data):\n",
    "        result[i] = re.sub(r\"<[a-zA-Z]+\\s?/?>\", \"\", result[i])\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(data: npt.NDArray):\n",
    "    import re\n",
    "\n",
    "    n_data = data.shape[0]\n",
    "    result = data.copy()\n",
    "    for i in range(n_data):\n",
    "        result[i] = re.sub(r\"https?://([\\w\\-\\._]+){2,}/[\\w\\-\\.\\-/=\\+_\\?]+\", \"\", result[i])\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_and_url(data):\n",
    "    \"\"\"Function to remove\n",
    "             1. HTML encodings\n",
    "             2. HTML tags (both closed and open)\n",
    "             3. URLs\n",
    "\n",
    "    Args:\n",
    "        data (npt.NDArray): A Numpy Array of type string\n",
    "\n",
    "    Returns:\n",
    "        _type_: npt.NDArray\n",
    "    \"\"\"\n",
    "    import re\n",
    "\n",
    "    n_data = data.shape[0]\n",
    "    result = data.copy()\n",
    "    for i in range(n_data):\n",
    "        # Remove HTML encodings\n",
    "        result[i] = re.sub(r\"&#\\d+;\", \"\", result[i])\n",
    "\n",
    "        # Remove HTML tags (both open and closed)\n",
    "        result[i] = re.sub(r\"<[a-zA-Z]+\\s?/?>\", \"\", result[i])\n",
    "\n",
    "        # Remove URLs\n",
    "        result[i] = re.sub(r\"https?://([\\w\\-\\._]+){2,}/[\\w\\-\\.\\-/=\\+_\\?]+\", \"\", result[i])\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_digits_with_tag(data: npt.NDArray):\n",
    "    import re\n",
    "\n",
    "    n_data = data.shape[0]\n",
    "    result = data.copy()\n",
    "    for i in range(n_data):\n",
    "        result[i] = re.sub(r\"\\d+\", \" NUM \", result[i])\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove non-alphabetical characters\n",
    "def remove_non_alpha_characters(data: npt.NDArray):\n",
    "    import re\n",
    "\n",
    "    n_data = data.shape[0]\n",
    "    result = data.copy()\n",
    "    for i in range(n_data):\n",
    "        result[i] = re.sub(r\"_+|\\\\|[^a-zA-Z0-9\\s]\", \" \", result[i])\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove extra spaces\n",
    "def remove_extra_spaces(data: npt.NDArray):\n",
    "    import re\n",
    "\n",
    "    n_data = data.shape[0]\n",
    "    result = data.copy()\n",
    "    for i in range(n_data):\n",
    "        result[i] = re.sub(r\"^\\s*|\\s\\s*\", \" \", result[i])\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install contractions package, if you don't have it\n",
    "# ! pip install contractions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expanding contractions\n",
    "def fix_contractions(data: npt.NDArray):\n",
    "    import contractions\n",
    "\n",
    "    def contraction_fixer(txt: str):\n",
    "        return \" \".join([contractions.fix(word) for word in txt.split()])\n",
    "\n",
    "    n_data = data.shape[0]\n",
    "    result = data.copy()\n",
    "    for i in range(n_data):\n",
    "        result[i] = contraction_fixer(result[i])\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting: to_lower\n",
      "Ended: to_lower\n",
      "Starting: remove_html_encodings\n",
      "Ended: remove_html_encodings\n",
      "Starting: remove_html_tags\n",
      "Ended: remove_html_tags\n",
      "Starting: remove_url\n",
      "Ended: remove_url\n",
      "Starting: fix_contractions\n",
      "Ended: fix_contractions\n",
      "Starting: remove_non_alpha_characters\n",
      "Ended: remove_non_alpha_characters\n",
      "Starting: remove_extra_spaces\n",
      "Ended: remove_extra_spaces\n"
     ]
    }
   ],
   "source": [
    "# A dictionary containing the columns and a list of functions to perform on it in order\n",
    "data_cleaning_pipeline = {\n",
    "    DATA_COL: [\n",
    "        to_lower,\n",
    "        remove_html_encodings,\n",
    "        remove_html_tags,\n",
    "        remove_url,\n",
    "        fix_contractions,\n",
    "        remove_non_alpha_characters,\n",
    "        remove_extra_spaces,\n",
    "    ]\n",
    "}\n",
    "\n",
    "cleaned_data = data.copy()\n",
    "\n",
    "# Process all the cleaning instructions\n",
    "for col, pipeline in data_cleaning_pipeline.items():\n",
    "    # Get the column to perform cleaning on\n",
    "    temp_data = cleaned_data[:, col].copy()\n",
    "\n",
    "    # Perform all the cleaning functions sequencially\n",
    "    for func in pipeline:\n",
    "        print(f\"Starting: {func.__name__}\")\n",
    "        temp_data = func(temp_data)\n",
    "        print(f\"Ended: {func.__name__}\")\n",
    "\n",
    "    # Replace the old column with cleaned one.\n",
    "    cleaned_data[:, col] = temp_data.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([' if you are looking for an elegant hotel in downtown chicago you have to stay here the ambassador east hotel has very comfortable and beautiful large rooms and is like a home away from home the perfect place for a business person and if you have a small pet you can bring them too i would give this place four stars and would definitely stay here again ',\n",
       "       ' the atmosphere at the talbott hotel is very welcoming when you first walk in the employees are friendly and will greet you when you first arrive the hotel seems to be luxurious even from the lobby and right away i was very excited for my stay i stayed in a double queen room with my friend the room is spacious enough and beds are very comfortable i did not try the room service but they do have a fitness club close by with a spa which is very convenient i also love how the room has individual climate controls very neat also handy that there is an ihome in the room i am usually stuck using just a regular alarm clock radio at other hotels i will most likely stay in this hotel another time it was a wonderful experience ',\n",
       "       ' i would been searching for a cool non chain hotel for a weekend getaway with my boyfriend i thought i would found a winner in the affinia soo disappointed in spite of the fact that our reservation was for two adults we arrived to find our room had a double bed what i was floored but after a delayed flight and a 2 hour cab ride was too tired to make a fuss we were hungry after all the delays and planned on attending an event that night so we called for room service hoping to keep it simple i was told that room service would call me back to take my order and nothing happened i did not want to be late to our event so we ended up going to the restaurant downstairs nothing special entree dessert and nab for two still came out to i am no hayseed i am well aware that everything costs more in chicago but this just was not worth it our room was by the elevator w parking garage view the bathroom was tiny shower curtain was stained funky toilet seat lid and the vanity featured that rude awakening lighting normally found in low rent dressing rooms i e holy crap have i looked that bad this whole time if you are a traveler who likes using the in room coffeemaker they charge for their chintzy coffees i do not use these but come on nice enough ambience in the upstairs bar but weak drinks that night our bed was so tiny and uncomfortable that i almost asked to switch rooms but the marathon was that weekend the hotel was so busy i did not want to be a pest i cannot believe i was so impressed with this place online next time i will happily pay more to stay someplace that is actually a treat ',\n",
       "       ' i vacationed at the fairmont chicago for 3 nights in july 2009 the room the bathroom the view and the staff were all wonderful i just wish the hotel were a few blocks north on mi ave sometimes when you are tired that extra couple of blocks seem like miles ',\n",
       "       ' the fairmont chicago millennium park has to be one of the most overpriced hotels i have stayed at in a while at almost 500 a night one would expect traveling with my wife i expected to pay the cost of the room already being outrageous but then i was hit with another 20 charge for her to stay even with a king bed they only figure in one person to their cost the beds were average and although there was much to do everything cost an arm or a leg it was unbelievable how much they charge for everything and to see people paying it like it was normal blew me away all in all for the amount of money that i was paying on a per night basis if i was in chicago for any longer than that one night i definitely would have stayed somewhere else i guess you live and you learn its just a shame that i had to learn with such a high bill for very average accommodations '],\n",
       "      dtype='<U4158')"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data[:5, 3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_data(CLEANED_DATA_FILE_PATH, cleaned_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['07Zfn0z', 'Fake', 'Pos',\n",
       "       ' if you are looking for an elegant hotel in downtown chicago you have to stay here the ambassador east hotel has very comfortable and beautiful large rooms and is like a home away from home the perfect place for a business person and if you have a small pet you can bring them too i would give this place four stars and would definitely stay here again '],\n",
       "      dtype='<U4049')"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Data\n",
    "preprocessed_data = load_data(CLEANED_DATA_FILE_PATH)\n",
    "preprocessed_data[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(data: npt.NDArray):\n",
    "    import nltk\n",
    "    from nltk.tokenize import word_tokenize\n",
    "\n",
    "    nltk.download(\"punkt\")\n",
    "\n",
    "    n_data = data.shape[0]\n",
    "    result = list()\n",
    "    for i in range(n_data):\n",
    "        result.append(word_tokenize(data[i]))\n",
    "    return np.array(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(data: npt.NDArray):\n",
    "    \"\"\"Remove stop words using the NLTK stopwords dictionary\n",
    "\n",
    "    Args:\n",
    "        string (str): a document\n",
    "\n",
    "    Returns:\n",
    "        str: a document with stopwords removed\n",
    "    \"\"\"\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "\n",
    "    nltk.download(\"stopwords\")\n",
    "\n",
    "    stopwords = set(stopwords.words())\n",
    "\n",
    "    def remover(word_list: List[str], stopwords: Set[str]):\n",
    "        return [word for word in word_list if not word in stopwords]\n",
    "\n",
    "    n_data = data.shape[0]\n",
    "    result = data.copy()\n",
    "    for i in range(n_data):\n",
    "        result[i] = remover(result[i], stopwords)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate lemmatized sentences back into one sentence\n",
    "def concatenate(data: npt.NDArray):\n",
    "    n_data = data.shape[0]\n",
    "    result = list()\n",
    "    for i in range(n_data):\n",
    "        result.append(\" \".join(data[i]))\n",
    "    return np.array(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing_pipeline = {DATA_COL: [tokenize, remove_stopwords, concatenate]}\n",
    "\n",
    "# # Run the pipeline\n",
    "# preprocessed_data = cleaned_data.copy()\n",
    "\n",
    "# # Process all the cleaning instructions\n",
    "# for col, pipeline in preprocessing_pipeline.items():\n",
    "#     # Get the column to perform cleaning on\n",
    "#     temp_data = preprocessed_data[:, col].copy()\n",
    "\n",
    "#     # Perform all the cleaning functions sequencially\n",
    "#     for func in pipeline:\n",
    "#         print(f\"Starting: {func.__name__}\")\n",
    "\n",
    "#         temp_data = func(temp_data)\n",
    "\n",
    "#         print(f\"Ended: {func.__name__}\")\n",
    "\n",
    "#     # Replace the old column with cleaned one.\n",
    "#     preprocessed_data[:, col] = temp_data.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store_data(PREPROCESSED_DATA_FILE_PATH, preprocessed_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessed_data[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data(CLEANED_DATA_FILE_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data 80-20 split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(data, test_size=0.2, stratify=data[:, SENTIMENT_TARGET_COL], random_state=RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((768, 4), (192, 4))"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/aditya/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "/Users/aditya/.pyenv/versions/3.10.6/envs/csci544/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:524: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfVectorizer(tokenizer=&lt;function word_tokenize at 0x16cbfba30&gt;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(tokenizer=&lt;function word_tokenize at 0x16cbfba30&gt;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "TfidfVectorizer(tokenizer=<function word_tokenize at 0x16cbfba30>)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=word_tokenize)\n",
    "vectorizer.fit(data[:, DATA_COL])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Perceptron\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Sentiment Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------ Sentiment Analysis -- Train ------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Neg       1.00      1.00      1.00       384\n",
      "         Pos       1.00      1.00      1.00       384\n",
      "\n",
      "    accuracy                           1.00       768\n",
      "   macro avg       1.00      1.00      1.00       768\n",
      "weighted avg       1.00      1.00      1.00       768\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "------------------------ Sentiment Analysis -- Test ------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Neg       0.95      0.95      0.95        96\n",
      "         Pos       0.95      0.95      0.95        96\n",
      "\n",
      "    accuracy                           0.95       192\n",
      "   macro avg       0.95      0.95      0.95       192\n",
      "weighted avg       0.95      0.95      0.95       192\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "X_tfidf_train = vectorizer.transform(train[:, DATA_COL])\n",
    "X_tfidf_test = vectorizer.transform(test[:, DATA_COL])\n",
    "y_train = train[:, SENTIMENT_TARGET_COL]\n",
    "y_test = test[:, SENTIMENT_TARGET_COL]\n",
    "\n",
    "clf = Perceptron(max_iter=10, alpha=1, random_state=RANDOM_SEED, tol=1e-4, early_stopping=True)  # 0.45103975891511683\n",
    "\n",
    "clf.fit(X_tfidf_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_tfidf_test)\n",
    "\n",
    "calculate_scores(y_train, clf.predict(X_tfidf_train), title=\"Sentiment Analysis -- Train\")\n",
    "calculate_scores(y_test, y_pred, title=\"Sentiment Analysis -- Test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Truthfulness Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------ Truthfulness Analysis -- Train ------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fake       0.99      0.98      0.99       387\n",
      "        True       0.98      0.99      0.99       381\n",
      "\n",
      "    accuracy                           0.99       768\n",
      "   macro avg       0.99      0.99      0.99       768\n",
      "weighted avg       0.99      0.99      0.99       768\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "------------------------ Truthfulness Analysis -- Test ------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fake       0.82      0.83      0.82        93\n",
      "        True       0.84      0.83      0.83        99\n",
      "\n",
      "    accuracy                           0.83       192\n",
      "   macro avg       0.83      0.83      0.83       192\n",
      "weighted avg       0.83      0.83      0.83       192\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "X_tfidf_train = vectorizer.transform(train[:, DATA_COL])\n",
    "X_tfidf_test = vectorizer.transform(test[:, DATA_COL])\n",
    "y_train = train[:, TRUTHFULNESS_TARGET_COL]\n",
    "y_test = test[:, TRUTHFULNESS_TARGET_COL]\n",
    "\n",
    "clf = Perceptron(max_iter=100, alpha=8e-3, random_state=RANDOM_SEED, early_stopping=True)  # 0.45103975891511683\n",
    "\n",
    "clf.fit(X_tfidf_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_tfidf_test)\n",
    "\n",
    "calculate_scores(y_train, clf.predict(X_tfidf_train), title=\"Truthfulness Analysis -- Train\")\n",
    "calculate_scores(y_test, y_pred, title=\"Truthfulness Analysis -- Test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Averaged Perceptron\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## No Libs Available\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit ('csci544')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e78917e7f90f3892e7e12462ef46781cf5994bd706032ea53be00d0b1f29dcb9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
