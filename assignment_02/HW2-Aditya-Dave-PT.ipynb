{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow multiple print statements in a cell in Jupyter Notebook\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install some dependencies\n",
    "! pip install emot contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"/Volumes/dataTwo/usc/CSCI_544/assignment_02/data\"\n",
    "MODEL_PATH = \"/Volumes/dataTwo/usc/CSCI_544/assignment_02/model\"\n",
    "\n",
    "ORIGINAL_DATA_FILE = \"amazon_reviews_us_Jewelry_v1_00.tsv\"\n",
    "SAMPLED_DATA_FILE = \"data_sampled.csv\"\n",
    "CLEANED_DATA_FILE = \"data_cleaned.csv\"\n",
    "\n",
    "# Files after clean and split\n",
    "DATA_FILE = \"data.csv\"\n",
    "\n",
    "# Files after preprocessing the splitted dataset\n",
    "PREPROCESSED_DATA_FILE = \"data_preprocessed.csv\"\n",
    "\n",
    "# Files containing the tfidf data\n",
    "TFIDF_DATA_FILE = \"data_tfidf.csv\"\n",
    "\n",
    "# custom created word vectors for the review dataset\n",
    "CUSTOM_WORD_VECTORS_MODEL_FILE = \"gensim_w2v_amazon_reviews_model\"\n",
    "\n",
    "# train and test data for word2vec avg. word vectors approach\n",
    "AVG_WORD_VECTORS_DATA_FILE = \"data_avg_word_vectors.pkl\"\n",
    "\n",
    "# train and test data for word2vec contatenate top 10 vectors appraoch\n",
    "TOP_10_WORD_VECTORS_DATA_FILE = \"data_avg_word_vectors.pkl\"\n",
    "\n",
    "DATA_COL = \"review_body\"\n",
    "TARGET_COL = \"star_rating\"\n",
    "\n",
    "N_SAMPLES = 20000\n",
    "\n",
    "RANDOM_SEED = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(RANDOM_SEED)\n",
    "rng = np.random.default_rng(seed=RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tab separated data file, and print the first 5 rows for confirmation\n",
    "data = pd.read_csv(f\"{DATA_PATH}/{ORIGINAL_DATA_FILE}\", sep=\"\\t\", usecols=[TARGET_COL, DATA_COL], low_memory=True)\n",
    "data.head()\n",
    "\n",
    "# Drop NA values\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Drop the outlier which is star_rating = \"2012-12-21\"\n",
    "data = data[data.star_rating != \"2012-12-21\"]\n",
    "\n",
    "# Convert all star rating to integer\n",
    "data[TARGET_COL] = data.star_rating.astype(int)\n",
    "\n",
    "# Remove nan valued rows\n",
    "data = data[data.review_body.notnull()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_data = data.groupby(TARGET_COL, group_keys=False).apply(lambda x: x.sample(N_SAMPLES, random_state=RANDOM_SEED))\n",
    "sampled_data.reset_index(inplace=True)\n",
    "sampled_data.drop(columns=[\"index\"], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data without cleaning\n",
    "sampled_data.to_csv(f\"{DATA_PATH}/{SAMPLED_DATA_FILE}\", header=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all reviews to lower case (optional according to study)\n",
    "def to_lower(data: pd.Series):\n",
    "    return data.str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_accented_characters(data: pd.Series):\n",
    "    import unicodedata\n",
    "\n",
    "    \"\"\"Removes accented characters from the Series\n",
    "\n",
    "    Args:\n",
    "        data (pd.Series): Series of string\n",
    "\n",
    "    Returns:\n",
    "        _type_: pd.Series\n",
    "    \"\"\"\n",
    "    import unicodedata\n",
    "\n",
    "    return data.apply(lambda x: unicodedata.normalize(\"NFKD\", x).encode(\"ascii\", \"ignore\").decode(\"utf-8\", \"ignore\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_encodings(data: pd.Series):\n",
    "    return data.str.replace(r\"&#\\d+;\", \" \", regex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_tags(data: pd.Series):\n",
    "    return data.str.replace(r\"<[a-zA-Z]+\\s?/?>\", \" \", regex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(data: pd.Series):\n",
    "    return data.str.replace(r\"https?://([\\w\\-\\._]+){2,}/[\\w\\-\\.\\-/=\\+_\\?]+\", \" \", regex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_and_url(data: pd.Series):\n",
    "    \"\"\"Function to remove\n",
    "             1. HTML encodings\n",
    "             2. HTML tags (both closed and open)\n",
    "             3. URLs\n",
    "\n",
    "    Args:\n",
    "        data (pd.Series): A Pandas series of type string\n",
    "\n",
    "    Returns:\n",
    "        _type_: pd.Series\n",
    "    \"\"\"\n",
    "    # Remove HTML encodings\n",
    "    data.str.replace(r\"&#\\d+;\", \" \", regex=True)\n",
    "\n",
    "    # Remove HTML tags (both open and closed)\n",
    "    data.str.replace(r\"<[a-zA-Z]+\\s?/?>\", \" \", regex=True)\n",
    "\n",
    "    # Remove URLs\n",
    "    data.str.replace(r\"https?://([\\w\\-\\._]+){2,}/[\\w\\-\\.\\-/=\\+_\\?]+\", \" \", regex=True)\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle emoji\n",
    "def convert_emoji_to_txt(data: pd.Series):\n",
    "    from emot.emo_unicode import UNICODE_EMOJI, EMOTICONS_EMO\n",
    "\n",
    "    EMO_TO_TXT_DICT = dict()\n",
    "    for emot in UNICODE_EMOJI:\n",
    "        EMO_TO_TXT_DICT[emot] = f\" {re.sub(r',|:|_', '', UNICODE_EMOJI[emot])} \"\n",
    "\n",
    "    for emo in EMOTICONS_EMO:\n",
    "        EMO_TO_TXT_DICT[emot] = f\" {re.sub(r',| ', '', EMOTICONS_EMO[emo])} \"\n",
    "\n",
    "    def convert_emojis(text, emo_to_txt_dict):\n",
    "        for emot in emo_to_txt_dict:\n",
    "            text = text.replace(emot, emo_to_txt_dict[emot])\n",
    "        return text\n",
    "\n",
    "    return data.apply(lambda x: convert_emojis(x, EMO_TO_TXT_DICT))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove non-alphabetical characters\n",
    "def remove_non_alpha_characters(data: pd.Series):\n",
    "    return data.str.replace(r\"_+|\\\\|[^a-zA-Z0-9\\s]\", \" \", regex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove extra spaces\n",
    "def remove_extra_spaces(data: pd.Series):\n",
    "    return data.str.replace(r\"^\\s*|\\s\\s*\", \" \", regex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expanding contractions\n",
    "def fix_contractions(data: pd.Series):\n",
    "    import contractions\n",
    "\n",
    "    def contraction_fixer(txt: str):\n",
    "        return \" \".join([contractions.fix(word) for word in txt.split()])\n",
    "\n",
    "    return data.apply(contraction_fixer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dictionary containing the columns and a list of functions to perform on it in order\n",
    "data_cleaning_pipeline = {\n",
    "    DATA_COL: [\n",
    "        convert_emoji_to_txt,\n",
    "        to_lower,\n",
    "        remove_accented_characters,\n",
    "        remove_html_encodings,\n",
    "        remove_html_tags,\n",
    "        remove_url,\n",
    "        fix_contractions,\n",
    "        remove_non_alpha_characters,\n",
    "        remove_extra_spaces,\n",
    "    ]\n",
    "}\n",
    "\n",
    "cleaned_data = sampled_data.copy()\n",
    "\n",
    "# Process all the cleaning instructions\n",
    "for col, pipeline in data_cleaning_pipeline.items():\n",
    "    # Get the column to perform cleaning on\n",
    "    temp_data = cleaned_data[col].copy()\n",
    "\n",
    "    # Perform all the cleaning functions sequencially\n",
    "    for func in pipeline:\n",
    "        print(f\"Starting: {func.__name__}\")\n",
    "        temp_data = func(temp_data)\n",
    "        print(f\"Ended: {func.__name__}\")\n",
    "\n",
    "    # Replace the old column with cleaned one.\n",
    "    cleaned_data[col] = temp_data.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store data file\n",
    "cleaned_data.to_csv(f\"{DATA_PATH}/{CLEANED_DATA_FILE}\", sep=\",\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing (used by TF-IDF Models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(data: pd.Series):\n",
    "    from nltk.tokenize import word_tokenize\n",
    "\n",
    "    nltk.download(\"punkt\")\n",
    "\n",
    "    return data.apply(word_tokenize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Set\n",
    "\n",
    "\n",
    "def remove_stopwords(data: pd.Series):\n",
    "    \"\"\"Remove stop words using the NLTK stopwords dictionary\n",
    "\n",
    "    Args:\n",
    "        string (str): a document\n",
    "\n",
    "    Returns:\n",
    "        str: a document with stopwords removed\n",
    "    \"\"\"\n",
    "    from nltk.corpus import stopwords\n",
    "\n",
    "    nltk.download(\"stopwords\")\n",
    "\n",
    "    stopwords = set(stopwords.words())\n",
    "\n",
    "    def remover(word_list: List[str], stopwords: Set[str]):\n",
    "        return [word for word in word_list if not word in stopwords]\n",
    "\n",
    "    return data.apply(lambda word_list: remover(word_list, stopwords))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(data: pd.Series, consider_pos_tag: bool = True):\n",
    "    from nltk.corpus import wordnet\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "    nltk.download(\"omw-1.4\")\n",
    "\n",
    "    # POS tagging\n",
    "    def perform_nltk_pos_tag(data: pd.Series):\n",
    "        from nltk import pos_tag\n",
    "\n",
    "        nltk.download(\"averaged_perceptron_tagger\")\n",
    "\n",
    "        return data.apply(pos_tag)\n",
    "\n",
    "    # Convert POS tag to wordnet pos tags\n",
    "    def wordnet_pos_tagger(tag: str):\n",
    "        if tag.startswith(\"J\"):\n",
    "            return wordnet.ADJ\n",
    "        elif tag.startswith(\"V\"):\n",
    "            return wordnet.VERB\n",
    "        elif tag.startswith(\"N\"):\n",
    "            return wordnet.NOUN\n",
    "        elif tag.startswith(\"R\"):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = list()\n",
    "\n",
    "    if consider_pos_tag:\n",
    "        pos_tagged_data = data.copy()\n",
    "        pos_tagged_data = perform_nltk_pos_tag(data)\n",
    "\n",
    "        for row in pos_tagged_data:\n",
    "\n",
    "            lemmatized_row = list()\n",
    "\n",
    "            if consider_pos_tag:\n",
    "                for word, tag in row:\n",
    "                    wordnet_pos_tag = wordnet_pos_tagger(tag)\n",
    "\n",
    "                    if wordnet_pos_tag is None:\n",
    "                        lemmatized_row.append(word)\n",
    "                    else:\n",
    "                        result = lemmatizer.lemmatize(word, wordnet_pos_tag)\n",
    "                        lemmatized_row.append(lemmatizer.lemmatize(word, wordnet_pos_tag))\n",
    "\n",
    "            lemmatized.append(lemmatized_row)\n",
    "    else:\n",
    "        for row in data:\n",
    "            lemmatized_row = list()\n",
    "\n",
    "            for word in row:\n",
    "                lemmatized_row.append(lemmatizer.lemmatize(word))\n",
    "\n",
    "            lemmatized.append(lemmatized_row)\n",
    "\n",
    "    return pd.Series(lemmatized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate lemmatized sentences back into one sentence\n",
    "def concatenate(data: pd.Series):\n",
    "    return data.apply(lambda words: \" \".join(words))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data 80-20 split\n",
    "import pickle as pkl\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# load clean data\n",
    "cleaned_data = pd.read_csv(f\"{DATA_PATH}/{CLEANED_DATA_FILE}\")\n",
    "\n",
    "# split cleaned data into train and test\n",
    "train, test = train_test_split(cleaned_data, test_size=0.2, stratify=cleaned_data[TARGET_COL], random_state=RANDOM_SEED)\n",
    "\n",
    "# save the split\n",
    "with open(f\"{DATA_PATH}/{DATA_FILE}\", mode=\"wb\") as file:\n",
    "    pkl.dump((train, test), file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "train, test = None, None\n",
    "with open(f\"{DATA_PATH}/{DATA_FILE}\", mode=\"rb\") as file:\n",
    "    train, test = pkl.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "preprocessing_pipeline = {DATA_COL: [tokenize, lemmatize, concatenate]}\n",
    "\n",
    "# Run the pipeline\n",
    "preprocessed_train_data = train.copy()\n",
    "preprocessed_test_data = test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "preprocessing_pipeline = {DATA_COL: [tokenize, lemmatize, concatenate]}\n",
    "\n",
    "# Run the pipeline\n",
    "preprocessed_train_data = train.copy()\n",
    "preprocessed_test_data = test.copy()\n",
    "\n",
    "# Process all the cleaning instructions\n",
    "for col, pipeline in preprocessing_pipeline.items():\n",
    "    # Get the column to perform cleaning on\n",
    "    temp_data_train = preprocessed_train_data[col].copy()\n",
    "    temp_data_test = preprocessed_test_data[col].copy()\n",
    "\n",
    "    # Perform all the cleaning functions sequencially\n",
    "    for func in pipeline:\n",
    "        print(f\"Starting: {func.__name__}\")\n",
    "\n",
    "        if func.__name__ == \"lemmatize\":\n",
    "            temp_data_train = func(temp_data_train, consider_pos_tag=True)\n",
    "            temp_data_test = func(temp_data_test, consider_pos_tag=True)\n",
    "        else:\n",
    "            temp_data_train = func(temp_data_train)\n",
    "            temp_data_test = func(temp_data_test)\n",
    "\n",
    "        print(f\"Ended: {func.__name__}\")\n",
    "\n",
    "    # Replace the old column with cleaned one.\n",
    "    preprocessed_train_data[col] = temp_data_train.copy()\n",
    "    preprocessed_test_data[col] = temp_data_test.copy()\n",
    "\n",
    "# Remove empty reviews\n",
    "preprocessed_train_data = preprocessed_train_data[preprocessed_train_data[DATA_COL].str.len() != 0]\n",
    "preprocessed_test_data = preprocessed_test_data[preprocessed_test_data[DATA_COL].str.len() != 0]\n",
    "\n",
    "# Remove NaN\n",
    "preprocessed_train_data.dropna(inplace=True)\n",
    "preprocessed_test_data.dropna(inplace=True)\n",
    "\n",
    "# Save the preprocessed data\n",
    "with open(f\"{DATA_PATH}/{PREPROCESSED_DATA_FILE}\", mode=\"wb\") as file:\n",
    "    pkl.dump((preprocessed_train_data, preprocessed_test_data), file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(y_true, y_pred, avg_type=\"macro\"):\n",
    "    from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "    precision = precision_score(y_true, y_pred, average=avg_type)\n",
    "    recall = recall_score(y_true, y_pred, average=avg_type)\n",
    "    f1 = f1_score(y_true, y_pred, average=avg_type)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    return precision, recall, f1, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metics(precision, recall, f1, accuracy):\n",
    "    print(f\"Avg. Precision: {precision}\")\n",
    "    print(f\"Avg. Recall: {recall}\")\n",
    "    print(f\"Avg. F1: {f1}\")\n",
    "    print(f\"Accuray Score: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load `word2vec-google-news-300` Model\n",
    "\n",
    "Learn how to extract word embeddings for your dataset. Try to check semantic similarities of the generated vectors using three examples of your own, e.g., King − Man + Woman = Queen or excellent ∼ outstanding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "w2v_google = api.load(\"word2vec-google-news-300\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_google.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"], topn=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_google.similarity(\"excellent\", \"outstanding\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three Semantic Similarity Examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_google.similarity(\"worst\", \"terrible\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_google.similarity(\"cheap\", \"disappointed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_google.most_similar(positive=[\"cheap\", \"worst\"], negative=[\"costly\"], topn=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Word2Vec on own Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "from gensim import utils\n",
    "\n",
    "\n",
    "class AmazonReviewCorpus:\n",
    "    def __iter__(self):\n",
    "        data = pd.read_csv(f\"{DATA_PATH}/{CLEANED_DATA_FILE}\", sep=\",\")\n",
    "\n",
    "        for review_body in data[\"review_body\"]:\n",
    "            yield utils.simple_preprocess(review_body)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim import utils\n",
    "\n",
    "w2v_custom = Word2Vec(vector_size=300, min_count=10, window=11, workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the vocabulary\n",
    "# reviews_data = pd.read_csv(f\"{DATA_PATH}/{CLEANED_DATA_FILE}\", sep=\",\")\n",
    "# reviews = reviews_data[\"review_body\"].apply(utils.simple_preprocess)\n",
    "reviews = AmazonReviewCorpus()\n",
    "\n",
    "w2v_custom.build_vocab(reviews, progress_per=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "w2v_custom.train(reviews, total_examples=w2v_custom.corpus_count, epochs=w2v_custom.epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "w2v_custom.save(f\"{MODEL_PATH}/{CUSTOM_WORD_VECTORS_MODEL_FILE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using trained model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "w2v_custom = Word2Vec.load(f\"{MODEL_PATH}/{CUSTOM_WORD_VECTORS_MODEL_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_custom.wv.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"], topn=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_custom.wv.similarity(\"excellent\", \"outstanding\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three Semantic Similarity Examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_google.similarity(\"worst\", \"terrible\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_google.similarity(\"cheap\", \"disappointed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_google.most_similar(positive=[\"cheap\", \"worst\"], negative=[\"costly\"], topn=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check semantic similarities from words used in earlier part.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you conclude from comparing vectors generated by yourself and the pretrained model? Which of the Word2Vec models seems to encode semantic similarities between words better?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avg. Word Vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the average Word2Vec vectors for each review as the input feature (x = N1 􏰀Ni=1 Wi for a review with N words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_avg_word_vector(words: List[str], w2v_model):\n",
    "    result_vector = np.ndarray(shape=(300,), buffer=np.zeros((300,)), dtype=float)\n",
    "    removed_word_count = 0\n",
    "    for word in words:\n",
    "        try:\n",
    "            result_vector += w2v_model[word]\n",
    "        except KeyError:\n",
    "            removed_word_count += 1\n",
    "    return result_vector / (len(words) - removed_word_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Perform Word2Vec conversion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned but not preprocessed data\n",
    "import pickle as pkl\n",
    "\n",
    "train_wv, test_wv = None, None\n",
    "with open(f\"{DATA_PATH}/{DATA_FILE}\", mode=\"rb\") as file:\n",
    "    train_wv, test_wv = pkl.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "# Preprocess using gensim'simple_preprocess\n",
    "train_wv[\"review_body\"] = train_wv[\"review_body\"].apply(simple_preprocess)\n",
    "test_wv[\"review_body\"] = test_wv[\"review_body\"].apply(simple_preprocess)\n",
    "\n",
    "TEMP_COL = \"avg_word_vector\"\n",
    "VECTOR_COLS = [f\"vector_{i}\" for i in range(300)]\n",
    "\n",
    "# Train Data\n",
    "train_wv[TEMP_COL] = train_wv[DATA_COL].apply(partial(calculate_avg_word_vector, w2v_model=w2v_google))\n",
    "\n",
    "wv_df = pd.DataFrame(train_wv[TEMP_COL].to_list(), index=train_wv[TEMP_COL].index, columns=VECTOR_COLS)\n",
    "wv_df.dropna(inplace=True)\n",
    "\n",
    "train_wv = pd.concat([train_wv, wv_df], axis=1)\n",
    "train_wv.dropna(inplace=True)\n",
    "\n",
    "X_wv_train = train_wv.drop([DATA_COL, TEMP_COL, TARGET_COL], axis=1)\n",
    "y_wv_train = train_wv[TARGET_COL]\n",
    "\n",
    "\n",
    "# Test Data\n",
    "test_wv[TEMP_COL] = test[DATA_COL].apply(partial(calculate_avg_word_vector, w2v_model=w2v_google))\n",
    "\n",
    "wv_df = pd.DataFrame(test_wv[TEMP_COL].to_list(), index=test_wv[TEMP_COL].index, columns=VECTOR_COLS)\n",
    "wv_df.dropna(inplace=True)\n",
    "\n",
    "test_wv = pd.concat([test_wv, wv_df], axis=1)\n",
    "test_wv.dropna(inplace=True)\n",
    "\n",
    "X_wv_test = train_wv.drop([DATA_COL, TEMP_COL, TARGET_COL], axis=1)\n",
    "y_wv_test = train_wv[TARGET_COL]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the avg'ed word vectors dataset\n",
    "import pickle as pkl\n",
    "\n",
    "with open(f\"{DATA_PATH}/{AVG_WORD_VECTORS_DATA_FILE}\", mode=\"wb\") as file:\n",
    "    pkl.dump((X_wv_train, y_wv_train, X_wv_test, y_wv_test), file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report your accuracy values on the testing split for these models similar to HW1, i.e., for each of perceptron and SVM models, report two accuracy values Word2Vec and TF-IDF features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train and test data\n",
    "import pickle as pkl\n",
    "\n",
    "train_preprocessed, test_preprocessed = None, None\n",
    "with open(f\"{DATA_PATH}/{PREPROCESSED_DATA_FILE}\", mode=\"rb\") as file:\n",
    "    train_preprocessed, test_preprocessed = pkl.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=word_tokenize)\n",
    "\n",
    "# Using entire data to fit as the dataset is small and as using entire dataset is not needed for homework requirement\n",
    "all_data = pd.concat([train_preprocessed, test_preprocessed], axis=0)\n",
    "vectorizer.fit(all_data[DATA_COL])\n",
    "\n",
    "X_tfidf_train = vectorizer.transform(train_preprocessed[DATA_COL])\n",
    "X_tfidf_test = vectorizer.transform(test_preprocessed[DATA_COL])\n",
    "y_tfidf_train = train_preprocessed[TARGET_COL]\n",
    "y_tfidf_test = test_preprocessed[TARGET_COL]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save train and test TFIDF vectors\n",
    "import pickle as pkl\n",
    "\n",
    "with open(f\"{DATA_PATH}/{TFIDF_DATA_FILE}\", mode=\"wb\") as file:\n",
    "    pkl.dump((X_tfidf_train, y_tfidf_train, X_tfidf_test, y_tfidf_test), file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### TF-IDF Based Approach\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TF-IDF data\n",
    "import pickle as pkl\n",
    "\n",
    "X_tfidf_train, y_tfidf_train, X_tfidf_test, y_tfidf_test = None, None, None, None\n",
    "with open(f\"{DATA_PATH}/{TFIDF_DATA_FILE}\", mode=\"rb\") as file:\n",
    "    X_tfidf_train, y_tfidf_train, X_tfidf_test, y_tfidf_test = pkl.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "perceptron_tfidf_clf = Perceptron(\n",
    "    max_iter=8000, alpha=0.012, random_state=RANDOM_SEED, tol=1e-4, early_stopping=True, class_weight=\"balanced\"\n",
    ")\n",
    "\n",
    "perceptron_tfidf_clf.fit(X_tfidf_train, y_tfidf_train)\n",
    "\n",
    "y_tfidf_pred = perceptron_tfidf_clf.predict(X_tfidf_test)\n",
    "\n",
    "print_metics(*calc_metrics(y_tfidf_test, y_tfidf_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete Model and related variables\n",
    "del perceptron_tfidf_clf, y_tfidf_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Word2Vec Based Approach - Avg. Word Vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Avg. Word2Vec Data\n",
    "import pickle as pkl\n",
    "\n",
    "X_wv_train, y_wv_train, X_wv_test, y_wv_test = None, None, None, None\n",
    "with open(f\"{DATA_PATH}/{AVG_WORD_VECTORS_DATA_FILE}\", mode=\"rb\") as file:\n",
    "    X_wv_train, y_wv_train, X_wv_test, y_wv_test = pkl.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "perceptron_wv_clf = Perceptron(\n",
    "    max_iter=8000, alpha=0.012, random_state=RANDOM_SEED, tol=1e-4, early_stopping=True, class_weight=\"balanced\"\n",
    ")\n",
    "\n",
    "perceptron_wv_clf = Perceptron(max_iter=8000, alpha=0.012, random_state=RANDOM_SEED, tol=1e-4, early_stopping=True)\n",
    "\n",
    "perceptron_wv_clf.fit(X_wv_train, y_wv_train)\n",
    "\n",
    "y_wv_pred = perceptron_wv_clf.predict(X_wv_test)\n",
    "\n",
    "print_metics(*calc_metrics(y_wv_test, y_wv_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete model and related variables\n",
    "del perceptron_wv_clf, y_wv_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### TF-IDF Based\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TF-IDF data\n",
    "import pickle as pkl\n",
    "\n",
    "X_tfidf_train, y_tfidf_train, X_tfidf_test, y_tfidf_test = None, None, None, None\n",
    "with open(f\"{DATA_PATH}/{TFIDF_DATA_FILE}\", mode=\"rb\") as file:\n",
    "    X_tfidf_train, y_tfidf_train, X_tfidf_test, y_tfidf_test = pkl.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# class_weight = {1: 0.9525, 2: 1.99825, 3: 1.9225, 4: 0.625, 5: 0.8585}\n",
    "\n",
    "svm_tfidf_clf = LinearSVC(dual=False, C=0.1, max_iter=1000, class_weight=\"balanced\", random_state=RANDOM_SEED)\n",
    "\n",
    "svm_tfidf_clf.fit(X_tfidf_train, y_tfidf_train)\n",
    "\n",
    "y_tfidf_pred = svm_tfidf_clf.predict(X_tfidf_test)\n",
    "\n",
    "print_metics(*calc_metrics(y_tfidf_test, y_tfidf_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete model and variables\n",
    "del svm_tfidf_clf, y_tfidf_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Word2Vec Based Approach - Avg. Word Vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Avg. Word2Vec Data\n",
    "import pickle as pkl\n",
    "\n",
    "X_wv_train, y_wv_train, X_wv_test, y_wv_test = None, None, None, None\n",
    "with open(f\"{DATA_PATH}/{AVG_WORD_VECTORS_DATA_FILE}\", mode=\"rb\") as file:\n",
    "    X_wv_train, y_wv_train, X_wv_test, y_wv_test = pkl.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "class_weight = {1: 0.9525, 2: 1.99825, 3: 1.9225, 4: 0.625, 5: 0.8585}\n",
    "\n",
    "svm_wv_clf = LinearSVC(dual=False, C=0.1, max_iter=1000, class_weight=class_weight, random_state=RANDOM_SEED)\n",
    "\n",
    "svm_wv_clf.fit(X_wv_train, y_wv_train)\n",
    "\n",
    "y_wv_avged_pred = svm_wv_clf.predict(X_wv_test)\n",
    "\n",
    "print_metics(*calc_metrics(y_wv_test, y_wv_avged_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete model and variables\n",
    "del svm_wv_clf, y_wv_avged_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you conclude from comparing performances for the models trained using the two different feature types (TF-IDF and your trained Word2Vec features)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## PyTorch Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as func\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "device = None\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using Device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data_loader, train_model, n_epochs, optimizer, criterion):\n",
    "    valid_loss_min = np.Inf\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss = 0.0\n",
    "\n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        train_model.train()  # prep model for training\n",
    "        for data, target in data_loader:\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = train_model(data)\n",
    "            # calculate the loss\n",
    "            loss = criterion(output, target)\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # update running training loss\n",
    "            train_loss += loss.item() * data.size(0)\n",
    "\n",
    "        # print training/validation statistics\n",
    "        # calculate average loss over an epoch\n",
    "        train_loss = train_loss / len(data_loader.dataset)\n",
    "\n",
    "        print(f\"Epoch: {epoch + 1} \\tTraining Loss: {train_loss:.6f}\")\n",
    "\n",
    "    return train_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, dataloader):\n",
    "    prediction_list = []\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        outputs = model(batch)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        prediction_list.append(predicted.cpu())\n",
    "    return prediction_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward Neural Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Word2Vec features, train a feedforward multilayer perceptron net- work for classification. Consider a network with two hidden layers, each with 50 and 10 nodes, respectively. You can use cross entropy loss and your own choice for other hyperparamters, e.g., nonlinearity, number of epochs, etc. Part of getting good results is to select good values for these hyperparamters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FNN_BATCH_SIZE = 200\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create FNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNN(nn.Module):\n",
    "    def __init__(self, n_input, n_output, dropout_rate) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_input = n_input\n",
    "        self.n_hidden_1 = 50\n",
    "        self.n_hidden_2 = 10\n",
    "        self.n_output = n_output\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.fc1 = nn.Linear(self.n_input, self.n_hidden_1)\n",
    "        self.fc2 = nn.Linear(self.n_hidden_1, self.n_hidden_2)\n",
    "        self.fc3 = nn.Linear(self.n_hidden_2, self.n_output)\n",
    "        self.dropout = nn.Dropout(self.dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = func.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = func.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnn_model = FNN(n_input=300, n_output=5, dropout_rate=0.2).to(device)\n",
    "fnn_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(fnn_model.parameters(), lr=1e-2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Using Avg. Word2Vec Vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Avg. Word2Vec Data\n",
    "import pickle as pkl\n",
    "\n",
    "X_wv_train, y_wv_train, X_wv_test, y_wv_test = None, None, None, None\n",
    "with open(f\"{DATA_PATH}/{AVG_WORD_VECTORS_DATA_FILE}\", mode=\"rb\") as file:\n",
    "    X_wv_train, y_wv_train, X_wv_test, y_wv_test = pkl.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class AmazonReviewsDataset(torch.utils.data.Dataset):\n",
    "    \"Characterizes a dataset for PyTorch\"\n",
    "\n",
    "    def __init__(self, inputs, transform=None):\n",
    "        \"Initialization\"\n",
    "        self.data = inputs\n",
    "\n",
    "    def __len__(self):\n",
    "        \"Denotes the total number of samples\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"Generates one sample of data\"\n",
    "        if torch.is_tensor(index):\n",
    "            index = index.tolist()\n",
    "\n",
    "        input, label = self.data[index][0], self.data[index][1]\n",
    "\n",
    "        return input, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_wv_train_tensor = torch.from_numpy(X_wv_train.values).float().to(device)\n",
    "y_wv_train_tensor = torch.from_numpy(y_wv_train.values).float().to(device)\n",
    "X_wv_test_tensor = torch.from_numpy(X_wv_test.values).float().to(device)\n",
    "y_wv_test_tensor = torch.from_numpy(y_wv_test.values).float().to(device)\n",
    "\n",
    "fnn_train = AmazonReviewsDataset(TensorDataset(X_wv_train_tensor, y_wv_train_tensor))\n",
    "fnn_train_loader = DataLoader(fnn_train, batch_size=FNN_BATCH_SIZE, drop_last=True, shuffle=True)\n",
    "\n",
    "fnn_test = AmazonReviewsDataset(TensorDataset(X_wv_test_tensor, y_wv_test_tensor))\n",
    "fnn_test_loader = DataLoader(fnn_test, batch_size=FNN_BATCH_SIZE, drop_last=True, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnn_model = train_model(fnn_train_loader, fnn_model, 1, optimizer, criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(fnn_test_loader, batch_size=FNN_BATCH_SIZE)\n",
    "\n",
    "y_wv_pred = predict(fnn_model, test_loader)\n",
    "\n",
    "print(*calc_metrics(y_wv_test, y_wv_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report accuracy on the testing split.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Concatenate First 10 Word2Vec Vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_top_n_wv(sentence: str, w2v_model, n=10):\n",
    "    top_n_wvs = np.zeros(300 * n, dtype=np.float64)\n",
    "    count = 1\n",
    "\n",
    "    words = sentence.split(\" \")\n",
    "    n_words = len(words)\n",
    "\n",
    "    for word in words:\n",
    "        if count == n or count == n_words:\n",
    "            break\n",
    "        count += 1\n",
    "        try:\n",
    "            top_n_wvs[(count - 1) * 300 : count * 300] = w2v_model[word]\n",
    "        except KeyError:\n",
    "            count -= 1\n",
    "\n",
    "    if np.all((top_n_wvs == 0)):\n",
    "        return np.nan\n",
    "\n",
    "    return top_n_wvs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Apply Top 10 Word Vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned but not preprocessed data\n",
    "import pickle as pkl\n",
    "\n",
    "train_wv_top_10, test_wv_top_10 = None, None\n",
    "with open(f\"{DATA_PATH}/{DATA_FILE}\", mode=\"rb\") as file:\n",
    "    train_wv_top_10, test_wv_top_10 = pkl.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "# Preprocess using gensim'simple_preprocess\n",
    "train_wv_top_10[\"review_body\"] = train_wv_top_10[\"review_body\"].apply(simple_preprocess)\n",
    "test_wv_top_10[\"review_body\"] = test_wv_top_10[\"review_body\"].apply(simple_preprocess)\n",
    "\n",
    "TEMP_COL = \"top_10_wv\"\n",
    "VECTOR_COLS = [\"vector_\" + f\"_{i}\" for i in range(3000)]\n",
    "\n",
    "# Train\n",
    "train_wv_top_10[TEMP_COL] = train_wv_top_10[DATA_COL].apply(partial(concatenate_top_n_wv, w2v_model=w2v_google, n=10))\n",
    "\n",
    "wv_df = pd.DataFrame(train_wv_top_10[TEMP_COL].to_list(), index=train_wv_top_10[TEMP_COL].index, columns=VECTOR_COLS)\n",
    "\n",
    "train_wv_top_10 = pd.concat([train_wv_top_10, wv_df], axis=1)\n",
    "train_wv_top_10.dropna(inplace=True)\n",
    "\n",
    "X_wv_train = train_wv_top_10.drop([DATA_COL, TEMP_COL, TARGET_COL], axis=1)\n",
    "y_wv_train = train_wv_top_10[TARGET_COL]\n",
    "\n",
    "\n",
    "# Test\n",
    "test_wv_top_10[TEMP_COL] = test[DATA_COL].apply(partial(concatenate_top_n_wv, w2v_model=w2v_google, n=10))\n",
    "\n",
    "wv_df = pd.DataFrame(test_wv_top_10[TEMP_COL].to_list(), index=test_wv_top_10[TEMP_COL].index, columns=VECTOR_COLS)\n",
    "\n",
    "test_wv_top_10 = pd.concat([test_wv_top_10, wv_df], axis=1)\n",
    "test_wv_top_10.dropna(inplace=True)\n",
    "\n",
    "X_wv_test = test_wv_top_10.drop([DATA_COL, TEMP_COL, TARGET_COL], axis=1)\n",
    "y_wv_test = test_wv_top_10[TARGET_COL]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the avg'ed word vectors dataset\n",
    "import pickle as pkl\n",
    "\n",
    "with open(f\"{DATA_PATH}/{TOP_10_WORD_VECTORS_DATA_FILE}\", mode=\"wb\") as file:\n",
    "    pkl.dump((X_wv_train, y_wv_train, X_wv_test, y_wv_test), file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report the accuracy value on the testing split for your MLP model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you conclude by comparing accuracy values you obtain with those obtained in the “’Simple Models” section?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple RNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report accuracy values on the testing split for your RNN model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you conclude by comparing accuracy values you obtain with those obtained with feedforward neural network models?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gated Recurrent Unit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you conclude by comparing accuracy values you obtain with those obtained using simple RNN?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Answer\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit ('csci544')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e78917e7f90f3892e7e12462ef46781cf5994bd706032ea53be00d0b1f29dcb9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
