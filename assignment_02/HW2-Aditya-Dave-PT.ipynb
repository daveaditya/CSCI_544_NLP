{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow multiple print statements in a cell in Jupyter Notebook\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/aditya/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emot in /Users/aditya/.pyenv/versions/3.10.6/envs/csci544/lib/python3.10/site-packages (3.1)\n",
      "Requirement already satisfied: contractions in /Users/aditya/.pyenv/versions/3.10.6/envs/csci544/lib/python3.10/site-packages (0.1.72)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in /Users/aditya/.pyenv/versions/3.10.6/envs/csci544/lib/python3.10/site-packages (from contractions) (0.0.21)\n",
      "Requirement already satisfied: pyahocorasick in /Users/aditya/.pyenv/versions/3.10.6/envs/csci544/lib/python3.10/site-packages (from textsearch>=0.0.21->contractions) (1.4.4)\n",
      "Requirement already satisfied: anyascii in /Users/aditya/.pyenv/versions/3.10.6/envs/csci544/lib/python3.10/site-packages (from textsearch>=0.0.21->contractions) (0.3.1)\n"
     ]
    }
   ],
   "source": [
    "# Install some dependencies\n",
    "! pip install emot contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"./data\"\n",
    "ORIGINAL_DATA_FILE = \"amazon_reviews_us_Jewelry_v1_00.tsv\"\n",
    "DATA_FILE = \"data.csv\"\n",
    "\n",
    "DATA_COL = \"review_body\"\n",
    "TARGET_COL = \"star_rating\"\n",
    "\n",
    "N_SAMPLES = 20000\n",
    "\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(RANDOM_SEED)\n",
    "rng = np.random.default_rng(seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d7/gcxbjppx70qfkdwy32kg04dh0000gn/T/ipykernel_35401/2797517173.py:2: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(f\"{DATA_PATH}/{ORIGINAL_DATA_FILE}\", sep=\"\\t\", usecols=[TARGET_COL, DATA_COL], low_memory=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>so beautiful even tho clearly not high end ......</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Great product.. I got this set for my mother, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Exactly as pictured and my daughter's friend l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Love it. Fits great. Super comfortable and nea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Got this as a Mother's Day gift for my Mom and...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  star_rating                                        review_body\n",
       "0           5  so beautiful even tho clearly not high end ......\n",
       "1           5  Great product.. I got this set for my mother, ...\n",
       "2           5  Exactly as pictured and my daughter's friend l...\n",
       "3           5  Love it. Fits great. Super comfortable and nea...\n",
       "4           5  Got this as a Mother's Day gift for my Mom and..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the tab separated data file, and print the first 5 rows for confirmation\n",
    "data = pd.read_csv(f\"{DATA_PATH}/{ORIGINAL_DATA_FILE}\", sep=\"\\t\", usecols=[TARGET_COL, DATA_COL], low_memory=True)\n",
    "data.head()\n",
    "\n",
    "# Drop NA values\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Drop the outlier which is star_rating = \"2012-12-21\"\n",
    "data = data[data.star_rating != \"2012-12-21\"]\n",
    "\n",
    "# Convert all star rating to integer\n",
    "data[TARGET_COL] = data.star_rating.astype(int)\n",
    "\n",
    "# Remove nan valued rows\n",
    "data = data[data.review_body.notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>so beautiful even tho clearly not high end ......</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Great product.. I got this set for my mother, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Exactly as pictured and my daughter's friend l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Love it. Fits great. Super comfortable and nea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Got this as a Mother's Day gift for my Mom and...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   star_rating                                        review_body\n",
       "0            5  so beautiful even tho clearly not high end ......\n",
       "1            5  Great product.. I got this set for my mother, ...\n",
       "2            5  Exactly as pictured and my daughter's friend l...\n",
       "3            5  Love it. Fits great. Super comfortable and nea...\n",
       "4            5  Got this as a Mother's Day gift for my Mom and..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_data = data.groupby(TARGET_COL, group_keys=False).apply(lambda x: x.sample(N_SAMPLES, random_state=RANDOM_SEED))\n",
    "sampled_data.reset_index(inplace=True)\n",
    "sampled_data.drop(columns=[\"index\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all reviews to lower case (optional according to study)\n",
    "def to_lower(data: pd.Series):\n",
    "    return data.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_accented_characters(data: pd.Series):\n",
    "    import unicodedata\n",
    "\n",
    "    \"\"\"Removes accented characters from the Series\n",
    "\n",
    "    Args:\n",
    "        data (pd.Series): Series of string\n",
    "\n",
    "    Returns:\n",
    "        _type_: pd.Series\n",
    "    \"\"\"\n",
    "    import unicodedata\n",
    "\n",
    "    return data.apply(lambda x: unicodedata.normalize(\"NFKD\", x).encode(\"ascii\", \"ignore\").decode(\"utf-8\", \"ignore\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_encodings(data: pd.Series):\n",
    "  return data.str.replace(r\"&#\\d+;\", \" \", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_tags(data: pd.Series):\n",
    "  return data.str.replace(r\"<[a-zA-Z]+\\s?/?>\", \" \", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(data: pd.Series):\n",
    "  return data.str.replace(r\"https?://([\\w\\-\\._]+){2,}/[\\w\\-\\.\\-/=\\+_\\?]+\", \" \", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_and_url(data: pd.Series):\n",
    "    \"\"\"Function to remove\n",
    "             1. HTML encodings\n",
    "             2. HTML tags (both closed and open)\n",
    "             3. URLs\n",
    "\n",
    "    Args:\n",
    "        data (pd.Series): A Pandas series of type string\n",
    "\n",
    "    Returns:\n",
    "        _type_: pd.Series\n",
    "    \"\"\"\n",
    "    # Remove HTML encodings\n",
    "    data.str.replace(r\"&#\\d+;\", \" \", regex=True)\n",
    "\n",
    "    # Remove HTML tags (both open and closed)\n",
    "    data.str.replace(r\"<[a-zA-Z]+\\s?/?>\", \" \", regex=True)\n",
    "\n",
    "    # Remove URLs\n",
    "    data.str.replace(r\"https?://([\\w\\-\\._]+){2,}/[\\w\\-\\.\\-/=\\+_\\?]+\", \" \", regex=True)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle emoji\n",
    "def convert_emoji_to_txt(data: pd.Series):\n",
    "  from emot.emo_unicode import UNICODE_EMOJI, EMOTICONS_EMO\n",
    "\n",
    "  EMO_TO_TXT_DICT = dict()\n",
    "  for emot in UNICODE_EMOJI:\n",
    "    EMO_TO_TXT_DICT[emot] = f\" {re.sub(r',|:|_', '', UNICODE_EMOJI[emot])} \"\n",
    "\n",
    "  for emo in EMOTICONS_EMO:\n",
    "    EMO_TO_TXT_DICT[emot] = f\" {re.sub(r',| ', '', EMOTICONS_EMO[emo])} \"\n",
    "\n",
    "  def convert_emojis(text, emo_to_txt_dict):\n",
    "    for emot in emo_to_txt_dict:\n",
    "        text = text.replace(emot, emo_to_txt_dict[emot])\n",
    "    return text\n",
    "\n",
    "  return data.apply(lambda x: convert_emojis(x, EMO_TO_TXT_DICT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove non-alphabetical characters\n",
    "def remove_non_alpha_characters(data: pd.Series):\n",
    "    return data.str.replace(r\"_+|\\\\|[^a-zA-Z0-9\\s]\", \" \", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove extra spaces\n",
    "def remove_extra_spaces(data: pd.Series):\n",
    "    return data.str.replace(r\"^\\s*|\\s\\s*\", \" \", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expanding contractions\n",
    "def fix_contractions(data: pd.Series):\n",
    "    import contractions\n",
    "\n",
    "    def contraction_fixer(txt: str):\n",
    "        return \" \".join([contractions.fix(word) for word in txt.split()])\n",
    "\n",
    "    return data.apply(contraction_fixer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting: convert_emoji_to_txt\n",
      "Ended: convert_emoji_to_txt\n",
      "Starting: to_lower\n",
      "Ended: to_lower\n",
      "Starting: remove_accented_characters\n",
      "Ended: remove_accented_characters\n",
      "Starting: remove_html_encodings\n",
      "Ended: remove_html_encodings\n",
      "Starting: remove_html_tags\n",
      "Ended: remove_html_tags\n",
      "Starting: remove_url\n",
      "Ended: remove_url\n",
      "Starting: fix_contractions\n",
      "Ended: fix_contractions\n",
      "Starting: remove_non_alpha_characters\n",
      "Ended: remove_non_alpha_characters\n",
      "Starting: remove_extra_spaces\n",
      "Ended: remove_extra_spaces\n",
      "Cleaned and sampled data stored.\n"
     ]
    }
   ],
   "source": [
    "# A dictionary containing the columns and a list of functions to perform on it in order\n",
    "data_cleaning_pipeline = {\n",
    "    DATA_COL: [\n",
    "        convert_emoji_to_txt,\n",
    "        to_lower,\n",
    "        remove_accented_characters,\n",
    "        remove_html_encodings,\n",
    "        remove_html_tags,\n",
    "        remove_url,\n",
    "        fix_contractions,\n",
    "        remove_non_alpha_characters,\n",
    "        remove_extra_spaces,\n",
    "    ]\n",
    "}\n",
    "\n",
    "cleaned_data = sampled_data.copy()\n",
    "\n",
    "# Process all the cleaning instructions\n",
    "for col, pipeline in data_cleaning_pipeline.items():\n",
    "    # Get the column to perform cleaning on\n",
    "    temp_data = cleaned_data[col].copy()\n",
    "\n",
    "    # Perform all the cleaning functions sequencially\n",
    "    for func in pipeline:\n",
    "        print(f\"Starting: {func.__name__}\")\n",
    "        temp_data = func(temp_data)\n",
    "        print(f\"Ended: {func.__name__}\")\n",
    "\n",
    "    # Replace the old column with cleaned one.\n",
    "    cleaned_data[col] = temp_data.copy()\n",
    "\n",
    "# Store data file\n",
    "cleaned_data.to_csv(f\"{DATA_PATH}/{DATA_FILE}\", sep=\",\", index=False)\n",
    "print(\"Cleaned and sampled data stored.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>too small even for the knuckles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>did not fit right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>this stupid kit has 16 gauge needles not 14ga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>i would not suggest this item i bought the on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>i am sure that it will be lovely once i get i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   star_rating                                        review_body\n",
       "0            1                   too small even for the knuckles \n",
       "1            1                                  did not fit right\n",
       "2            1   this stupid kit has 16 gauge needles not 14ga...\n",
       "3            1   i would not suggest this item i bought the on...\n",
       "4            1   i am sure that it will be lovely once i get i..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(f\"{DATA_PATH}/{DATA_FILE}\", sep=\",\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing (used by TF-IDF Models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(data: pd.Series):\n",
    "    from nltk.tokenize import word_tokenize\n",
    "\n",
    "    nltk.download(\"punkt\")\n",
    "\n",
    "    return data.apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Set\n",
    "\n",
    "def remove_stopwords(data: pd.Series):\n",
    "    \"\"\"Remove stop words using the NLTK stopwords dictionary\n",
    "\n",
    "    Args:\n",
    "        string (str): a document\n",
    "\n",
    "    Returns:\n",
    "        str: a document with stopwords removed\n",
    "    \"\"\"\n",
    "    from nltk.corpus import stopwords\n",
    "\n",
    "    nltk.download(\"stopwords\")\n",
    "\n",
    "    stopwords = set(stopwords.words())\n",
    "\n",
    "    def remover(word_list: List[str], stopwords: Set[str]):\n",
    "        return [word for word in word_list if not word in stopwords]\n",
    "\n",
    "    return data.apply(lambda word_list: remover(word_list, stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(data: pd.Series, consider_pos_tag: bool = True):\n",
    "    from nltk.corpus import wordnet\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "    nltk.download(\"omw-1.4\")\n",
    "\n",
    "    # POS tagging\n",
    "    def perform_nltk_pos_tag(data: pd.Series):\n",
    "        from nltk import pos_tag\n",
    "\n",
    "        nltk.download(\"averaged_perceptron_tagger\")\n",
    "\n",
    "        return data.apply(pos_tag)\n",
    "\n",
    "    # Convert POS tag to wordnet pos tags\n",
    "    def wordnet_pos_tagger(tag: str):\n",
    "        if tag.startswith(\"J\"):\n",
    "            return wordnet.ADJ\n",
    "        elif tag.startswith(\"V\"):\n",
    "            return wordnet.VERB\n",
    "        elif tag.startswith(\"N\"):\n",
    "            return wordnet.NOUN\n",
    "        elif tag.startswith(\"R\"):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = list()\n",
    "\n",
    "    if consider_pos_tag:\n",
    "        pos_tagged_data = data.copy()\n",
    "        pos_tagged_data = perform_nltk_pos_tag(data)\n",
    "\n",
    "        for row in pos_tagged_data:\n",
    "\n",
    "            lemmatized_row = list()\n",
    "\n",
    "            if consider_pos_tag:\n",
    "                for word, tag in row:\n",
    "                    wordnet_pos_tag = wordnet_pos_tagger(tag)\n",
    "\n",
    "                    if wordnet_pos_tag is None:\n",
    "                        lemmatized_row.append(word)\n",
    "                    else:\n",
    "                        result = lemmatizer.lemmatize(word, wordnet_pos_tag)\n",
    "                        lemmatized_row.append(lemmatizer.lemmatize(word, wordnet_pos_tag))\n",
    "\n",
    "            lemmatized.append(lemmatized_row)\n",
    "    else:\n",
    "        for row in data:\n",
    "            lemmatized_row = list()\n",
    "\n",
    "            for word in row:\n",
    "                lemmatized_row.append(lemmatizer.lemmatize(word))\n",
    "\n",
    "            lemmatized.append(lemmatized_row)\n",
    "\n",
    "    return pd.Series(lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate lemmatized sentences back into one sentence\n",
    "def concatenate(data: pd.Series):\n",
    "    return data.apply(lambda words: \" \".join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_pipeline = {DATA_COL: [tokenize, lemmatize, concatenate]}\n",
    "\n",
    "# Run the pipeline\n",
    "preprocessed_data = cleaned_data.copy()\n",
    "\n",
    "# Process all the cleaning instructions\n",
    "for col, pipeline in preprocessing_pipeline.items():\n",
    "    # Get the column to perform cleaning on\n",
    "    temp_data = preprocessed_data[col].copy()\n",
    "\n",
    "    # Perform all the cleaning functions sequencially\n",
    "    for func in pipeline:\n",
    "        print(f\"Starting: {func.__name__}\")\n",
    "\n",
    "        if func.__name__ == \"lemmatize\":\n",
    "            temp_data = func(temp_data, consider_pos_tag=True)\n",
    "        else:\n",
    "            temp_data = func(temp_data)\n",
    "\n",
    "        print(f\"Ended: {func.__name__}\")\n",
    "\n",
    "    # Replace the old column with cleaned one.\n",
    "    preprocessed_data[col] = temp_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TF-DF Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove empty reviews\n",
    "preprocessed_data = preprocessed_data[preprocessed_data[DATA_COL].str.len() != 0]\n",
    "\n",
    "final_data = preprocessed_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data 80-20 split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(data, test_size=0.2, stratify=data[TARGET_COL], random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TF-IDF\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=word_tokenize)\n",
    "vectorizer.fit(final_data[DATA_COL])\n",
    "\n",
    "X_tfidf_train = vectorizer.transform(train[DATA_COL])\n",
    "X_tfidf_test = vectorizer.transform(test[DATA_COL])\n",
    "y_train = train[TARGET_COL]\n",
    "y_test = test[TARGET_COL]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(y_true, y_pred):\n",
    "    from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "    precision = precision_score(y_true, y_pred, average=None)\n",
    "    recall = recall_score(y_true, y_pred, average=None)\n",
    "    f1 = f1_score(y_true, y_pred, average=None)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    return precision, recall, f1, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metics(precision, recall, f1, accuracy):\n",
    "    for rating_precision, rating_recall, rating_f1 in zip(precision, recall, f1):\n",
    "        print(f\"{rating_precision},{rating_recall},{rating_f1}\")\n",
    "\n",
    "    print(f\"{np.mean(precision)},{np.mean(recall)},{np.mean(f1)}\")\n",
    "    print(f\"{accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load `word2vec-google-news-300` Model\n",
    "\n",
    "Learn how to extract word embeddings for your dataset. Try to check semantic similarities of the generated vectors using three examples of your own, e.g., King − Man + Woman = Queen or excellent ∼ outstanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Word2Vec on own Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check semantic similarities from words used in earlier part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you conclude from comparing vectors generated by yourself and the pretrained model? Which of the Word2Vec models seems to encode semantic similarities between words better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the average Word2Vec vectors for each review as the input feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF Based Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "class_weight = {1: 0.9525, 2: 1.99825, 3: 1.9225, 4: 0.625, 5: 0.8585}\n",
    "clf = Perceptron(max_iter=8000, alpha=0.012, random_state=RANDOM_SEED, tol=1e-4, early_stopping=True, class_weight=\"balanced\") # 0.45103975891511683\n",
    "\n",
    "clf.fit(X_tfidf_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_tfidf_test)\n",
    "\n",
    "calc_metrics(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec Based Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "class_weight = {1: 0.9525, 2: 1.99825, 3: 1.9225, 4: 0.625, 5: 0.8585} # 0.5746383318772281\n",
    "\n",
    "clf = LinearSVC(dual=False, C=0.1, max_iter=1000, class_weight=class_weight, random_state=RANDOM_SEED)\n",
    "\n",
    "clf.fit(X_tfidf_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_tfidf_test)\n",
    "\n",
    "calc_metrics(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec Based Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report your accuracy values on the testing split for these models similar to HW1, i.e., for each of perceptron and SVM models, report two accuracy values Word2Vec and TF-IDF features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you conclude from comparing performances for the models trained using the two different feature types (TF-IDF and your trained Word2Vec features)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Word2Vec features, train a feedforward multilayer perceptron network for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Avg. Word2Vec Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report accuracy on the testing split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate First 10 Word2Vec Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report the accuracy value on the testing split for your MLP model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you conclude by comparing accuracy values you obtain with those obtained in the “’Simple Models” section?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report accuracy values on the testing split for your RNN model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you conclude by comparing accuracy values you obtain with those obtained with feedforward neural network models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gated Recurrent Unit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you conclude by comparing accuracy values you obtain with those obtained using simple RNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit ('csci544')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e78917e7f90f3892e7e12462ef46781cf5994bd706032ea53be00d0b1f29dcb9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
