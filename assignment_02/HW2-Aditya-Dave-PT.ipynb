{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow multiple print statements in a cell in Jupyter Notebook\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/aditya/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emot in /Users/aditya/.pyenv/versions/3.10.6/envs/csci544/lib/python3.10/site-packages (3.1)\n",
      "Requirement already satisfied: contractions in /Users/aditya/.pyenv/versions/3.10.6/envs/csci544/lib/python3.10/site-packages (0.1.72)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in /Users/aditya/.pyenv/versions/3.10.6/envs/csci544/lib/python3.10/site-packages (from contractions) (0.0.21)\n",
      "Requirement already satisfied: pyahocorasick in /Users/aditya/.pyenv/versions/3.10.6/envs/csci544/lib/python3.10/site-packages (from textsearch>=0.0.21->contractions) (1.4.4)\n",
      "Requirement already satisfied: anyascii in /Users/aditya/.pyenv/versions/3.10.6/envs/csci544/lib/python3.10/site-packages (from textsearch>=0.0.21->contractions) (0.3.1)\n"
     ]
    }
   ],
   "source": [
    "# Install some dependencies\n",
    "! pip install emot contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"/Volumes/dataTwo/usc/CSCI_544/assignment_02/data\"\n",
    "MODEL_PATH = \"/Volumes/dataTwo/usc/CSCI_544/assignment_02/model\"\n",
    "\n",
    "ORIGINAL_DATA_FILE = \"amazon_reviews_us_Jewelry_v1_00.tsv\"\n",
    "SAMPLED_DATA_FILE = \"data_sampled.csv\"\n",
    "CLEANED_DATA_FILE = \"data_cleaned.csv\"\n",
    "\n",
    "# Files after clean and split\n",
    "DATA_FILE = \"data.pkl\"\n",
    "\n",
    "# Files after preprocessing the splitted dataset\n",
    "PREPROCESSED_DATA_FILE = \"data_preprocessed.csv\"\n",
    "\n",
    "# Files containing the tfidf data\n",
    "TFIDF_DATA_FILE = \"data_tfidf.csv\"\n",
    "\n",
    "# custom created word vectors for the review dataset\n",
    "CUSTOM_WORD_VECTORS_MODEL_FILE = \"gensim_w2v_amazon_reviews_model\"\n",
    "\n",
    "# train and test data for word2vec avg. word vectors approach\n",
    "AVG_WORD_VECTORS_DATA_FILE = \"data_avg_word_vectors.pkl\"\n",
    "\n",
    "# train and test data for word2vec contatenate top 10 vectors appraoch\n",
    "TOP_10_WORD_VECTORS_DATA_FILE = \"data_avg_word_vectors.pkl\"\n",
    "\n",
    "# train and test data for word2vec contatenate top 10 vectors appraoch\n",
    "WORDS_20_WORD_VECTORS_DATA_FILE = \"data_words_20_word_vectors.pkl\"\n",
    "\n",
    "DATA_COL = \"review_body\"\n",
    "TARGET_COL = \"star_rating\"\n",
    "\n",
    "N_SAMPLES = 20000\n",
    "\n",
    "RANDOM_SEED = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(RANDOM_SEED)\n",
    "rng = np.random.default_rng(seed=RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aditya/.pyenv/versions/3.10.6/envs/csci544/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3398: DtypeWarning: Columns (7) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>so beautiful even tho clearly not high end ......</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>Great product.. I got this set for my mother, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Exactly as pictured and my daughter's friend l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Love it. Fits great. Super comfortable and nea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Got this as a Mother's Day gift for my Mom and...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   star_rating                                        review_body\n",
       "0            4  so beautiful even tho clearly not high end ......\n",
       "1            4  Great product.. I got this set for my mother, ...\n",
       "2            4  Exactly as pictured and my daughter's friend l...\n",
       "3            4  Love it. Fits great. Super comfortable and nea...\n",
       "4            4  Got this as a Mother's Day gift for my Mom and..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the tab separated data file, and print the first 5 rows for confirmation\n",
    "data = pd.read_csv(f\"{DATA_PATH}/{ORIGINAL_DATA_FILE}\", sep=\"\\t\", usecols=[TARGET_COL, DATA_COL], low_memory=True)\n",
    "\n",
    "# Drop NA values\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Drop the outlier which is star_rating = \"2012-12-21\"\n",
    "data = data[data.star_rating != \"2012-12-21\"]\n",
    "\n",
    "# Convert all star rating to integer\n",
    "data[TARGET_COL] = data.star_rating.astype(int)\n",
    "\n",
    "# Make target col in range of 0-4\n",
    "data[TARGET_COL] = data[TARGET_COL] - 1\n",
    "\n",
    "# Remove nan valued rows\n",
    "data = data[data.review_body.notnull()]\n",
    "\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_data = data.groupby(TARGET_COL, group_keys=False).apply(lambda x: x.sample(N_SAMPLES, random_state=RANDOM_SEED))\n",
    "sampled_data.reset_index(inplace=True, drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data without cleaning\n",
    "sampled_data.to_csv(f\"{DATA_PATH}/{SAMPLED_DATA_FILE}\", header=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all reviews to lower case (optional according to study)\n",
    "def to_lower(data: pd.Series):\n",
    "    return data.str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_accented_characters(data: pd.Series):\n",
    "    import unicodedata\n",
    "\n",
    "    \"\"\"Removes accented characters from the Series\n",
    "\n",
    "    Args:\n",
    "        data (pd.Series): Series of string\n",
    "\n",
    "    Returns:\n",
    "        _type_: pd.Series\n",
    "    \"\"\"\n",
    "    import unicodedata\n",
    "\n",
    "    return data.apply(lambda x: unicodedata.normalize(\"NFKD\", x).encode(\"ascii\", \"ignore\").decode(\"utf-8\", \"ignore\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_encodings(data: pd.Series):\n",
    "    return data.str.replace(r\"&#\\d+;\", \" \", regex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_tags(data: pd.Series):\n",
    "    return data.str.replace(r\"<[a-zA-Z]+\\s?/?>\", \" \", regex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(data: pd.Series):\n",
    "    return data.str.replace(r\"https?://([\\w\\-\\._]+){2,}/[\\w\\-\\.\\-/=\\+_\\?]+\", \" \", regex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_and_url(data: pd.Series):\n",
    "    \"\"\"Function to remove\n",
    "             1. HTML encodings\n",
    "             2. HTML tags (both closed and open)\n",
    "             3. URLs\n",
    "\n",
    "    Args:\n",
    "        data (pd.Series): A Pandas series of type string\n",
    "\n",
    "    Returns:\n",
    "        _type_: pd.Series\n",
    "    \"\"\"\n",
    "    # Remove HTML encodings\n",
    "    data.str.replace(r\"&#\\d+;\", \" \", regex=True)\n",
    "\n",
    "    # Remove HTML tags (both open and closed)\n",
    "    data.str.replace(r\"<[a-zA-Z]+\\s?/?>\", \" \", regex=True)\n",
    "\n",
    "    # Remove URLs\n",
    "    data.str.replace(r\"https?://([\\w\\-\\._]+){2,}/[\\w\\-\\.\\-/=\\+_\\?]+\", \" \", regex=True)\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle emoji\n",
    "def convert_emoji_to_txt(data: pd.Series):\n",
    "    from emot.emo_unicode import UNICODE_EMOJI, EMOTICONS_EMO\n",
    "\n",
    "    EMO_TO_TXT_DICT = dict()\n",
    "    for emot in UNICODE_EMOJI:\n",
    "        EMO_TO_TXT_DICT[emot] = f\" {re.sub(r',|:|_', '', UNICODE_EMOJI[emot])} \"\n",
    "\n",
    "    for emo in EMOTICONS_EMO:\n",
    "        EMO_TO_TXT_DICT[emot] = f\" {re.sub(r',| ', '', EMOTICONS_EMO[emo])} \"\n",
    "\n",
    "    def convert_emojis(text, emo_to_txt_dict):\n",
    "        for emot in emo_to_txt_dict:\n",
    "            text = text.replace(emot, emo_to_txt_dict[emot])\n",
    "        return text\n",
    "\n",
    "    return data.apply(lambda x: convert_emojis(x, EMO_TO_TXT_DICT))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove non-alphabetical characters\n",
    "def remove_non_alpha_characters(data: pd.Series):\n",
    "    return data.str.replace(r\"_+|\\\\|[^a-zA-Z0-9\\s]\", \" \", regex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove extra spaces\n",
    "def remove_extra_spaces(data: pd.Series):\n",
    "    return data.str.replace(r\"^\\s*|\\s\\s*\", \" \", regex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expanding contractions\n",
    "def fix_contractions(data: pd.Series):\n",
    "    import contractions\n",
    "\n",
    "    def contraction_fixer(txt: str):\n",
    "        return \" \".join([contractions.fix(word) for word in txt.split()])\n",
    "\n",
    "    return data.apply(contraction_fixer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting: convert_emoji_to_txt\n",
      "Ended: convert_emoji_to_txt\n",
      "Starting: to_lower\n",
      "Ended: to_lower\n",
      "Starting: remove_accented_characters\n",
      "Ended: remove_accented_characters\n",
      "Starting: remove_html_encodings\n",
      "Ended: remove_html_encodings\n",
      "Starting: remove_html_tags\n",
      "Ended: remove_html_tags\n",
      "Starting: remove_url\n",
      "Ended: remove_url\n",
      "Starting: fix_contractions\n",
      "Ended: fix_contractions\n",
      "Starting: remove_non_alpha_characters\n",
      "Ended: remove_non_alpha_characters\n",
      "Starting: remove_extra_spaces\n",
      "Ended: remove_extra_spaces\n"
     ]
    }
   ],
   "source": [
    "# A dictionary containing the columns and a list of functions to perform on it in order\n",
    "data_cleaning_pipeline = {\n",
    "    DATA_COL: [\n",
    "        convert_emoji_to_txt,\n",
    "        to_lower,\n",
    "        remove_accented_characters,\n",
    "        remove_html_encodings,\n",
    "        remove_html_tags,\n",
    "        remove_url,\n",
    "        fix_contractions,\n",
    "        remove_non_alpha_characters,\n",
    "        remove_extra_spaces,\n",
    "    ]\n",
    "}\n",
    "\n",
    "cleaned_data = sampled_data.copy()\n",
    "\n",
    "# Process all the cleaning instructions\n",
    "for col, pipeline in data_cleaning_pipeline.items():\n",
    "    # Get the column to perform cleaning on\n",
    "    temp_data = cleaned_data[col].copy()\n",
    "\n",
    "    # Perform all the cleaning functions sequencially\n",
    "    for func in pipeline:\n",
    "        print(f\"Starting: {func.__name__}\")\n",
    "        temp_data = func(temp_data)\n",
    "        print(f\"Ended: {func.__name__}\")\n",
    "\n",
    "    # Replace the old column with cleaned one.\n",
    "    cleaned_data[col] = temp_data.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store data file\n",
    "cleaned_data.to_csv(f\"{DATA_PATH}/{CLEANED_DATA_FILE}\", sep=\",\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing (used by TF-IDF Models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(data: pd.Series):\n",
    "    from nltk.tokenize import word_tokenize\n",
    "\n",
    "    nltk.download(\"punkt\")\n",
    "\n",
    "    return data.apply(word_tokenize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Set\n",
    "\n",
    "\n",
    "def remove_stopwords(data: pd.Series):\n",
    "    \"\"\"Remove stop words using the NLTK stopwords dictionary\n",
    "\n",
    "    Args:\n",
    "        string (str): a document\n",
    "\n",
    "    Returns:\n",
    "        str: a document with stopwords removed\n",
    "    \"\"\"\n",
    "    from nltk.corpus import stopwords\n",
    "\n",
    "    nltk.download(\"stopwords\")\n",
    "\n",
    "    stopwords = set(stopwords.words())\n",
    "\n",
    "    def remover(word_list: List[str], stopwords: Set[str]):\n",
    "        return [word for word in word_list if not word in stopwords]\n",
    "\n",
    "    return data.apply(lambda word_list: remover(word_list, stopwords))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(data: pd.Series, consider_pos_tag: bool = True):\n",
    "    from nltk.corpus import wordnet\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "    nltk.download(\"omw-1.4\")\n",
    "\n",
    "    # POS tagging\n",
    "    def perform_nltk_pos_tag(data: pd.Series):\n",
    "        from nltk import pos_tag\n",
    "\n",
    "        nltk.download(\"averaged_perceptron_tagger\")\n",
    "\n",
    "        return data.apply(pos_tag)\n",
    "\n",
    "    # Convert POS tag to wordnet pos tags\n",
    "    def wordnet_pos_tagger(tag: str):\n",
    "        if tag.startswith(\"J\"):\n",
    "            return wordnet.ADJ\n",
    "        elif tag.startswith(\"V\"):\n",
    "            return wordnet.VERB\n",
    "        elif tag.startswith(\"N\"):\n",
    "            return wordnet.NOUN\n",
    "        elif tag.startswith(\"R\"):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = list()\n",
    "\n",
    "    if consider_pos_tag:\n",
    "        pos_tagged_data = data.copy()\n",
    "        pos_tagged_data = perform_nltk_pos_tag(data)\n",
    "\n",
    "        for row in pos_tagged_data:\n",
    "\n",
    "            lemmatized_row = list()\n",
    "\n",
    "            if consider_pos_tag:\n",
    "                for word, tag in row:\n",
    "                    wordnet_pos_tag = wordnet_pos_tagger(tag)\n",
    "\n",
    "                    if wordnet_pos_tag is None:\n",
    "                        lemmatized_row.append(word)\n",
    "                    else:\n",
    "                        result = lemmatizer.lemmatize(word, wordnet_pos_tag)\n",
    "                        lemmatized_row.append(lemmatizer.lemmatize(word, wordnet_pos_tag))\n",
    "\n",
    "            lemmatized.append(lemmatized_row)\n",
    "    else:\n",
    "        for row in data:\n",
    "            lemmatized_row = list()\n",
    "\n",
    "            for word in row:\n",
    "                lemmatized_row.append(lemmatizer.lemmatize(word))\n",
    "\n",
    "            lemmatized.append(lemmatized_row)\n",
    "\n",
    "    return pd.Series(lemmatized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate lemmatized sentences back into one sentence\n",
    "def concatenate(data: pd.Series):\n",
    "    return data.apply(lambda words: \" \".join(words))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data 80-20 split\n",
    "import pickle as pkl\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# load clean data\n",
    "cleaned_data = pd.read_csv(f\"{DATA_PATH}/{CLEANED_DATA_FILE}\")\n",
    "\n",
    "# split cleaned data into train and test\n",
    "train, test = train_test_split(cleaned_data, test_size=0.2, stratify=cleaned_data[TARGET_COL], random_state=RANDOM_SEED)\n",
    "train.reset_index(inplace=True, drop=True)\n",
    "test.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# save the split\n",
    "with open(f\"{DATA_PATH}/{DATA_FILE}\", mode=\"wb\") as file:\n",
    "    pkl.dump((train, test), file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "train, test = None, None\n",
    "with open(f\"{DATA_PATH}/{DATA_FILE}\", mode=\"rb\") as file:\n",
    "    train, test = pkl.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting: tokenize\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/aditya/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/aditya/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ended: tokenize\n",
      "Starting: lemmatize\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /Users/aditya/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/aditya/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/aditya/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/aditya/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ended: lemmatize\n",
      "Starting: concatenate\n",
      "Ended: concatenate\n"
     ]
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "preprocessing_pipeline = {DATA_COL: [tokenize, lemmatize, concatenate]}\n",
    "\n",
    "# Run the pipeline\n",
    "preprocessed_train_data = train.copy()\n",
    "preprocessed_test_data = test.copy()\n",
    "\n",
    "# Process all the cleaning instructions\n",
    "for col, pipeline in preprocessing_pipeline.items():\n",
    "    # Get the column to perform cleaning on\n",
    "    temp_data_train = preprocessed_train_data[col].copy()\n",
    "    temp_data_test = preprocessed_test_data[col].copy()\n",
    "\n",
    "    # Perform all the cleaning functions sequencially\n",
    "    for func in pipeline:\n",
    "        print(f\"Starting: {func.__name__}\")\n",
    "\n",
    "        if func.__name__ == \"lemmatize\":\n",
    "            temp_data_train = func(temp_data_train, consider_pos_tag=True)\n",
    "            temp_data_test = func(temp_data_test, consider_pos_tag=True)\n",
    "        else:\n",
    "            temp_data_train = func(temp_data_train)\n",
    "            temp_data_test = func(temp_data_test)\n",
    "\n",
    "        print(f\"Ended: {func.__name__}\")\n",
    "\n",
    "    # Replace the old column with cleaned one.\n",
    "    preprocessed_train_data[col] = temp_data_train\n",
    "    preprocessed_test_data[col] = temp_data_test\n",
    "\n",
    "# Remove empty reviews\n",
    "preprocessed_train_data = preprocessed_train_data[preprocessed_train_data[DATA_COL].str.len() != 0]\n",
    "preprocessed_test_data = preprocessed_test_data[preprocessed_test_data[DATA_COL].str.len() != 0]\n",
    "\n",
    "# Remove NaN\n",
    "new_preprocessed_train_data = preprocessed_train_data.dropna()\n",
    "new_preprocessed_test_data = preprocessed_test_data.dropna()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the preprocessed data\n",
    "with open(f\"{DATA_PATH}/{PREPROCESSED_DATA_FILE}\", mode=\"wb\") as file:\n",
    "    pkl.dump((preprocessed_train_data, preprocessed_test_data), file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load `word2vec-google-news-300` Model\n",
    "\n",
    "Learn how to extract word embeddings for your dataset. Try to check semantic similarities of the generated vectors using three examples of your own, e.g., King − Man + Woman = Queen or excellent ∼ outstanding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /Users/aditya/.pyenv/versions/3.10.6/envs/csci544/lib/python3.10/site-packages (4.2.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /Users/aditya/.pyenv/versions/3.10.6/envs/csci544/lib/python3.10/site-packages (from gensim) (1.9.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/aditya/.pyenv/versions/3.10.6/envs/csci544/lib/python3.10/site-packages (from gensim) (6.2.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /Users/aditya/.pyenv/versions/3.10.6/envs/csci544/lib/python3.10/site-packages (from gensim) (1.23.2)\n"
     ]
    }
   ],
   "source": [
    "! pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "w2v_google = api.load(\"word2vec-google-news-300\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7118192911148071)]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_google.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"], topn=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5567486"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_google.similarity(\"excellent\", \"outstanding\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three Semantic Similarity Examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.55750686"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_google.similarity(\"worst\", \"terrible\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05468019"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_google.similarity(\"cheap\", \"disappointed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('crappiest', 0.46163424849510193)]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_google.most_similar(positive=[\"cheap\", \"worst\"], negative=[\"costly\"], topn=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Word2Vec on own Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "from gensim import utils\n",
    "\n",
    "\n",
    "class AmazonReviewCorpus:\n",
    "    def __iter__(self):\n",
    "        data = pd.read_csv(f\"{DATA_PATH}/{CLEANED_DATA_FILE}\", sep=\",\")\n",
    "\n",
    "        for review_body in data[\"review_body\"]:\n",
    "            yield utils.simple_preprocess(review_body)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim import utils\n",
    "\n",
    "w2v_custom = Word2Vec(vector_size=300, min_count=10, window=11, workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the vocabulary\n",
    "# reviews_data = pd.read_csv(f\"{DATA_PATH}/{CLEANED_DATA_FILE}\", sep=\",\")\n",
    "# reviews = reviews_data[\"review_body\"].apply(utils.simple_preprocess)\n",
    "reviews = AmazonReviewCorpus()\n",
    "\n",
    "w2v_custom.build_vocab(reviews, progress_per=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11713384, 16910110)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "w2v_custom.train(reviews, total_examples=w2v_custom.corpus_count, epochs=w2v_custom.epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "w2v_custom.save(f\"{MODEL_PATH}/{CUSTOM_WORD_VECTORS_MODEL_FILE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using trained model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "w2v_custom = Word2Vec.load(f\"{MODEL_PATH}/{CUSTOM_WORD_VECTORS_MODEL_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dealer', 0.5510936975479126)]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_custom.wv.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"], topn=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7820383"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_custom.wv.similarity(\"excellent\", \"outstanding\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three Semantic Similarity Examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.55750686"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_google.similarity(\"worst\", \"terrible\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05468019"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_google.similarity(\"cheap\", \"disappointed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('crappiest', 0.46163424849510193)]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_google.most_similar(positive=[\"cheap\", \"worst\"], negative=[\"costly\"], topn=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check semantic similarities from words used in earlier part.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you conclude from comparing vectors generated by yourself and the pretrained model? Which of the Word2Vec models seems to encode semantic similarities between words better?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avg. Word Vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the average Word2Vec vectors for each review as the input feature (x = N1 􏰀Ni=1 Wi for a review with N words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_avg_word_vector(words: List[str], w2v_model):\n",
    "    result_vector = np.ndarray(shape=(300,), buffer=np.zeros((300,)), dtype=float)\n",
    "    removed_word_count = 0\n",
    "    for word in words:\n",
    "        try:\n",
    "            result_vector += w2v_model[word]\n",
    "        except KeyError:\n",
    "            removed_word_count += 1\n",
    "    return result_vector / (len(words) - removed_word_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Perform Word2Vec conversion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned but not preprocessed data\n",
    "import pickle as pkl\n",
    "\n",
    "train_wv, test_wv = None, None\n",
    "with open(f\"{DATA_PATH}/{DATA_FILE}\", mode=\"rb\") as file:\n",
    "    train_wv, test_wv = pkl.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "# Preprocess using gensim'simple_preprocess\n",
    "train_wv[\"review_body\"] = train_wv[\"review_body\"].apply(simple_preprocess)\n",
    "test_wv[\"review_body\"] = test_wv[\"review_body\"].apply(simple_preprocess)\n",
    "\n",
    "TEMP_COL = \"avg_word_vector\"\n",
    "VECTOR_COLS = [f\"vector_{i}\" for i in range(300)]\n",
    "\n",
    "# Train Data\n",
    "train_wv[TEMP_COL] = train_wv[DATA_COL].apply(partial(calculate_avg_word_vector, w2v_model=w2v_google))\n",
    "\n",
    "wv_df = pd.DataFrame(train_wv[TEMP_COL].to_list(), index=train_wv[TEMP_COL].index, columns=VECTOR_COLS)\n",
    "wv_df.dropna(inplace=True)\n",
    "\n",
    "train_wv = pd.concat([train_wv, wv_df], axis=1)\n",
    "train_wv.dropna(inplace=True)\n",
    "\n",
    "X_wv_train = train_wv.drop([DATA_COL, TEMP_COL, TARGET_COL], axis=1)\n",
    "y_wv_train = train_wv[TARGET_COL]\n",
    "\n",
    "\n",
    "# Test Data\n",
    "test_wv[TEMP_COL] = test[DATA_COL].apply(partial(calculate_avg_word_vector, w2v_model=w2v_google))\n",
    "\n",
    "wv_df = pd.DataFrame(test_wv[TEMP_COL].to_list(), index=test_wv[TEMP_COL].index, columns=VECTOR_COLS)\n",
    "wv_df.dropna(inplace=True)\n",
    "\n",
    "test_wv = pd.concat([test_wv, wv_df], axis=1)\n",
    "test_wv.dropna(inplace=True)\n",
    "\n",
    "X_wv_test = test_wv.drop([DATA_COL, TEMP_COL, TARGET_COL], axis=1)\n",
    "y_wv_test = test_wv[TARGET_COL]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the avg'ed word vectors dataset\n",
    "import pickle as pkl\n",
    "\n",
    "with open(f\"{DATA_PATH}/{AVG_WORD_VECTORS_DATA_FILE}\", mode=\"wb\") as file:\n",
    "    pkl.dump((X_wv_train, y_wv_train, X_wv_test, y_wv_test), file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report your accuracy values on the testing split for these models similar to HW1, i.e., for each of perceptron and SVM models, report two accuracy values Word2Vec and TF-IDF features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train and test data\n",
    "import pickle as pkl\n",
    "\n",
    "train_preprocessed, test_preprocessed = None, None\n",
    "with open(f\"{DATA_PATH}/{PREPROCESSED_DATA_FILE}\", mode=\"rb\") as file:\n",
    "    train_preprocessed, test_preprocessed = pkl.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/aditya/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aditya/.pyenv/versions/3.10.6/envs/csci544/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:524: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfVectorizer(tokenizer=&lt;function word_tokenize at 0x1678c8040&gt;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(tokenizer=&lt;function word_tokenize at 0x1678c8040&gt;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "TfidfVectorizer(tokenizer=<function word_tokenize at 0x1678c8040>)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=word_tokenize)\n",
    "\n",
    "# Using entire data to fit as the dataset is small and as using entire dataset is not needed for homework requirement\n",
    "all_data = pd.concat([train_preprocessed, test_preprocessed], axis=0)\n",
    "vectorizer.fit(all_data[DATA_COL])\n",
    "\n",
    "X_tfidf_train = vectorizer.transform(train_preprocessed[DATA_COL])\n",
    "X_tfidf_test = vectorizer.transform(test_preprocessed[DATA_COL])\n",
    "y_tfidf_train = train_preprocessed[TARGET_COL]\n",
    "y_tfidf_test = test_preprocessed[TARGET_COL]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save train and test TFIDF vectors\n",
    "import pickle as pkl\n",
    "\n",
    "with open(f\"{DATA_PATH}/{TFIDF_DATA_FILE}\", mode=\"wb\") as file:\n",
    "    pkl.dump((X_tfidf_train, y_tfidf_train, X_tfidf_test, y_tfidf_test), file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### TF-IDF Based Approach\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TF-IDF data\n",
    "import pickle as pkl\n",
    "\n",
    "X_tfidf_train, y_tfidf_train, X_tfidf_test, y_tfidf_test = None, None, None, None\n",
    "with open(f\"{DATA_PATH}/{TFIDF_DATA_FILE}\", mode=\"rb\") as file:\n",
    "    X_tfidf_train, y_tfidf_train, X_tfidf_test, y_tfidf_test = pkl.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Perceptron(alpha=0.012, class_weight=&#x27;balanced&#x27;, early_stopping=True,\n",
       "           max_iter=8000, random_state=42, tol=0.0001)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Perceptron</label><div class=\"sk-toggleable__content\"><pre>Perceptron(alpha=0.012, class_weight=&#x27;balanced&#x27;, early_stopping=True,\n",
       "           max_iter=8000, random_state=42, tol=0.0001)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "Perceptron(alpha=0.012, class_weight='balanced', early_stopping=True,\n",
       "           max_iter=8000, random_state=42, tol=0.0001)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.33      0.42      3999\n",
      "           1       0.30      0.59      0.40      4000\n",
      "           2       0.34      0.24      0.28      4000\n",
      "           3       0.39      0.36      0.38      3999\n",
      "           4       0.61      0.52      0.56      3998\n",
      "\n",
      "    accuracy                           0.41     19996\n",
      "   macro avg       0.44      0.41      0.41     19996\n",
      "weighted avg       0.44      0.41      0.41     19996\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "perceptron_tfidf_clf = Perceptron(\n",
    "    max_iter=8000, alpha=0.012, random_state=RANDOM_SEED, tol=1e-4, early_stopping=True, class_weight=\"balanced\"\n",
    ")\n",
    "\n",
    "perceptron_tfidf_clf.fit(X_tfidf_train, y_tfidf_train)\n",
    "\n",
    "y_tfidf_pred = perceptron_tfidf_clf.predict(X_tfidf_test)\n",
    "\n",
    "\n",
    "print(classification_report(y_tfidf_test, y_tfidf_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete Model and related variables\n",
    "del perceptron_tfidf_clf, y_tfidf_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Word2Vec Based Approach - Avg. Word Vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Avg. Word2Vec Data\n",
    "import pickle as pkl\n",
    "\n",
    "X_wv_train, y_wv_train, X_wv_test, y_wv_test = None, None, None, None\n",
    "with open(f\"{DATA_PATH}/{AVG_WORD_VECTORS_DATA_FILE}\", mode=\"rb\") as file:\n",
    "    X_wv_train, y_wv_train, X_wv_test, y_wv_test = pkl.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Perceptron(alpha=0.012, early_stopping=True, max_iter=8000, random_state=42,\n",
       "           tol=0.0001)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Perceptron</label><div class=\"sk-toggleable__content\"><pre>Perceptron(alpha=0.012, early_stopping=True, max_iter=8000, random_state=42,\n",
       "           tol=0.0001)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "Perceptron(alpha=0.012, early_stopping=True, max_iter=8000, random_state=42,\n",
       "           tol=0.0001)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      3999\n",
      "           1       0.20      1.00      0.33      4000\n",
      "           2       0.00      0.00      0.00      4000\n",
      "           3       0.00      0.00      0.00      3997\n",
      "           4       0.00      0.00      0.00      3997\n",
      "\n",
      "    accuracy                           0.20     19993\n",
      "   macro avg       0.04      0.20      0.07     19993\n",
      "weighted avg       0.04      0.20      0.07     19993\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aditya/.pyenv/versions/3.10.6/envs/csci544/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/aditya/.pyenv/versions/3.10.6/envs/csci544/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/aditya/.pyenv/versions/3.10.6/envs/csci544/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "perceptron_wv_clf = Perceptron(\n",
    "    max_iter=8000, alpha=0.012, random_state=RANDOM_SEED, tol=1e-4, early_stopping=True, class_weight=\"balanced\"\n",
    ")\n",
    "\n",
    "perceptron_wv_clf = Perceptron(max_iter=8000, alpha=0.012, random_state=RANDOM_SEED, tol=1e-4, early_stopping=True)\n",
    "\n",
    "perceptron_wv_clf.fit(X_wv_train, y_wv_train)\n",
    "\n",
    "y_wv_pred = perceptron_wv_clf.predict(X_wv_test)\n",
    "\n",
    "print(classification_report(y_wv_test, y_wv_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete model and related variables\n",
    "del perceptron_wv_clf, y_wv_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### TF-IDF Based\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TF-IDF data\n",
    "import pickle as pkl\n",
    "\n",
    "X_tfidf_train, y_tfidf_train, X_tfidf_test, y_tfidf_test = None, None, None, None\n",
    "with open(f\"{DATA_PATH}/{TFIDF_DATA_FILE}\", mode=\"rb\") as file:\n",
    "    X_tfidf_train, y_tfidf_train, X_tfidf_test, y_tfidf_test = pkl.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearSVC(C=0.1, class_weight=&#x27;balanced&#x27;, dual=False, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC(C=0.1, class_weight=&#x27;balanced&#x27;, dual=False, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearSVC(C=0.1, class_weight='balanced', dual=False, random_state=42)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.72      0.64      3999\n",
      "           1       0.42      0.31      0.36      4000\n",
      "           2       0.44      0.40      0.42      4000\n",
      "           3       0.49      0.42      0.45      3999\n",
      "           4       0.64      0.78      0.70      3998\n",
      "\n",
      "    accuracy                           0.53     19996\n",
      "   macro avg       0.51      0.53      0.51     19996\n",
      "weighted avg       0.51      0.53      0.51     19996\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "svm_tfidf_clf = LinearSVC(dual=False, C=0.1, max_iter=1000, class_weight=\"balanced\", random_state=RANDOM_SEED)\n",
    "\n",
    "svm_tfidf_clf.fit(X_tfidf_train, y_tfidf_train)\n",
    "\n",
    "y_tfidf_pred = svm_tfidf_clf.predict(X_tfidf_test)\n",
    "\n",
    "print(classification_report(y_tfidf_test, y_tfidf_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete model and variables\n",
    "del svm_tfidf_clf, y_tfidf_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Word2Vec Based Approach - Avg. Word Vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Avg. Word2Vec Data\n",
    "import pickle as pkl\n",
    "\n",
    "X_wv_train, y_wv_train, X_wv_test, y_wv_test = None, None, None, None\n",
    "with open(f\"{DATA_PATH}/{AVG_WORD_VECTORS_DATA_FILE}\", mode=\"rb\") as file:\n",
    "    X_wv_train, y_wv_train, X_wv_test, y_wv_test = pkl.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearSVC(C=0.1, dual=False, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC(C=0.1, dual=False, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearSVC(C=0.1, dual=False, random_state=42)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.20      0.96      0.33      3999\n",
      "           1       0.20      0.02      0.04      4000\n",
      "           2       0.00      0.00      0.00      4000\n",
      "           3       0.00      0.00      0.00      3997\n",
      "           4       0.34      0.01      0.03      3997\n",
      "\n",
      "    accuracy                           0.20     19993\n",
      "   macro avg       0.15      0.20      0.08     19993\n",
      "weighted avg       0.15      0.20      0.08     19993\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aditya/.pyenv/versions/3.10.6/envs/csci544/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/aditya/.pyenv/versions/3.10.6/envs/csci544/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/aditya/.pyenv/versions/3.10.6/envs/csci544/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "class_weight = {0: 0.9525, 1: 1.99825, 2: 1.9225, 3: 0.625, 4: 0.8585}\n",
    "\n",
    "svm_wv_clf = LinearSVC(dual=False, C=0.1, max_iter=1000, random_state=RANDOM_SEED)\n",
    "\n",
    "svm_wv_clf.fit(X_wv_train, y_wv_train)\n",
    "\n",
    "y_wv_avged_pred = svm_wv_clf.predict(X_wv_test)\n",
    "\n",
    "print(classification_report(y_wv_test, y_wv_avged_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete model and variables\n",
    "del svm_wv_clf, y_wv_avged_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you conclude from comparing performances for the models trained using the two different feature types (TF-IDF and your trained Word2Vec features)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## PyTorch Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as func\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "device = None\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using Device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data_loader, train_model, n_epochs, optimizer, criterion):\n",
    "    valid_loss_min = np.Inf\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss = 0.0\n",
    "\n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        train_model.train()  # prep model for training\n",
    "        for data, target in data_loader:\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = train_model(data)\n",
    "            # calculate the loss\n",
    "            loss = criterion(output, target)\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # update running training loss\n",
    "            train_loss += loss.item() * data.size(0)\n",
    "\n",
    "        # print training/validation statistics\n",
    "        # calculate average loss over an epoch\n",
    "        train_loss = train_loss / len(data_loader.dataset)\n",
    "\n",
    "        print(f\"Epoch: {epoch + 1} \\tTraining Loss: {train_loss:.6f}\")\n",
    "\n",
    "    return train_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, dataloader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_preds, y_true = list(), list()\n",
    "        for input, target in dataloader:\n",
    "            outputs = model(input)\n",
    "            print(outputs.shape)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            y_preds.append(predicted.cpu().numpy().squeeze())\n",
    "            y_true.append(target.cpu().numpy().squeeze())\n",
    "    return np.array(y_preds), np.array(y_true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward Neural Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Word2Vec features, train a feedforward multilayer perceptron net- work for classification. Consider a network with two hidden layers, each with 50 and 10 nodes, respectively. You can use cross entropy loss and your own choice for other hyperparamters, e.g., nonlinearity, number of epochs, etc. Part of getting good results is to select good values for these hyperparamters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FNN_BATCH_SIZE = 200\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create FNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNN(nn.Module):\n",
    "    def __init__(self, n_input, n_output, dropout_rate) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_input = n_input\n",
    "        self.n_hidden_1 = 50\n",
    "        self.n_hidden_2 = 10\n",
    "        self.n_output = n_output\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.fc1 = nn.Linear(self.n_input, self.n_hidden_1)\n",
    "        self.fc2 = nn.Linear(self.n_hidden_1, self.n_hidden_2)\n",
    "        self.fc3 = nn.Linear(self.n_hidden_2, self.n_output)\n",
    "        self.dropout = nn.Dropout(self.dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = func.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = func.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class AmazonReviewsDataset(torch.utils.data.Dataset):\n",
    "    \"Characterizes a dataset for PyTorch\"\n",
    "\n",
    "    def __init__(self, inputs, transform=None):\n",
    "        \"Initialization\"\n",
    "        self.data = inputs\n",
    "\n",
    "    def __len__(self):\n",
    "        \"Denotes the total number of samples\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"Generates one sample of data\"\n",
    "        if torch.is_tensor(index):\n",
    "            index = index.tolist()\n",
    "\n",
    "        input, label = self.data[index][0], self.data[index][1]\n",
    "\n",
    "        return input, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Using Avg. Word2Vec Vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Avg. Word2Vec Data\n",
    "import pickle as pkl\n",
    "\n",
    "X_wv_train, y_wv_train, X_wv_test, y_wv_test = None, None, None, None\n",
    "with open(f\"{DATA_PATH}/{AVG_WORD_VECTORS_DATA_FILE}\", mode=\"rb\") as file:\n",
    "    X_wv_train, y_wv_train, X_wv_test, y_wv_test = pkl.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnn_wv_model = FNN(n_input=300, n_output=5, dropout_rate=0.2).to(device)\n",
    "fnn_wv_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function and Optimizer\n",
    "criterion_wv = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_wv = torch.optim.Adam(fnn_wv_model.parameters(), lr=1e-2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_wv_train_tensor = torch.FloatTensor(X_wv_train.values).to(device)\n",
    "y_wv_train_tensor = torch.LongTensor(y_wv_train.values).to(device)\n",
    "X_wv_test_tensor = torch.FloatTensor(X_wv_test.values).to(device)\n",
    "y_wv_test_tensor = torch.LongTensor(y_wv_test.values).to(device)\n",
    "\n",
    "fnn_wv_train = AmazonReviewsDataset(TensorDataset(X_wv_train_tensor, y_wv_train_tensor))\n",
    "fnn_wv_train_loader = DataLoader(fnn_wv_train, batch_size=FNN_BATCH_SIZE, drop_last=True, shuffle=True)\n",
    "\n",
    "fnn_wv_test = AmazonReviewsDataset(TensorDataset(X_wv_test_tensor, y_wv_test_tensor))\n",
    "fnn_wv_test_loader = DataLoader(fnn_wv_test, batch_size=FNN_BATCH_SIZE, drop_last=True, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnn_wv_model = train_model(fnn_wv_train_loader, fnn_wv_model, 1, optimizer_wv, criterion_wv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_wv_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_wv_pred = predict(fnn_wv_model, fnn_wv_test_loader)\n",
    "\n",
    "print(y_wv_pred.shape, y_wv_test.shape)\n",
    "\n",
    "print(classification_report(y_wv_pred, y_wv_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_wv_pred[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_wv_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report accuracy on the testing split.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del fnn_wv_model, criterion_wv, optimizer_wv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Concatenate First 10 Word2Vec Vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_top_n_wv(sentence: str, w2v_model, n=10):\n",
    "    top_n_wvs = np.zeros(300 * n, dtype=np.float64)\n",
    "    count = 1\n",
    "\n",
    "    words = sentence.split(\" \")\n",
    "    n_words = len(words)\n",
    "\n",
    "    for word in words:\n",
    "        if count == n or count == n_words:\n",
    "            break\n",
    "        count += 1\n",
    "        try:\n",
    "            top_n_wvs[(count - 1) * 300 : count * 300] = w2v_model[word]\n",
    "        except KeyError:\n",
    "            count -= 1\n",
    "\n",
    "    if np.all((top_n_wvs == 0)):\n",
    "        return np.nan\n",
    "\n",
    "    return top_n_wvs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Apply Top 10 Word Vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned but not preprocessed data\n",
    "import pickle as pkl\n",
    "\n",
    "train_wv_top_10, test_wv_top_10 = None, None\n",
    "with open(f\"{DATA_PATH}/{DATA_FILE}\", mode=\"rb\") as file:\n",
    "    train_wv_top_10, test_wv_top_10 = pkl.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "# Preprocess using gensim'simple_preprocess\n",
    "train_wv_top_10[\"review_body\"] = train_wv_top_10[\"review_body\"].apply(simple_preprocess)\n",
    "test_wv_top_10[\"review_body\"] = test_wv_top_10[\"review_body\"].apply(simple_preprocess)\n",
    "\n",
    "TEMP_COL = \"top_10_wv\"\n",
    "VECTOR_COLS = [\"vector_\" + f\"_{i}\" for i in range(3000)]\n",
    "\n",
    "# Train\n",
    "train_wv_top_10[TEMP_COL] = train_wv_top_10[DATA_COL].apply(partial(concatenate_top_n_wv, w2v_model=w2v_google, n=10))\n",
    "\n",
    "wv_df = pd.DataFrame(train_wv_top_10[TEMP_COL].to_list(), index=train_wv_top_10[TEMP_COL].index, columns=VECTOR_COLS)\n",
    "\n",
    "train_wv_top_10 = pd.concat([train_wv_top_10, wv_df], axis=1)\n",
    "train_wv_top_10.dropna(inplace=True)\n",
    "\n",
    "X_wv_top_10_train = train_wv_top_10.drop([DATA_COL, TEMP_COL, TARGET_COL], axis=1)\n",
    "y_wv_top_10_train = train_wv_top_10[TARGET_COL]\n",
    "\n",
    "\n",
    "# Test\n",
    "test_wv_top_10[TEMP_COL] = test[DATA_COL].apply(partial(concatenate_top_n_wv, w2v_model=w2v_google, n=10))\n",
    "\n",
    "wv_df = pd.DataFrame(test_wv_top_10[TEMP_COL].to_list(), index=test_wv_top_10[TEMP_COL].index, columns=VECTOR_COLS)\n",
    "\n",
    "test_wv_top_10 = pd.concat([test_wv_top_10, wv_df], axis=1)\n",
    "test_wv_top_10.dropna(inplace=True)\n",
    "\n",
    "X_wv_top_10_test = test_wv_top_10.drop([DATA_COL, TEMP_COL, TARGET_COL], axis=1)\n",
    "y_wv_top_10_test = test_wv_top_10[TARGET_COL]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the avg'ed word vectors dataset\n",
    "import pickle as pkl\n",
    "\n",
    "with open(f\"{DATA_PATH}/{TOP_10_WORD_VECTORS_DATA_FILE}\", mode=\"wb\") as file:\n",
    "    pkl.dump((X_wv_top_10_train, y_wv_top_10_train, X_wv_top_10_test, y_wv_top_10_test), file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Using Top 10 Word2Vec Vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Avg. Word2Vec Data\n",
    "import pickle as pkl\n",
    "\n",
    "X_wv_top_10_train, y_wv_top_10_train, X_wv_top_10_test, y_wv_top_10_test = None, None, None, None\n",
    "with open(f\"{DATA_PATH}/{AVG_WORD_VECTORS_DATA_FILE}\", mode=\"rb\") as file:\n",
    "    X_wv_top_10_train, y_wv_top_10_train, X_wv_top_10_test, y_wv_top_10_test = pkl.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_wv_top_10_train_tensor = torch.FloatTensor(X_wv_top_10_train.values).to(device)\n",
    "y_wv_top_10_train_tensor = torch.LongTensor(y_wv_top_10_train.values).to(device)\n",
    "X_wv_top_10_test_tensor = torch.FloatTensor(X_wv_top_10_test.values).to(device)\n",
    "y_wv_top_10_test_tensor = torch.LongTensor(y_wv_top_10_test.values).to(device)\n",
    "\n",
    "fnn_wv_top_10_train = AmazonReviewsDataset(TensorDataset(X_wv_top_10_train_tensor, y_wv_top_10_train_tensor))\n",
    "fnn_wv_top_10_train_loader = DataLoader(fnn_wv_top_10_train, batch_size=FNN_BATCH_SIZE, drop_last=True, shuffle=True)\n",
    "\n",
    "fnn_wv_top_10_test = AmazonReviewsDataset(TensorDataset(X_wv_top_10_test_tensor, y_wv_top_10_test_tensor))\n",
    "fnn_wv_top_10_test_loader = DataLoader(fnn_wv_top_10_test, batch_size=1, drop_last=True, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnn_wv_top_10_model = FNN(n_input=300, n_output=5, dropout_rate=0.2).to(device)\n",
    "fnn_wv_top_10_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function and Optimizer\n",
    "criterion_wv_top_10 = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_wv_top_10 = torch.optim.Adam(fnn_wv_top_10_model.parameters(), lr=1e-2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnn_wv_top_10_model = train_model(\n",
    "    fnn_wv_top_10_train_loader, fnn_wv_top_10_model, 1, optimizer_wv_top_10, criterion_wv_top_10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_wv_top_10_pred = predict(fnn_wv_top_10_model, fnn_wv_top_10_test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report the accuracy value on the testing split for your MLP model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del fnn_wv_top_10_model, criterion_wv_top_10, optimizer_wv_top_10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you conclude by comparing accuracy values you obtain with those obtained in the “’Simple Models” section?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset with Review Length of 20 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_words(words: List[str], w2v_model, n=20):\n",
    "    result_array = np.zeros((20, 300), dtype=np.float32)\n",
    "    idx = 0\n",
    "    for word in words:\n",
    "        if idx == n or idx == len(words):\n",
    "            break\n",
    "        try:\n",
    "            result_array[idx] = w2v_model[word]\n",
    "            idx += 1\n",
    "        except KeyError:\n",
    "            continue\n",
    "    if idx == 0:\n",
    "        return np.nan\n",
    "    return result_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned but not preprocessed data\n",
    "import pickle as pkl\n",
    "\n",
    "train_wv_20_words, test_wv_20_words = None, None\n",
    "with open(f\"{DATA_PATH}/{DATA_FILE}\", mode=\"rb\") as file:\n",
    "    train_wv_20_words, test_wv_20_words = pkl.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "# Preprocess using gensim'simple_preprocess\n",
    "train_wv_20_words[\"review_body\"] = train_wv_20_words[\"review_body\"].apply(simple_preprocess)\n",
    "test_wv_20_words[\"review_body\"] = test_wv_20_words[\"review_body\"].apply(simple_preprocess)\n",
    "\n",
    "TEMP_COL = \"avg_word_vector\"\n",
    "VECTOR_COLS = [f\"vector_{i}\" for i in range(300)]\n",
    "\n",
    "# Train Data\n",
    "train_wv_20_words[TEMP_COL] = train_wv_20_words[DATA_COL].apply(partial(get_top_n_words, w2v_model=w2v_google))\n",
    "\n",
    "wv_df = pd.DataFrame(\n",
    "    train_wv_20_words[TEMP_COL].to_list(), index=train_wv_20_words[TEMP_COL].index, columns=VECTOR_COLS\n",
    ")\n",
    "wv_df.dropna(inplace=True)\n",
    "\n",
    "train_wv_20_words = pd.concat([train_wv_20_words, wv_df], axis=1)\n",
    "train_wv_20_words.dropna(inplace=True)\n",
    "\n",
    "X_wv_20_words_train = train_wv_20_words.drop([DATA_COL, TEMP_COL, TARGET_COL], axis=1)\n",
    "y_wv_20_words_train = train_wv_20_words[TARGET_COL]\n",
    "\n",
    "\n",
    "# Test Data\n",
    "test_wv_20_words[TEMP_COL] = test[DATA_COL].apply(partial(get_top_n_words, w2v_model=w2v_google))\n",
    "\n",
    "wv_df = pd.DataFrame(test_wv_20_words[TEMP_COL].to_list(), index=test_wv_20_words[TEMP_COL].index, columns=VECTOR_COLS)\n",
    "wv_df.dropna(inplace=True)\n",
    "\n",
    "test_wv = pd.concat([test_wv_20_words, wv_df], axis=1)\n",
    "test_wv.dropna(inplace=True)\n",
    "\n",
    "X_wv_20_words_test = test_wv.drop([DATA_COL, TEMP_COL, TARGET_COL], axis=1)\n",
    "y_wv_20_words_test = test_wv[TARGET_COL]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the top 20 words word vectors dataset\n",
    "import pickle as pkl\n",
    "\n",
    "with open(f\"{DATA_PATH}/{WORDS_20_WORD_VECTORS_DATA_FILE}\", mode=\"wb\") as file:\n",
    "    pkl.dump((X_wv_20_words_train, y_wv_20_words_train, X_wv_20_words_test, y_wv_20_words_test), file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working on RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the top 20 word vectors dataset\n",
    "import pickle as pkl\n",
    "\n",
    "X_wv_20_words_train, y_wv_20_words_train, X_wv_20_words_test, y_rnn_test = None, None, None, None\n",
    "with open(f\"{DATA_PATH}/{WORDS_20_WORD_VECTORS_DATA_FILE}\", mode=\"wb\") as file:\n",
    "    X_wv_20_words_train, y_wv_20_words_train, X_wv_20_words_test, y_rnn_test = pkl.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_wv_20_words_train_tensor = torch.FloatTensor(X_wv_20_words_train.values).to(device)\n",
    "y_wv_20_words_train_tensor = torch.LongTensor(y_wv_20_words_train.values).to(device)\n",
    "X_wv_20_words_test_tensor = torch.FloatTensor(X_wv_20_words_test.values).to(device)\n",
    "y_wv_20_words_test_tensor = torch.LongTensor(y_rnn_test.values).to(device)\n",
    "\n",
    "rnn_train = AmazonReviewsDataset(TensorDataset(X_wv_train_tensor, y_wv_train_tensor))\n",
    "rnn_train_loader = DataLoader(fnn_wv_train, batch_size=FNN_BATCH_SIZE, drop_last=True, shuffle=True)\n",
    "\n",
    "rnn_test = AmazonReviewsDataset(TensorDataset(X_wv_test_tensor, y_wv_test_tensor))\n",
    "rnn_test_loader = DataLoader(fnn_wv_test, batch_size=FNN_BATCH_SIZE, drop_last=True, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size, n_hidden_states):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.n_hidden_states = n_hidden_states\n",
    "        self.n_layers = 1\n",
    "        self.rnn = torch.nn.RNN(input_size, self.n_hidden_states, self.n_layers, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(self.n_hidden_states, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.n_hidden_states)\n",
    "        return hidden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_rnn = SimpleRNN(input_size=300, n_hidden_states=20)\n",
    "simple_rnn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function and Optimizer\n",
    "criterion_simple_rnn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_simple_rnn = torch.optim.Adam(simple_rnn.parameters(), lr=1e-2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_rnn = train_model(rnn_train_loader, simple_rnn, 1, optimizer_simple_rnn, criterion_simple_rnn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_rnn_pred = predict(simple_rnn, rnn_test_loader)\n",
    "\n",
    "print(y_rnn_pred.shape, y_rnn_test.shape)\n",
    "\n",
    "print(classification_report(y_rnn_pred, y_rnn_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del simple_rnn, criterion_simple_rnn, optimizer_simple_rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report accuracy values on the testing split for your RNN model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you conclude by comparing accuracy values you obtain with those obtained with feedforward neural network models?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gated Recurrent Unit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_size, n_hidden_states):\n",
    "        super(GRU, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.n_hidden_states = n_hidden_states\n",
    "        self.n_layers = 1\n",
    "        self.rnn = torch.nn.GRU(300, self.hidden_size, self.n_layers, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(self.hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_size)\n",
    "        return hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru = GRU(input_size=300, n_hidden_states=20)\n",
    "gru\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function and Optimizer\n",
    "criterion_gru = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_gru = torch.optim.Adam(gru.parameters(), lr=1e-2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_rnn = train_model(rnn_train_loader, simple_rnn, 1, optimizer_simple_rnn, criterion_simple_rnn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_gru_pred = predict(gru, rnn_test_loader)\n",
    "\n",
    "print(y_gru_pred.shape, y_rnn_test.shape)\n",
    "\n",
    "print(classification_report(y_gru_pred, y_rnn_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you conclude by comparing accuracy values you obtain with those obtained using simple RNN?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Answer\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "e78917e7f90f3892e7e12462ef46781cf5994bd706032ea53be00d0b1f29dcb9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
