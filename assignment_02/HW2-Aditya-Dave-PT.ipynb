{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow multiple print statements in a cell in Jupyter Notebook\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/aditya/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emot in /Users/aditya/.pyenv/versions/3.10.6/envs/csci544/lib/python3.10/site-packages (3.1)\n",
      "Requirement already satisfied: contractions in /Users/aditya/.pyenv/versions/3.10.6/envs/csci544/lib/python3.10/site-packages (0.1.72)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in /Users/aditya/.pyenv/versions/3.10.6/envs/csci544/lib/python3.10/site-packages (from contractions) (0.0.21)\n",
      "Requirement already satisfied: pyahocorasick in /Users/aditya/.pyenv/versions/3.10.6/envs/csci544/lib/python3.10/site-packages (from textsearch>=0.0.21->contractions) (1.4.4)\n",
      "Requirement already satisfied: anyascii in /Users/aditya/.pyenv/versions/3.10.6/envs/csci544/lib/python3.10/site-packages (from textsearch>=0.0.21->contractions) (0.3.1)\n"
     ]
    }
   ],
   "source": [
    "# Install some dependencies\n",
    "! pip install emot contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "### Data Files\n",
    "####################################################\n",
    "\n",
    "DATA_PATH = \"/Volumes/dataTwo/usc/CSCI_544/assignment_02/data\"\n",
    "MODEL_PATH = \"/Volumes/dataTwo/usc/CSCI_544/assignment_02/model\"\n",
    "\n",
    "ORIGINAL_DATA_FILE = \"amazon_reviews_us_Jewelry_v1_00.tsv\"\n",
    "SAMPLED_DATA_FILE = \"data_sampled.csv\"\n",
    "CLEANED_DATA_FILE = \"data_cleaned.csv\"\n",
    "\n",
    "# Files after clean and split\n",
    "DATA_FILE = \"data.pkl\"\n",
    "\n",
    "# Files after preprocessing the splitted dataset\n",
    "PREPROCESSED_DATA_FILE = \"data_preprocessed.csv\"\n",
    "\n",
    "# Files containing the tfidf data\n",
    "TFIDF_DATA_FILE = \"data_tfidf.csv\"\n",
    "\n",
    "# custom created word vectors for the review dataset\n",
    "CUSTOM_WORD_VECTORS_MODEL_FILE = \"gensim_w2v_amazon_reviews_model\"\n",
    "\n",
    "# train and test data for word2vec avg. word vectors approach\n",
    "AVG_WORD_VECTORS_DATA_FILE = \"data_avg_word_vectors.pkl\"\n",
    "\n",
    "# train and test data for word2vec contatenate top 10 vectors appraoch\n",
    "TOP_10_WORD_VECTORS_DATA_FILE = \"data_avg_word_vectors.pkl\"\n",
    "\n",
    "# train and test data for word2vec contatenate top 10 vectors appraoch\n",
    "WORDS_20_WORD_VECTORS_DATA_FILE = \"data_words_20_word_vectors.pkl\"\n",
    "\n",
    "\n",
    "####################################################\n",
    "### Model Files\n",
    "####################################################\n",
    "\n",
    "PERCEPTRON_TFIDF_MODEL_FILE = \"perceptron_tfidf.model\"\n",
    "PERCEPTRON_AVG_WV_MODEL_FILE = \"perceptron_avg_wv.model\"\n",
    "\n",
    "SVM_TFIDF_MODEL_FILE = \"svm_tfidf.model\"\n",
    "SVM_AVG_WV_MODEL_FILE = \"svh_avg_wv.model\"\n",
    "\n",
    "FNN_AVG_WV_MODEL_FILE = \"fnn_avg_wv.pth\"\n",
    "FNN_TOP_10_WV_MODEL_FILE = \"fnn_top_10_wv.pth\"\n",
    "\n",
    "RNN_TOP_20_WV_MODEL_FILE = \"rnn_top_20_wv.pth\"\n",
    "GRU_TOP_20_WV_MODEL_FILE = \"gru_top_20_wv.pth\"\n",
    "\n",
    "\n",
    "DATA_COL = \"review_body\"\n",
    "TARGET_COL = \"star_rating\"\n",
    "\n",
    "N_SAMPLES = 20000\n",
    "\n",
    "RANDOM_SEED = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(RANDOM_SEED)\n",
    "rng = np.random.default_rng(seed=RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aditya/.pyenv/versions/3.10.6/envs/csci544/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3398: DtypeWarning: Columns (7) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>so beautiful even tho clearly not high end ......</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>Great product.. I got this set for my mother, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Exactly as pictured and my daughter's friend l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Love it. Fits great. Super comfortable and nea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Got this as a Mother's Day gift for my Mom and...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   star_rating                                        review_body\n",
       "0            4  so beautiful even tho clearly not high end ......\n",
       "1            4  Great product.. I got this set for my mother, ...\n",
       "2            4  Exactly as pictured and my daughter's friend l...\n",
       "3            4  Love it. Fits great. Super comfortable and nea...\n",
       "4            4  Got this as a Mother's Day gift for my Mom and..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the tab separated data file, and print the first 5 rows for confirmation\n",
    "data = pd.read_csv(f\"{DATA_PATH}/{ORIGINAL_DATA_FILE}\", sep=\"\\t\", usecols=[TARGET_COL, DATA_COL], low_memory=True)\n",
    "\n",
    "# Drop NA values\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Drop the outlier which is star_rating = \"2012-12-21\"\n",
    "data = data[data.star_rating != \"2012-12-21\"]\n",
    "\n",
    "# Convert all star rating to integer\n",
    "data[TARGET_COL] = data.star_rating.astype(int)\n",
    "\n",
    "# Make target col in range of 0-4\n",
    "data[TARGET_COL] = data[TARGET_COL] - 1\n",
    "\n",
    "# Remove nan valued rows\n",
    "data = data[data.review_body.notnull()]\n",
    "\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_data = data.groupby(TARGET_COL, group_keys=False).apply(lambda x: x.sample(N_SAMPLES, random_state=RANDOM_SEED))\n",
    "sampled_data.reset_index(inplace=True, drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data without cleaning\n",
    "sampled_data.to_csv(f\"{DATA_PATH}/{SAMPLED_DATA_FILE}\", header=True, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free Some Memory\n",
    "del data, sampled_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all reviews to lower case (optional according to study)\n",
    "def to_lower(data: pd.Series):\n",
    "    return data.str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_accented_characters(data: pd.Series):\n",
    "    import unicodedata\n",
    "\n",
    "    \"\"\"Removes accented characters from the Series\n",
    "\n",
    "    Args:\n",
    "        data (pd.Series): Series of string\n",
    "\n",
    "    Returns:\n",
    "        _type_: pd.Series\n",
    "    \"\"\"\n",
    "    import unicodedata\n",
    "\n",
    "    return data.apply(lambda x: unicodedata.normalize(\"NFKD\", x).encode(\"ascii\", \"ignore\").decode(\"utf-8\", \"ignore\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_encodings(data: pd.Series):\n",
    "    return data.str.replace(r\"&#\\d+;\", \" \", regex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_tags(data: pd.Series):\n",
    "    return data.str.replace(r\"<[a-zA-Z]+\\s?/?>\", \" \", regex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(data: pd.Series):\n",
    "    return data.str.replace(r\"https?://([\\w\\-\\._]+){2,}/[\\w\\-\\.\\-/=\\+_\\?]+\", \" \", regex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_and_url(data: pd.Series):\n",
    "    \"\"\"Function to remove\n",
    "             1. HTML encodings\n",
    "             2. HTML tags (both closed and open)\n",
    "             3. URLs\n",
    "\n",
    "    Args:\n",
    "        data (pd.Series): A Pandas series of type string\n",
    "\n",
    "    Returns:\n",
    "        _type_: pd.Series\n",
    "    \"\"\"\n",
    "    # Remove HTML encodings\n",
    "    data.str.replace(r\"&#\\d+;\", \" \", regex=True)\n",
    "\n",
    "    # Remove HTML tags (both open and closed)\n",
    "    data.str.replace(r\"<[a-zA-Z]+\\s?/?>\", \" \", regex=True)\n",
    "\n",
    "    # Remove URLs\n",
    "    data.str.replace(r\"https?://([\\w\\-\\._]+){2,}/[\\w\\-\\.\\-/=\\+_\\?]+\", \" \", regex=True)\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle emoji\n",
    "def convert_emoji_to_txt(data: pd.Series):\n",
    "    from emot.emo_unicode import UNICODE_EMOJI, EMOTICONS_EMO\n",
    "\n",
    "    EMO_TO_TXT_DICT = dict()\n",
    "    for emot in UNICODE_EMOJI:\n",
    "        EMO_TO_TXT_DICT[emot] = f\" {re.sub(r',|:|_', '', UNICODE_EMOJI[emot])} \"\n",
    "\n",
    "    for emo in EMOTICONS_EMO:\n",
    "        EMO_TO_TXT_DICT[emot] = f\" {re.sub(r',| ', '', EMOTICONS_EMO[emo])} \"\n",
    "\n",
    "    def convert_emojis(text, emo_to_txt_dict):\n",
    "        for emot in emo_to_txt_dict:\n",
    "            text = text.replace(emot, emo_to_txt_dict[emot])\n",
    "        return text\n",
    "\n",
    "    return data.apply(lambda x: convert_emojis(x, EMO_TO_TXT_DICT))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove non-alphabetical characters\n",
    "def remove_non_alpha_characters(data: pd.Series):\n",
    "    return data.str.replace(r\"_+|\\\\|[^a-zA-Z0-9\\s]\", \" \", regex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove extra spaces\n",
    "def remove_extra_spaces(data: pd.Series):\n",
    "    return data.str.replace(r\"^\\s*|\\s\\s*\", \" \", regex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expanding contractions\n",
    "def fix_contractions(data: pd.Series):\n",
    "    import contractions\n",
    "\n",
    "    def contraction_fixer(txt: str):\n",
    "        return \" \".join([contractions.fix(word) for word in txt.split()])\n",
    "\n",
    "    return data.apply(contraction_fixer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Too small even for the knuckles.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Did not fit right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>This stupid kit has 16 gauge needles not 14gauge.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>I would not suggest this item I bought the one...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>I am sure that it will be lovely once I get it...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   star_rating                                        review_body\n",
       "0            0                   Too small even for the knuckles.\n",
       "1            0                                  Did not fit right\n",
       "2            0  This stupid kit has 16 gauge needles not 14gauge.\n",
       "3            0  I would not suggest this item I bought the one...\n",
       "4            0  I am sure that it will be lovely once I get it..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read Sample Data\n",
    "sampled_data = pd.read_csv(f\"{DATA_PATH}/{SAMPLED_DATA_FILE}\", sep=\",\")\n",
    "sampled_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting: convert_emoji_to_txt\n",
      "Ended: convert_emoji_to_txt\n",
      "Starting: to_lower\n",
      "Ended: to_lower\n",
      "Starting: remove_accented_characters\n",
      "Ended: remove_accented_characters\n",
      "Starting: remove_html_encodings\n",
      "Ended: remove_html_encodings\n",
      "Starting: remove_html_tags\n",
      "Ended: remove_html_tags\n",
      "Starting: remove_url\n",
      "Ended: remove_url\n",
      "Starting: fix_contractions\n",
      "Ended: fix_contractions\n",
      "Starting: remove_non_alpha_characters\n",
      "Ended: remove_non_alpha_characters\n",
      "Starting: remove_extra_spaces\n",
      "Ended: remove_extra_spaces\n"
     ]
    }
   ],
   "source": [
    "# A dictionary containing the columns and a list of functions to perform on it in order\n",
    "data_cleaning_pipeline = {\n",
    "    DATA_COL: [\n",
    "        convert_emoji_to_txt,\n",
    "        to_lower,\n",
    "        remove_accented_characters,\n",
    "        remove_html_encodings,\n",
    "        remove_html_tags,\n",
    "        remove_url,\n",
    "        fix_contractions,\n",
    "        remove_non_alpha_characters,\n",
    "        remove_extra_spaces,\n",
    "    ]\n",
    "}\n",
    "\n",
    "cleaned_data = sampled_data.copy()\n",
    "\n",
    "# Process all the cleaning instructions\n",
    "for col, pipeline in data_cleaning_pipeline.items():\n",
    "    # Get the column to perform cleaning on\n",
    "    temp_data = cleaned_data[col].copy()\n",
    "\n",
    "    # Perform all the cleaning functions sequencially\n",
    "    for func in pipeline:\n",
    "        print(f\"Starting: {func.__name__}\")\n",
    "        temp_data = func(temp_data)\n",
    "        print(f\"Ended: {func.__name__}\")\n",
    "\n",
    "    # Replace the old column with cleaned one.\n",
    "    cleaned_data[col] = temp_data.copy()\n",
    "\n",
    "\n",
    "# Remove empty reviews\n",
    "cleaned_data = cleaned_data[cleaned_data[DATA_COL].str.len() != 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store data file\n",
    "cleaned_data.to_csv(f\"{DATA_PATH}/{CLEANED_DATA_FILE}\", sep=\",\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free Some Memory\n",
    "del sampled_data, cleaned_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing \n",
    "(used by TF-IDF Models)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(data: pd.Series):\n",
    "    from nltk.tokenize import word_tokenize\n",
    "\n",
    "    nltk.download(\"punkt\")\n",
    "\n",
    "    return data.apply(word_tokenize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Set\n",
    "\n",
    "\n",
    "def remove_stopwords(data: pd.Series):\n",
    "    \"\"\"Remove stop words using the NLTK stopwords dictionary\n",
    "\n",
    "    Args:\n",
    "        string (str): a document\n",
    "\n",
    "    Returns:\n",
    "        str: a document with stopwords removed\n",
    "    \"\"\"\n",
    "    from nltk.corpus import stopwords\n",
    "\n",
    "    nltk.download(\"stopwords\")\n",
    "\n",
    "    stopwords = set(stopwords.words())\n",
    "\n",
    "    def remover(word_list: List[str], stopwords: Set[str]):\n",
    "        return [word for word in word_list if not word in stopwords]\n",
    "\n",
    "    return data.apply(lambda word_list: remover(word_list, stopwords))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(data: pd.Series, consider_pos_tag: bool = True):\n",
    "    from nltk.corpus import wordnet\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "    nltk.download(\"omw-1.4\")\n",
    "\n",
    "    # POS tagging\n",
    "    def perform_nltk_pos_tag(data: pd.Series):\n",
    "        from nltk import pos_tag\n",
    "\n",
    "        nltk.download(\"averaged_perceptron_tagger\")\n",
    "\n",
    "        return data.apply(pos_tag)\n",
    "\n",
    "    # Convert POS tag to wordnet pos tags\n",
    "    def wordnet_pos_tagger(tag: str):\n",
    "        if tag.startswith(\"J\"):\n",
    "            return wordnet.ADJ\n",
    "        elif tag.startswith(\"V\"):\n",
    "            return wordnet.VERB\n",
    "        elif tag.startswith(\"N\"):\n",
    "            return wordnet.NOUN\n",
    "        elif tag.startswith(\"R\"):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = list()\n",
    "\n",
    "    if consider_pos_tag:\n",
    "        pos_tagged_data = data.copy()\n",
    "        pos_tagged_data = perform_nltk_pos_tag(data)\n",
    "\n",
    "        for row in pos_tagged_data:\n",
    "\n",
    "            lemmatized_row = list()\n",
    "\n",
    "            if consider_pos_tag:\n",
    "                for word, tag in row:\n",
    "                    wordnet_pos_tag = wordnet_pos_tagger(tag)\n",
    "\n",
    "                    if wordnet_pos_tag is None:\n",
    "                        lemmatized_row.append(word)\n",
    "                    else:\n",
    "                        result = lemmatizer.lemmatize(word, wordnet_pos_tag)\n",
    "                        lemmatized_row.append(lemmatizer.lemmatize(word, wordnet_pos_tag))\n",
    "\n",
    "            lemmatized.append(lemmatized_row)\n",
    "    else:\n",
    "        for row in data:\n",
    "            lemmatized_row = list()\n",
    "\n",
    "            for word in row:\n",
    "                lemmatized_row.append(lemmatizer.lemmatize(word))\n",
    "\n",
    "            lemmatized.append(lemmatized_row)\n",
    "\n",
    "    return pd.Series(lemmatized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate lemmatized sentences back into one sentence\n",
    "def concatenate(data: pd.Series):\n",
    "    return data.apply(lambda words: \" \".join(words))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data 80-20 split\n",
    "import pickle as pkl\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# load clean data\n",
    "cleaned_data = pd.read_csv(f\"{DATA_PATH}/{CLEANED_DATA_FILE}\")\n",
    "\n",
    "# split cleaned data into train and test\n",
    "train, test = train_test_split(cleaned_data, test_size=0.2, stratify=cleaned_data[TARGET_COL], random_state=RANDOM_SEED)\n",
    "train.reset_index(inplace=True, drop=True)\n",
    "test.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# save the split\n",
    "with open(f\"{DATA_PATH}/{DATA_FILE}\", mode=\"wb\") as file:\n",
    "    pkl.dump((train, test), file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free Some Memory\n",
    "del cleaned_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "train, test = None, None\n",
    "with open(f\"{DATA_PATH}/{DATA_FILE}\", mode=\"rb\") as file:\n",
    "    train, test = pkl.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting: tokenize\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/aditya/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/aditya/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ended: tokenize\n",
      "Starting: lemmatize\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /Users/aditya/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/aditya/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/aditya/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/aditya/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ended: lemmatize\n",
      "Starting: concatenate\n",
      "Ended: concatenate\n"
     ]
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "preprocessing_pipeline = {DATA_COL: [tokenize, lemmatize, concatenate]}\n",
    "\n",
    "# Run the pipeline\n",
    "preprocessed_train_data = train.copy()\n",
    "preprocessed_test_data = test.copy()\n",
    "\n",
    "# Process all the cleaning instructions\n",
    "for col, pipeline in preprocessing_pipeline.items():\n",
    "    # Get the column to perform cleaning on\n",
    "    temp_data_train = preprocessed_train_data[col].copy()\n",
    "    temp_data_test = preprocessed_test_data[col].copy()\n",
    "\n",
    "    # Perform all the cleaning functions sequencially\n",
    "    for func in pipeline:\n",
    "        print(f\"Starting: {func.__name__}\")\n",
    "\n",
    "        if func.__name__ == \"lemmatize\":\n",
    "            temp_data_train = func(temp_data_train, consider_pos_tag=True)\n",
    "            temp_data_test = func(temp_data_test, consider_pos_tag=True)\n",
    "        else:\n",
    "            temp_data_train = func(temp_data_train)\n",
    "            temp_data_test = func(temp_data_test)\n",
    "\n",
    "        print(f\"Ended: {func.__name__}\")\n",
    "\n",
    "    # Replace the old column with cleaned one.\n",
    "    preprocessed_train_data[col] = temp_data_train\n",
    "    preprocessed_test_data[col] = temp_data_test\n",
    "\n",
    "# Remove empty reviews\n",
    "preprocessed_train_data = preprocessed_train_data[preprocessed_train_data[DATA_COL].str.len() != 0]\n",
    "preprocessed_test_data = preprocessed_test_data[preprocessed_test_data[DATA_COL].str.len() != 0]\n",
    "\n",
    "# Remove NaN\n",
    "preprocessed_train_data.dropna(inplace=True)\n",
    "preprocessed_test_data.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the preprocessed data\n",
    "with open(f\"{DATA_PATH}/{PREPROCESSED_DATA_FILE}\", mode=\"wb\") as file:\n",
    "    pkl.dump((preprocessed_train_data, preprocessed_test_data), file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>purchase this for my daughter she be very happ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>buy it to replace a broken chain and it be ver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>i love the design but unfortunately it be so c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>this item turn out to be much chunky than i ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>i order dark palette and i get spring pastel t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   star_rating                                        review_body\n",
       "0            4  purchase this for my daughter she be very happ...\n",
       "1            3  buy it to replace a broken chain and it be ver...\n",
       "2            0  i love the design but unfortunately it be so c...\n",
       "3            1  this item turn out to be much chunky than i ha...\n",
       "4            1  i order dark palette and i get spring pastel t..."
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_train_data.head() #come_here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free Some Memory\n",
    "del train, test, preprocessed_train_data, preprocessed_test_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load `word2vec-google-news-300` Model\n",
    "\n",
    "Learn how to extract word embeddings for your dataset. Try to check semantic similarities of the generated vectors using three examples of your own, e.g., King − Man + Woman = Queen or excellent ∼ outstanding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /Users/aditya/.pyenv/versions/3.10.6/envs/csci544/lib/python3.10/site-packages (4.2.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/aditya/.pyenv/versions/3.10.6/envs/csci544/lib/python3.10/site-packages (from gensim) (6.2.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /Users/aditya/.pyenv/versions/3.10.6/envs/csci544/lib/python3.10/site-packages (from gensim) (1.23.2)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /Users/aditya/.pyenv/versions/3.10.6/envs/csci544/lib/python3.10/site-packages (from gensim) (1.9.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "w2v_google = api.load(\"word2vec-google-news-300\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7118192911148071)]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_google.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"], topn=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5567486"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_google.similarity(\"excellent\", \"outstanding\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three Semantic Similarity Examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.55750686"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_google.similarity(\"worst\", \"terrible\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18926445"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_google.similarity(\"cheap\", \"best\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('inexpensive', 0.46950358152389526)]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_google.most_similar(positive=[\"cheap\", \"damaged\"], negative=[\"worst\"], topn=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Word2Vec on own Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "from gensim import utils\n",
    "\n",
    "\n",
    "class AmazonReviewCorpus:\n",
    "    def __iter__(self):\n",
    "        data = pd.read_csv(f\"{DATA_PATH}/{CLEANED_DATA_FILE}\", sep=\",\")\n",
    "\n",
    "        for review_body in data[\"review_body\"]:\n",
    "            yield utils.simple_preprocess(review_body)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim import utils\n",
    "\n",
    "w2v_custom = Word2Vec(vector_size=300, min_count=10, window=11, workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the vocabulary\n",
    "reviews = AmazonReviewCorpus()\n",
    "w2v_custom.build_vocab(reviews, progress_per=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11712504, 16910110)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "w2v_custom.train(reviews, total_examples=w2v_custom.corpus_count, epochs=w2v_custom.epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "w2v_custom.save(f\"{MODEL_PATH}/{CUSTOM_WORD_VECTORS_MODEL_FILE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using trained model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "w2v_custom = Word2Vec.load(f\"{MODEL_PATH}/{CUSTOM_WORD_VECTORS_MODEL_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dealer', 0.5524013042449951)]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_custom.wv.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"], topn=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81633675"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_custom.wv.similarity(\"excellent\", \"outstanding\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three Semantic Similarity Examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37887666"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_custom.wv.similarity(\"worst\", \"terrible\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14116523"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_custom.wv.similarity(\"cheap\", \"best\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dirty', 0.4841519594192505)]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_custom.wv.most_similar(positive=[\"cheap\", \"damaged\"], negative=[\"worst\"], topn=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free Some Memory\n",
    "del w2v_custom\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you conclude from comparing vectors generated by yourself and the pretrained model? Which of the Word2Vec models seems to encode semantic similarities between words better?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avg. Word Vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the average Word2Vec vectors for each review as the input feature (x = N1 􏰀Ni=1 Wi for a review with N words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_avg_word_vector(words: List[str], w2v_model):\n",
    "    result_vector = np.ndarray(shape=(300,), buffer=np.zeros((300,)), dtype=float)\n",
    "    removed_word_count = 0\n",
    "    for word in words:\n",
    "        try:\n",
    "            result_vector += w2v_model[word]\n",
    "        except KeyError:\n",
    "            removed_word_count += 1\n",
    "    return result_vector / (len(words) - removed_word_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Perform Word2Vec conversion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned but not preprocessed data\n",
    "import pickle as pkl\n",
    "\n",
    "train_wv, test_wv = None, None\n",
    "with open(f\"{DATA_PATH}/{DATA_FILE}\", mode=\"rb\") as file:\n",
    "    train_wv, test_wv = pkl.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "# Preprocess using gensim'simple_preprocess\n",
    "train_wv[DATA_COL] = train_wv[DATA_COL].apply(simple_preprocess)\n",
    "test_wv[DATA_COL] = test_wv[DATA_COL].apply(simple_preprocess)\n",
    "\n",
    "TEMP_COL = \"avg_word_vector\"\n",
    "VECTOR_COLS = [f\"vector_{i}\" for i in range(300)]\n",
    "\n",
    "# Train Data\n",
    "train_wv[TEMP_COL] = train_wv[DATA_COL].apply(partial(calculate_avg_word_vector, w2v_model=w2v_google))\n",
    "\n",
    "wv_df = pd.DataFrame(train_wv[TEMP_COL].to_list(), index=train_wv[TEMP_COL].index, columns=VECTOR_COLS)\n",
    "\n",
    "train_wv = pd.concat([train_wv, wv_df], axis=1)\n",
    "train_wv.dropna(inplace=True)\n",
    "\n",
    "X_wv_train = train_wv.drop([DATA_COL, TEMP_COL, TARGET_COL], axis=1)\n",
    "y_wv_train = train_wv[TARGET_COL]\n",
    "\n",
    "\n",
    "# Test Data\n",
    "test_wv[TEMP_COL] = test_wv[DATA_COL].apply(partial(calculate_avg_word_vector, w2v_model=w2v_google))\n",
    "\n",
    "test_wv_df = pd.DataFrame(test_wv[TEMP_COL].to_list(), index=test_wv[TEMP_COL].index, columns=VECTOR_COLS)\n",
    "\n",
    "test_wv = pd.concat([test_wv, test_wv_df], axis=1)\n",
    "test_wv.dropna(inplace=True)\n",
    "\n",
    "X_wv_test = test_wv.drop([DATA_COL, TEMP_COL, TARGET_COL], axis=1)\n",
    "y_wv_test = test_wv[TARGET_COL]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vector_0</th>\n",
       "      <th>vector_1</th>\n",
       "      <th>vector_2</th>\n",
       "      <th>vector_3</th>\n",
       "      <th>vector_4</th>\n",
       "      <th>vector_5</th>\n",
       "      <th>vector_6</th>\n",
       "      <th>vector_7</th>\n",
       "      <th>vector_8</th>\n",
       "      <th>vector_9</th>\n",
       "      <th>...</th>\n",
       "      <th>vector_290</th>\n",
       "      <th>vector_291</th>\n",
       "      <th>vector_292</th>\n",
       "      <th>vector_293</th>\n",
       "      <th>vector_294</th>\n",
       "      <th>vector_295</th>\n",
       "      <th>vector_296</th>\n",
       "      <th>vector_297</th>\n",
       "      <th>vector_298</th>\n",
       "      <th>vector_299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.042153</td>\n",
       "      <td>0.014164</td>\n",
       "      <td>-0.031451</td>\n",
       "      <td>0.093822</td>\n",
       "      <td>-0.019848</td>\n",
       "      <td>-0.023349</td>\n",
       "      <td>0.060690</td>\n",
       "      <td>-0.084643</td>\n",
       "      <td>0.057523</td>\n",
       "      <td>0.043411</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.065152</td>\n",
       "      <td>0.063943</td>\n",
       "      <td>-0.116344</td>\n",
       "      <td>0.008554</td>\n",
       "      <td>-0.038616</td>\n",
       "      <td>-0.005599</td>\n",
       "      <td>0.004150</td>\n",
       "      <td>-0.079447</td>\n",
       "      <td>0.062683</td>\n",
       "      <td>-0.045749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.062426</td>\n",
       "      <td>0.022175</td>\n",
       "      <td>0.032739</td>\n",
       "      <td>0.080762</td>\n",
       "      <td>-0.113143</td>\n",
       "      <td>-0.013770</td>\n",
       "      <td>0.066288</td>\n",
       "      <td>-0.067008</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.072654</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.062382</td>\n",
       "      <td>0.053597</td>\n",
       "      <td>-0.101337</td>\n",
       "      <td>0.092712</td>\n",
       "      <td>-0.013452</td>\n",
       "      <td>0.061283</td>\n",
       "      <td>0.063713</td>\n",
       "      <td>-0.041640</td>\n",
       "      <td>0.076851</td>\n",
       "      <td>-0.067822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.016350</td>\n",
       "      <td>0.009404</td>\n",
       "      <td>0.029967</td>\n",
       "      <td>0.091029</td>\n",
       "      <td>-0.042893</td>\n",
       "      <td>-0.050030</td>\n",
       "      <td>0.069718</td>\n",
       "      <td>-0.084670</td>\n",
       "      <td>0.088954</td>\n",
       "      <td>0.075950</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.089612</td>\n",
       "      <td>0.045468</td>\n",
       "      <td>-0.122181</td>\n",
       "      <td>0.046200</td>\n",
       "      <td>-0.016519</td>\n",
       "      <td>0.012339</td>\n",
       "      <td>0.000575</td>\n",
       "      <td>-0.070973</td>\n",
       "      <td>0.068710</td>\n",
       "      <td>-0.078934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.014460</td>\n",
       "      <td>0.011441</td>\n",
       "      <td>0.032575</td>\n",
       "      <td>0.130020</td>\n",
       "      <td>-0.073035</td>\n",
       "      <td>-0.010424</td>\n",
       "      <td>0.089496</td>\n",
       "      <td>-0.063508</td>\n",
       "      <td>0.024117</td>\n",
       "      <td>0.094315</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.102476</td>\n",
       "      <td>0.020297</td>\n",
       "      <td>-0.061961</td>\n",
       "      <td>0.107353</td>\n",
       "      <td>0.003710</td>\n",
       "      <td>0.016532</td>\n",
       "      <td>0.031951</td>\n",
       "      <td>-0.066222</td>\n",
       "      <td>0.059737</td>\n",
       "      <td>-0.078636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.069726</td>\n",
       "      <td>0.051299</td>\n",
       "      <td>0.026449</td>\n",
       "      <td>0.081157</td>\n",
       "      <td>-0.036284</td>\n",
       "      <td>-0.056623</td>\n",
       "      <td>0.053578</td>\n",
       "      <td>-0.101608</td>\n",
       "      <td>0.029151</td>\n",
       "      <td>0.094448</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.099021</td>\n",
       "      <td>0.018674</td>\n",
       "      <td>-0.094229</td>\n",
       "      <td>0.028641</td>\n",
       "      <td>0.015122</td>\n",
       "      <td>-0.019486</td>\n",
       "      <td>0.021722</td>\n",
       "      <td>-0.040834</td>\n",
       "      <td>0.069369</td>\n",
       "      <td>-0.000185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   vector_0  vector_1  vector_2  vector_3  vector_4  vector_5  vector_6  \\\n",
       "0  0.042153  0.014164 -0.031451  0.093822 -0.019848 -0.023349  0.060690   \n",
       "1  0.062426  0.022175  0.032739  0.080762 -0.113143 -0.013770  0.066288   \n",
       "2  0.016350  0.009404  0.029967  0.091029 -0.042893 -0.050030  0.069718   \n",
       "3 -0.014460  0.011441  0.032575  0.130020 -0.073035 -0.010424  0.089496   \n",
       "4  0.069726  0.051299  0.026449  0.081157 -0.036284 -0.056623  0.053578   \n",
       "\n",
       "   vector_7  vector_8  vector_9  ...  vector_290  vector_291  vector_292  \\\n",
       "0 -0.084643  0.057523  0.043411  ...   -0.065152    0.063943   -0.116344   \n",
       "1 -0.067008  0.025391  0.072654  ...   -0.062382    0.053597   -0.101337   \n",
       "2 -0.084670  0.088954  0.075950  ...   -0.089612    0.045468   -0.122181   \n",
       "3 -0.063508  0.024117  0.094315  ...   -0.102476    0.020297   -0.061961   \n",
       "4 -0.101608  0.029151  0.094448  ...   -0.099021    0.018674   -0.094229   \n",
       "\n",
       "   vector_293  vector_294  vector_295  vector_296  vector_297  vector_298  \\\n",
       "0    0.008554   -0.038616   -0.005599    0.004150   -0.079447    0.062683   \n",
       "1    0.092712   -0.013452    0.061283    0.063713   -0.041640    0.076851   \n",
       "2    0.046200   -0.016519    0.012339    0.000575   -0.070973    0.068710   \n",
       "3    0.107353    0.003710    0.016532    0.031951   -0.066222    0.059737   \n",
       "4    0.028641    0.015122   -0.019486    0.021722   -0.040834    0.069369   \n",
       "\n",
       "   vector_299  \n",
       "0   -0.045749  \n",
       "1   -0.067822  \n",
       "2   -0.078934  \n",
       "3   -0.078636  \n",
       "4   -0.000185  \n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_wv_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the avg'ed word vectors dataset\n",
    "import pickle as pkl\n",
    "\n",
    "with open(f\"{DATA_PATH}/{AVG_WORD_VECTORS_DATA_FILE}\", mode=\"wb\") as file:\n",
    "    pkl.dump((X_wv_train, y_wv_train, X_wv_test, y_wv_test), file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free Some Memory\n",
    "del train_wv, test_wv, X_wv_train, y_wv_train, X_wv_test, y_wv_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report your accuracy values on the testing split for these models similar to HW1, i.e., for each of perceptron and SVM models, report two accuracy values Word2Vec and TF-IDF features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train and test data\n",
    "import pickle as pkl\n",
    "\n",
    "train_preprocessed, test_preprocessed = None, None\n",
    "with open(f\"{DATA_PATH}/{PREPROCESSED_DATA_FILE}\", mode=\"rb\") as file:\n",
    "    train_preprocessed, test_preprocessed = pkl.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/aditya/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aditya/.pyenv/versions/3.10.6/envs/csci544/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:524: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfVectorizer(tokenizer=&lt;function word_tokenize at 0x13fc48040&gt;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(tokenizer=&lt;function word_tokenize at 0x13fc48040&gt;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "TfidfVectorizer(tokenizer=<function word_tokenize at 0x13fc48040>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=word_tokenize)\n",
    "\n",
    "# Using entire data to fit as the dataset is small and as using entire dataset is not needed for homework requirement\n",
    "all_data = pd.concat([train_preprocessed, test_preprocessed], axis=0)\n",
    "vectorizer.fit(all_data[DATA_COL])\n",
    "\n",
    "X_tfidf_train = vectorizer.transform(train_preprocessed[DATA_COL])\n",
    "X_tfidf_test = vectorizer.transform(test_preprocessed[DATA_COL])\n",
    "y_tfidf_train = train_preprocessed[TARGET_COL]\n",
    "y_tfidf_test = test_preprocessed[TARGET_COL]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save train and test TFIDF vectors\n",
    "import pickle as pkl\n",
    "\n",
    "with open(f\"{DATA_PATH}/{TFIDF_DATA_FILE}\", mode=\"wb\") as file:\n",
    "    pkl.dump((X_tfidf_train, y_tfidf_train, X_tfidf_test, y_tfidf_test), file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free Some Memory\n",
    "del train_preprocessed, test_preprocessed, all_data, X_tfidf_train, y_tfidf_train, X_tfidf_test, y_tfidf_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### TF-IDF Based Approach\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TF-IDF data\n",
    "import pickle as pkl\n",
    "\n",
    "X_tfidf_train, y_tfidf_train, X_tfidf_test, y_tfidf_test = None, None, None, None\n",
    "with open(f\"{DATA_PATH}/{TFIDF_DATA_FILE}\", mode=\"rb\") as file:\n",
    "    X_tfidf_train, y_tfidf_train, X_tfidf_test, y_tfidf_test = pkl.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Perceptron(alpha=0.012, class_weight=&#x27;balanced&#x27;, early_stopping=True,\n",
       "           max_iter=8000, random_state=42, tol=0.0001)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Perceptron</label><div class=\"sk-toggleable__content\"><pre>Perceptron(alpha=0.012, class_weight=&#x27;balanced&#x27;, early_stopping=True,\n",
       "           max_iter=8000, random_state=42, tol=0.0001)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "Perceptron(alpha=0.012, class_weight='balanced', early_stopping=True,\n",
       "           max_iter=8000, random_state=42, tol=0.0001)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "perceptron_tfidf_clf = Perceptron(\n",
    "    max_iter=8000, alpha=0.012, random_state=RANDOM_SEED, tol=1e-4, early_stopping=True, class_weight=\"balanced\"\n",
    ")\n",
    "\n",
    "perceptron_tfidf_clf.fit(X_tfidf_train, y_tfidf_train)\n",
    "\n",
    "y_tfidf_pred = perceptron_tfidf_clf.predict(X_tfidf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.33      0.42      3999\n",
      "           1       0.30      0.59      0.40      4000\n",
      "           2       0.34      0.24      0.28      4000\n",
      "           3       0.39      0.36      0.38      3999\n",
      "           4       0.61      0.52      0.56      3998\n",
      "\n",
      "    accuracy                           0.41     19996\n",
      "   macro avg       0.44      0.41      0.41     19996\n",
      "weighted avg       0.44      0.41      0.41     19996\n",
      " \n",
      "Accuracy Score:  0.4077815563112622\n"
     ]
    }
   ],
   "source": [
    "# Print Metrics\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "print(\n",
    "    classification_report(y_tfidf_test, y_tfidf_pred), \"\\nAccuracy Score: \", accuracy_score(y_tfidf_test, y_tfidf_pred)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "with open(f\"{MODEL_PATH}/{PERCEPTRON_TFIDF_MODEL_FILE}\", mode=\"wb\") as file:\n",
    "    import joblib\n",
    "\n",
    "    joblib.dump(perceptron_tfidf_clf, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete Model and related variables\n",
    "del X_tfidf_train, y_tfidf_train, X_tfidf_test, y_tfidf_test, perceptron_tfidf_clf, y_tfidf_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Word2Vec Based Approach - Avg. Word Vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Avg. Word2Vec Data\n",
    "import pickle as pkl\n",
    "\n",
    "X_wv_train, y_wv_train, X_wv_test, y_wv_test = None, None, None, None\n",
    "with open(f\"{DATA_PATH}/{AVG_WORD_VECTORS_DATA_FILE}\", mode=\"rb\") as file:\n",
    "    X_wv_train, y_wv_train, X_wv_test, y_wv_test = pkl.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Perceptron(alpha=0.012, early_stopping=True, max_iter=8000, random_state=42,\n",
       "           tol=0.0001)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Perceptron</label><div class=\"sk-toggleable__content\"><pre>Perceptron(alpha=0.012, early_stopping=True, max_iter=8000, random_state=42,\n",
       "           tol=0.0001)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "Perceptron(alpha=0.012, early_stopping=True, max_iter=8000, random_state=42,\n",
       "           tol=0.0001)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "perceptron_wv_clf = Perceptron(\n",
    "    max_iter=8000, alpha=0.012, random_state=RANDOM_SEED, tol=1e-4, early_stopping=True, class_weight=\"balanced\"\n",
    ")\n",
    "\n",
    "perceptron_wv_clf = Perceptron(max_iter=8000, alpha=0.012, random_state=RANDOM_SEED, tol=1e-4, early_stopping=True)\n",
    "\n",
    "perceptron_wv_clf.fit(X_wv_train, y_wv_train)\n",
    "\n",
    "y_wv_pred = perceptron_wv_clf.predict(X_wv_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.50      0.55      3996\n",
      "           1       0.35      0.31      0.33      3998\n",
      "           2       0.38      0.09      0.14      3999\n",
      "           3       0.31      0.58      0.41      3994\n",
      "           4       0.53      0.64      0.58      3992\n",
      "\n",
      "    accuracy                           0.42     19979\n",
      "   macro avg       0.43      0.42      0.40     19979\n",
      "weighted avg       0.43      0.42      0.40     19979\n",
      " \n",
      "Accuracy Score:  0.4237449321787877\n"
     ]
    }
   ],
   "source": [
    "# Print Metrics\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "print(classification_report(y_wv_test, y_wv_pred), \"\\nAccuracy Score: \", accuracy_score(y_wv_test, y_wv_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "with open(f\"{MODEL_PATH}/{PERCEPTRON_AVG_WV_MODEL_FILE}\", mode=\"wb\") as file:\n",
    "    import joblib\n",
    "\n",
    "    joblib.dump(perceptron_wv_clf, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete model and related variables\n",
    "del X_wv_train, y_wv_train, X_wv_test, y_wv_test, perceptron_wv_clf, y_wv_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### TF-IDF Based\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TF-IDF data\n",
    "import pickle as pkl\n",
    "\n",
    "X_tfidf_train, y_tfidf_train, X_tfidf_test, y_tfidf_test = None, None, None, None\n",
    "with open(f\"{DATA_PATH}/{TFIDF_DATA_FILE}\", mode=\"rb\") as file:\n",
    "    X_tfidf_train, y_tfidf_train, X_tfidf_test, y_tfidf_test = pkl.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearSVC(C=0.1,\n",
       "          class_weight={0: 0.9525, 1: 1.99825, 2: 1.9225, 3: 0.625, 4: 0.8585},\n",
       "          dual=False, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC(C=0.1,\n",
       "          class_weight={0: 0.9525, 1: 1.99825, 2: 1.9225, 3: 0.625, 4: 0.8585},\n",
       "          dual=False, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearSVC(C=0.1,\n",
       "          class_weight={0: 0.9525, 1: 1.99825, 2: 1.9225, 3: 0.625, 4: 0.8585},\n",
       "          dual=False, random_state=42)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.50      0.57      3999\n",
      "           1       0.38      0.51      0.44      4000\n",
      "           2       0.38      0.57      0.46      4000\n",
      "           3       0.61      0.14      0.23      3999\n",
      "           4       0.66      0.78      0.72      3998\n",
      "\n",
      "    accuracy                           0.50     19996\n",
      "   macro avg       0.54      0.50      0.48     19996\n",
      "weighted avg       0.54      0.50      0.48     19996\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "class_weight = {0: 0.9525, 1: 1.99825, 2: 1.9225, 3: 0.625, 4: 0.8585}\n",
    "\n",
    "svm_tfidf_clf = LinearSVC(dual=False, C=0.1, max_iter=1000, class_weight=class_weight, random_state=RANDOM_SEED)\n",
    "\n",
    "svm_tfidf_clf.fit(X_tfidf_train, y_tfidf_train)\n",
    "\n",
    "y_tfidf_pred = svm_tfidf_clf.predict(X_tfidf_test)\n",
    "\n",
    "print(classification_report(y_tfidf_test, y_tfidf_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.50      0.57      3999\n",
      "           1       0.38      0.51      0.44      4000\n",
      "           2       0.38      0.57      0.46      4000\n",
      "           3       0.61      0.14      0.23      3999\n",
      "           4       0.66      0.78      0.72      3998\n",
      "\n",
      "    accuracy                           0.50     19996\n",
      "   macro avg       0.54      0.50      0.48     19996\n",
      "weighted avg       0.54      0.50      0.48     19996\n",
      " \n",
      "Accuracy Score:  0.5004500900180036\n"
     ]
    }
   ],
   "source": [
    "# Print Metrics\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "print(\n",
    "    classification_report(y_tfidf_test, y_tfidf_pred), \"\\nAccuracy Score: \", accuracy_score(y_tfidf_test, y_tfidf_pred)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "with open(f\"{MODEL_PATH}/{SVM_TFIDF_MODEL_FILE}\", mode=\"wb\") as file:\n",
    "    import joblib\n",
    "\n",
    "    joblib.dump(svm_tfidf_clf, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete model and variables\n",
    "del X_tfidf_train, y_tfidf_train, X_tfidf_test, y_tfidf_test, svm_tfidf_clf, y_tfidf_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Word2Vec Based Approach - Avg. Word Vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Avg. Word2Vec Data\n",
    "import pickle as pkl\n",
    "\n",
    "X_wv_train, y_wv_train, X_wv_test, y_wv_test = None, None, None, None\n",
    "with open(f\"{DATA_PATH}/{AVG_WORD_VECTORS_DATA_FILE}\", mode=\"rb\") as file:\n",
    "    X_wv_train, y_wv_train, X_wv_test, y_wv_test = pkl.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearSVC(C=0.1,\n",
       "          class_weight={0: 0.9525, 1: 1.99825, 2: 1.9225, 3: 0.625, 4: 0.8585},\n",
       "          dual=False, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC(C=0.1,\n",
       "          class_weight={0: 0.9525, 1: 1.99825, 2: 1.9225, 3: 0.625, 4: 0.8585},\n",
       "          dual=False, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearSVC(C=0.1,\n",
       "          class_weight={0: 0.9525, 1: 1.99825, 2: 1.9225, 3: 0.625, 4: 0.8585},\n",
       "          dual=False, random_state=42)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "class_weight = {0: 0.9525, 1: 1.99825, 2: 1.9225, 3: 0.625, 4: 0.8585}\n",
    "\n",
    "svm_wv_clf = LinearSVC(dual=False, C=0.1, max_iter=1000, class_weight=class_weight, random_state=RANDOM_SEED)\n",
    "\n",
    "svm_wv_clf.fit(X_wv_train, y_wv_train)\n",
    "\n",
    "y_wv_avged_pred = svm_wv_clf.predict(X_wv_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.37      0.48      3996\n",
      "           1       0.35      0.57      0.43      3998\n",
      "           2       0.33      0.56      0.42      3999\n",
      "           3       0.51      0.02      0.04      3994\n",
      "           4       0.64      0.69      0.67      3992\n",
      "\n",
      "    accuracy                           0.44     19979\n",
      "   macro avg       0.50      0.44      0.41     19979\n",
      "weighted avg       0.50      0.44      0.41     19979\n",
      " \n",
      "Accuracy Score:  0.4421142199309275\n"
     ]
    }
   ],
   "source": [
    "# Print Metrics\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "print(\n",
    "    classification_report(y_wv_test, y_wv_avged_pred), \"\\nAccuracy Score: \", accuracy_score(y_wv_test, y_wv_avged_pred)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "with open(f\"{MODEL_PATH}/{SVM_AVG_WV_MODEL_FILE}\", mode=\"wb\") as file:\n",
    "    import joblib\n",
    "\n",
    "    joblib.dump(svm_wv_clf, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete model and variables\n",
    "del X_wv_train, y_wv_train, X_wv_test, y_wv_test, svm_wv_clf, y_wv_avged_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you conclude from comparing performances for the models trained using the two different feature types (TF-IDF and your trained Word2Vec features)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## PyTorch Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as func\n",
    "from torch.utils.data import TensorDataset, DataLoader, SubsetRandomSampler\n",
    "\n",
    "device = None\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "## TODO: REMOVE THIS LINE\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using Device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data_loader, model, n_epochs, optimizer, criterion):\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss = 0.0\n",
    "\n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train()  # prep model for training\n",
    "        for inputs, target in data_loader:\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(inputs)\n",
    "            # calculate the loss\n",
    "            loss = criterion(output.squeeze(1), target)\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # update running training loss\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        # calculate average loss over an epoch\n",
    "        train_loss = train_loss / len(data_loader.dataset)\n",
    "\n",
    "        print(f\"Epoch: {epoch + 1} \\tTraining Loss: {train_loss:.6f}\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data_loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_preds, y_true = list(), list()\n",
    "        for input, target in data_loader:\n",
    "            output = model(input)\n",
    "            _, predictions = torch.max(output, 1)\n",
    "            predictions = predictions.cpu().numpy().squeeze()\n",
    "            targets = target.cpu().numpy().squeeze()\n",
    "            y_preds.append(predictions)\n",
    "            y_true.append(targets)\n",
    "    return np.ravel(np.array(y_true)), np.ravel(np.array(y_preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward Neural Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Word2Vec features, train a feedforward multilayer perceptron net- work for classification. Consider a network with two hidden layers, each with 50 and 10 nodes, respectively. You can use cross entropy loss and your own choice for other hyperparamters, e.g., nonlinearity, number of epochs, etc. Part of getting good results is to select good values for these hyperparamters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create FNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNN(nn.Module):\n",
    "    def __init__(self, n_input, n_output, dropout_rate) -> None:\n",
    "        super(FNN, self).__init__()\n",
    "\n",
    "        self.n_input = n_input\n",
    "        self.n_hidden_1 = 50\n",
    "        self.n_hidden_2 = 10\n",
    "        self.n_output = n_output\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.fc1 = nn.Linear(self.n_input, self.n_hidden_1)\n",
    "        self.fc2 = nn.Linear(self.n_hidden_1, self.n_hidden_2)\n",
    "        self.fc3 = nn.Linear(self.n_hidden_2, self.n_output)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = func.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = func.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Using Avg. Word2Vec Vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Avg. Word2Vec Data\n",
    "import pickle as pkl\n",
    "\n",
    "X_wv_train, y_wv_train, X_wv_test, y_wv_test = None, None, None, None\n",
    "with open(f\"{DATA_PATH}/{AVG_WORD_VECTORS_DATA_FILE}\", mode=\"rb\") as file:\n",
    "    X_wv_train, y_wv_train, X_wv_test, y_wv_test = pkl.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FNN(\n",
       "  (fc1): Linear(in_features=300, out_features=50, bias=True)\n",
       "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
       "  (fc3): Linear(in_features=10, out_features=5, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnn_wv_model = FNN(n_input=300, n_output=5, dropout_rate=0.2).to(device)\n",
    "fnn_wv_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "FNN_AVG_WV_BATCH_SIZE = 16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_wv_train_tensor = torch.FloatTensor(X_wv_train.values).to(device)\n",
    "y_wv_train_tensor = torch.LongTensor(y_wv_train.values).to(device)\n",
    "X_wv_test_tensor = torch.FloatTensor(X_wv_test.values).to(device)\n",
    "y_wv_test_tensor = torch.LongTensor(y_wv_test.values).to(device)\n",
    "\n",
    "fnn_wv_train = TensorDataset(X_wv_train_tensor, y_wv_train_tensor)\n",
    "fnn_wv_train_loader = DataLoader(fnn_wv_train, batch_size=FNN_AVG_WV_BATCH_SIZE, drop_last=True, shuffle=True)\n",
    "\n",
    "fnn_wv_test = TensorDataset(X_wv_test_tensor, y_wv_test_tensor)\n",
    "fnn_wv_test_loader = DataLoader(fnn_wv_test, batch_size=FNN_AVG_WV_BATCH_SIZE, drop_last=True, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "class_weights = torch.FloatTensor(\n",
    "    class_weight.compute_class_weight(\"balanced\", classes=np.unique(y_wv_train.values), y=y_wv_train.values)\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function and Optimizer\n",
    "criterion_wv = nn.CrossEntropyLoss(weight=class_weights).to(device)\n",
    "\n",
    "optimizer_wv = torch.optim.Adam(fnn_wv_model.parameters(), lr=32e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free Some Memory\n",
    "del X_wv_train, y_wv_train, X_wv_test, y_wv_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.334452\n",
      "Epoch: 2 \tTraining Loss: 1.240569\n",
      "Epoch: 3 \tTraining Loss: 1.222275\n",
      "Epoch: 4 \tTraining Loss: 1.209150\n",
      "Epoch: 5 \tTraining Loss: 1.200559\n",
      "Epoch: 6 \tTraining Loss: 1.193135\n",
      "Epoch: 7 \tTraining Loss: 1.189160\n",
      "Epoch: 8 \tTraining Loss: 1.185600\n",
      "Epoch: 9 \tTraining Loss: 1.175386\n",
      "Epoch: 10 \tTraining Loss: 1.172929\n",
      "Epoch: 11 \tTraining Loss: 1.169147\n",
      "Epoch: 12 \tTraining Loss: 1.167287\n",
      "Epoch: 13 \tTraining Loss: 1.162369\n",
      "Epoch: 14 \tTraining Loss: 1.159795\n",
      "Epoch: 15 \tTraining Loss: 1.157287\n",
      "Epoch: 16 \tTraining Loss: 1.154580\n",
      "Epoch: 17 \tTraining Loss: 1.151378\n",
      "Epoch: 18 \tTraining Loss: 1.151889\n",
      "Epoch: 19 \tTraining Loss: 1.148454\n",
      "Epoch: 20 \tTraining Loss: 1.146562\n",
      "Epoch: 21 \tTraining Loss: 1.142714\n",
      "Epoch: 22 \tTraining Loss: 1.143547\n",
      "Epoch: 23 \tTraining Loss: 1.140720\n",
      "Epoch: 24 \tTraining Loss: 1.139199\n",
      "Epoch: 25 \tTraining Loss: 1.138398\n",
      "Epoch: 26 \tTraining Loss: 1.137973\n",
      "Epoch: 27 \tTraining Loss: 1.136690\n",
      "Epoch: 28 \tTraining Loss: 1.134158\n",
      "Epoch: 29 \tTraining Loss: 1.130968\n",
      "Epoch: 30 \tTraining Loss: 1.129127\n",
      "Epoch: 31 \tTraining Loss: 1.130163\n",
      "Epoch: 32 \tTraining Loss: 1.127723\n"
     ]
    }
   ],
   "source": [
    "fnn_wv_model = train_model(\n",
    "    fnn_wv_train_loader, fnn_wv_model, n_epochs=32, optimizer=optimizer_wv, criterion=criterion_wv\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_wv_true, y_wv_pred = predict(fnn_wv_model, fnn_wv_test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report accuracy on the testing split.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.69      0.63      3994\n",
      "           1       0.39      0.32      0.35      3996\n",
      "           2       0.42      0.38      0.40      3998\n",
      "           3       0.46      0.39      0.42      3991\n",
      "           4       0.62      0.76      0.68      3989\n",
      "\n",
      "    accuracy                           0.51     19968\n",
      "   macro avg       0.49      0.51      0.50     19968\n",
      "weighted avg       0.49      0.51      0.50     19968\n",
      " \n",
      "Accuracy Score:  0.5084635416666666\n"
     ]
    }
   ],
   "source": [
    "# Print Metrics\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "print(classification_report(y_wv_true, y_wv_pred), \"\\nAccuracy Score: \", accuracy_score(y_wv_true, y_wv_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model\n",
    "torch.save(fnn_wv_model.state_dict(), f\"{MODEL_PATH}/{FNN_AVG_WV_MODEL_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "del fnn_wv_model, criterion_wv, optimizer_wv, y_wv_true, y_wv_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Concatenate First 10 Word2Vec Vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_top_n_wv(words_in_sentence: str, w2v_model, n=10):\n",
    "    top_n_wvs = np.zeros(300 * n, dtype=np.float64)\n",
    "    count = 0\n",
    "\n",
    "    for word in words_in_sentence:\n",
    "        if count == n or count == words_in_sentence:\n",
    "            break\n",
    "        count += 1\n",
    "        try:\n",
    "            top_n_wvs[(count - 1) * 300 : count * 300] = w2v_model[word]\n",
    "        except KeyError:\n",
    "            count -= 1\n",
    "            continue\n",
    "\n",
    "    if np.all((top_n_wvs == 0)):\n",
    "        return np.nan\n",
    "\n",
    "    return top_n_wvs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Apply Top 10 Word Vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned but not preprocessed data\n",
    "import pickle as pkl\n",
    "\n",
    "train_wv_top_10, test_wv_top_10 = None, None\n",
    "with open(f\"{DATA_PATH}/{DATA_FILE}\", mode=\"rb\") as file:\n",
    "    train_wv_top_10, test_wv_top_10 = pkl.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "# Preprocess using gensim'simple_preprocess\n",
    "train_wv_top_10[DATA_COL] = train_wv_top_10[DATA_COL].apply(simple_preprocess)\n",
    "test_wv_top_10[DATA_COL] = test_wv_top_10[DATA_COL].apply(simple_preprocess)\n",
    "\n",
    "TEMP_COL = \"top_10_wv\"\n",
    "VECTOR_COLS = [f\"vector_{i}\" for i in range(3000)]\n",
    "\n",
    "# Train\n",
    "train_wv_top_10[TEMP_COL] = train_wv_top_10[DATA_COL].apply(partial(concatenate_top_n_wv, w2v_model=w2v_google, n=10))\n",
    "train_wv_top_10.dropna(inplace=True)\n",
    "\n",
    "wv_df = pd.DataFrame(train_wv_top_10[TEMP_COL].to_list(), index=train_wv_top_10[TEMP_COL].index, columns=VECTOR_COLS)\n",
    "\n",
    "train_wv_top_10 = pd.concat([train_wv_top_10, wv_df], axis=1)\n",
    "train_wv_top_10.dropna(inplace=True)\n",
    "\n",
    "X_wv_top_10_train = train_wv_top_10.drop([DATA_COL, TEMP_COL, TARGET_COL], axis=1)\n",
    "y_wv_top_10_train = train_wv_top_10[TARGET_COL]\n",
    "\n",
    "\n",
    "# Test\n",
    "test_wv_top_10[TEMP_COL] = test_wv_top_10[DATA_COL].apply(partial(concatenate_top_n_wv, w2v_model=w2v_google, n=10))\n",
    "test_wv_top_10.dropna(inplace=True)\n",
    "\n",
    "wv_df = pd.DataFrame(test_wv_top_10[TEMP_COL].to_list(), index=test_wv_top_10[TEMP_COL].index, columns=VECTOR_COLS)\n",
    "\n",
    "test_wv_top_10 = pd.concat([test_wv_top_10, wv_df], axis=1)\n",
    "test_wv_top_10.dropna(inplace=True)\n",
    "\n",
    "X_wv_top_10_test = test_wv_top_10.drop([DATA_COL, TEMP_COL, TARGET_COL], axis=1)\n",
    "y_wv_top_10_test = test_wv_top_10[TARGET_COL]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the avg'ed word vectors dataset\n",
    "import pickle as pkl\n",
    "\n",
    "with open(f\"{DATA_PATH}/{TOP_10_WORD_VECTORS_DATA_FILE}\", mode=\"wb\") as file:\n",
    "    pkl.dump((X_wv_top_10_train, y_wv_top_10_train, X_wv_top_10_test, y_wv_top_10_test), file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free Some Memory\n",
    "del train_wv_top_10, test_wv_top_10, X_wv_top_10_train, y_wv_top_10_train, X_wv_top_10_test, y_wv_top_10_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Using Top 10 Word2Vec Vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Avg. Word2Vec Data\n",
    "import pickle as pkl\n",
    "\n",
    "X_wv_top_10_train, y_wv_top_10_train, X_wv_top_10_test, y_wv_top_10_test = None, None, None, None\n",
    "with open(f\"{DATA_PATH}/{AVG_WORD_VECTORS_DATA_FILE}\", mode=\"rb\") as file:\n",
    "    X_wv_top_10_train, y_wv_top_10_train, X_wv_top_10_test, y_wv_top_10_test = pkl.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "FNN_TOP_10_WV_BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_wv_top_10_train_tensor = torch.FloatTensor(X_wv_top_10_train.values).to(device)\n",
    "y_wv_top_10_train_tensor = torch.LongTensor(y_wv_top_10_train.values).to(device)\n",
    "X_wv_top_10_test_tensor = torch.FloatTensor(X_wv_top_10_test.values).to(device)\n",
    "y_wv_top_10_test_tensor = torch.LongTensor(y_wv_top_10_test.values).to(device)\n",
    "\n",
    "fnn_wv_top_10_train = TensorDataset(X_wv_top_10_train_tensor, y_wv_top_10_train_tensor)\n",
    "fnn_wv_top_10_train_loader = DataLoader(fnn_wv_top_10_train, batch_size=FNN_TOP_10_WV_BATCH_SIZE, drop_last=True, shuffle=True)\n",
    "\n",
    "fnn_wv_top_10_test = TensorDataset(X_wv_top_10_test_tensor, y_wv_top_10_test_tensor)\n",
    "fnn_wv_top_10_test_loader = DataLoader(fnn_wv_top_10_test, batch_size=FNN_TOP_10_WV_BATCH_SIZE, drop_last=True, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FNN(\n",
       "  (fc1): Linear(in_features=3000, out_features=50, bias=True)\n",
       "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
       "  (fc3): Linear(in_features=10, out_features=5, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnn_wv_top_10_model = FNN(n_input=3000, n_output=5, dropout_rate=0.2).to(device)\n",
    "fnn_wv_top_10_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "class_weights = torch.FloatTensor(\n",
    "    class_weight.compute_class_weight(\n",
    "        \"balanced\", classes=np.unique(y_wv_top_10_train.values), y=y_wv_top_10_train.values\n",
    "    )\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function and Optimizer\n",
    "criterion_wv_top_10 = nn.CrossEntropyLoss(weight=class_weights).to(device)\n",
    "\n",
    "optimizer_wv_top_10 = torch.optim.Adam(fnn_wv_top_10_model.parameters(), lr=32e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "del (X_wv_top_10_train, y_wv_top_10_train, X_wv_top_10_test, y_wv_top_10_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.395733\n",
      "Epoch: 2 \tTraining Loss: 1.338381\n",
      "Epoch: 3 \tTraining Loss: 1.314909\n",
      "Epoch: 4 \tTraining Loss: 1.293625\n",
      "Epoch: 5 \tTraining Loss: 1.278479\n",
      "Epoch: 6 \tTraining Loss: 1.260963\n",
      "Epoch: 7 \tTraining Loss: 1.249624\n",
      "Epoch: 8 \tTraining Loss: 1.237256\n",
      "Epoch: 9 \tTraining Loss: 1.226008\n",
      "Epoch: 10 \tTraining Loss: 1.213949\n",
      "Epoch: 11 \tTraining Loss: 1.203274\n",
      "Epoch: 12 \tTraining Loss: 1.195260\n",
      "Epoch: 13 \tTraining Loss: 1.185953\n",
      "Epoch: 14 \tTraining Loss: 1.176826\n",
      "Epoch: 15 \tTraining Loss: 1.172334\n",
      "Epoch: 16 \tTraining Loss: 1.160815\n",
      "Epoch: 17 \tTraining Loss: 1.152731\n",
      "Epoch: 18 \tTraining Loss: 1.147118\n",
      "Epoch: 19 \tTraining Loss: 1.141121\n",
      "Epoch: 20 \tTraining Loss: 1.135564\n",
      "Epoch: 21 \tTraining Loss: 1.128876\n",
      "Epoch: 22 \tTraining Loss: 1.123837\n",
      "Epoch: 23 \tTraining Loss: 1.117985\n",
      "Epoch: 24 \tTraining Loss: 1.108776\n",
      "Epoch: 25 \tTraining Loss: 1.106559\n",
      "Epoch: 26 \tTraining Loss: 1.098990\n",
      "Epoch: 27 \tTraining Loss: 1.096045\n",
      "Epoch: 28 \tTraining Loss: 1.089394\n",
      "Epoch: 29 \tTraining Loss: 1.085065\n",
      "Epoch: 30 \tTraining Loss: 1.081186\n",
      "Epoch: 31 \tTraining Loss: 1.075400\n",
      "Epoch: 32 \tTraining Loss: 1.072492\n"
     ]
    }
   ],
   "source": [
    "fnn_wv_top_10_model = train_model(\n",
    "    fnn_wv_top_10_train_loader,\n",
    "    fnn_wv_top_10_model,\n",
    "    n_epochs=32,\n",
    "    optimizer=optimizer_wv_top_10,\n",
    "    criterion=criterion_wv_top_10,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_wv_top_10_true, y_wv_top_10_pred = predict(fnn_wv_top_10_model, fnn_wv_top_10_test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report the accuracy value on the testing split for your MLP model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.37      0.44      3995\n",
      "           1       0.31      0.38      0.34      3995\n",
      "           2       0.35      0.34      0.35      3998\n",
      "           3       0.35      0.45      0.39      3992\n",
      "           4       0.54      0.45      0.49      3988\n",
      "\n",
      "    accuracy                           0.40     19968\n",
      "   macro avg       0.42      0.40      0.40     19968\n",
      "weighted avg       0.42      0.40      0.40     19968\n",
      " \n",
      "Accuracy Score:  0.39758613782051283\n"
     ]
    }
   ],
   "source": [
    "# Print Metrics\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "print(\n",
    "    classification_report(y_wv_top_10_true, y_wv_top_10_pred),\n",
    "    \"\\nAccuracy Score: \",\n",
    "    accuracy_score(y_wv_top_10_true, y_wv_top_10_pred),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model\n",
    "torch.save(fnn_wv_top_10_model.state_dict(), f\"{MODEL_PATH}/{FNN_TOP_10_WV_MODEL_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "del (\n",
    "    fnn_wv_top_10_model,\n",
    "    criterion_wv_top_10,\n",
    "    optimizer_wv_top_10,\n",
    "    y_wv_top_10_true,\n",
    "    y_wv_top_10_pred,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you conclude by comparing accuracy values you obtain with those obtained in the “’Simple Models” section?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Dataset with Review Length of 20 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_words(words_in_sentence: List[str], w2v_model, n=20):\n",
    "    top_n_words = np.zeros((20, 300), dtype=np.float32)\n",
    "    idx = 0\n",
    "    for word in words_in_sentence:\n",
    "        if idx == n or idx == len(words_in_sentence):\n",
    "            break\n",
    "        try:\n",
    "            top_n_words[idx] = w2v_model[word]\n",
    "            idx += 1\n",
    "        except KeyError:\n",
    "            continue\n",
    "    if idx == 0:\n",
    "        return np.nan\n",
    "    return top_n_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned but not preprocessed data\n",
    "import pickle as pkl\n",
    "\n",
    "train_wv_20_words, test_wv_20_words = None, None\n",
    "with open(f\"{DATA_PATH}/{DATA_FILE}\", mode=\"rb\") as file:\n",
    "    train_wv_20_words, test_wv_20_words = pkl.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "# Preprocess using gensim'simple_preprocess\n",
    "train_wv_20_words[DATA_COL] = train_wv_20_words[DATA_COL].apply(simple_preprocess)\n",
    "test_wv_20_words[DATA_COL] = test_wv_20_words[DATA_COL].apply(simple_preprocess)\n",
    "\n",
    "TEMP_COL = \"top_n_words\"\n",
    "VECTOR_COLS = [f\"vector_{i}\" for i in range(300)]\n",
    "\n",
    "# Train Data\n",
    "train_wv_20_words[TEMP_COL] = train_wv_20_words[DATA_COL].apply(partial(get_top_n_words, w2v_model=w2v_google))\n",
    "train_wv_20_words.dropna(inplace=True)\n",
    "\n",
    "X_wv_20_words_train = train_wv_20_words[TEMP_COL].values\n",
    "X_wv_20_words_train = np.array([data for data in X_wv_20_words_train], dtype=np.float64)\n",
    "y_wv_20_words_train = train_wv_20_words[TARGET_COL].values\n",
    "\n",
    "\n",
    "# Test Data\n",
    "test_wv_20_words[TEMP_COL] = test_wv_20_words[DATA_COL].apply(partial(get_top_n_words, w2v_model=w2v_google))\n",
    "test_wv_20_words.dropna(inplace=True)\n",
    "\n",
    "X_wv_20_words_test = test_wv_20_words[TEMP_COL].values\n",
    "X_wv_20_words_test = np.array([data for data in X_wv_20_words_test], dtype=np.float64)\n",
    "y_wv_20_words_test = test_wv_20_words[TARGET_COL].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the top 20 words word vectors dataset\n",
    "import pickle as pkl\n",
    "\n",
    "with open(f\"{DATA_PATH}/{WORDS_20_WORD_VECTORS_DATA_FILE}\", mode=\"wb\") as file:\n",
    "    pkl.dump((X_wv_20_words_train, y_wv_20_words_train, X_wv_20_words_test, y_wv_20_words_test), file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free Some Memory\n",
    "del (\n",
    "    train_wv_20_words,\n",
    "    test_wv_20_words,\n",
    "    X_wv_20_words_train,\n",
    "    y_wv_20_words_train,\n",
    "    X_wv_20_words_test,\n",
    "    y_wv_20_words_test,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working on RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN_BATCH_SIZE = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the top 20 word vectors dataset\n",
    "import pickle as pkl\n",
    "\n",
    "X_wv_20_words_train, y_wv_20_words_train, X_wv_20_words_test, y_rnn_test = None, None, None, None\n",
    "with open(f\"{DATA_PATH}/{WORDS_20_WORD_VECTORS_DATA_FILE}\", mode=\"rb\") as file:\n",
    "    X_wv_20_words_train, y_wv_20_words_train, X_wv_20_words_test, y_rnn_test = pkl.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_wv_20_words_train_tensor = torch.FloatTensor(X_wv_20_words_train).to(device)\n",
    "y_wv_20_words_train_tensor = torch.LongTensor(y_wv_20_words_train).to(device)\n",
    "X_wv_20_words_test_tensor = torch.FloatTensor(X_wv_20_words_test).to(device)\n",
    "y_wv_20_words_test_tensor = torch.LongTensor(y_rnn_test).to(device)\n",
    "\n",
    "rnn_train = TensorDataset(X_wv_20_words_train_tensor, y_wv_20_words_train_tensor)\n",
    "rnn_train_loader = DataLoader(rnn_train, batch_size=RNN_BATCH_SIZE, drop_last=True, shuffle=True)\n",
    "\n",
    "rnn_test = TensorDataset(X_wv_20_words_test_tensor, y_wv_20_words_test_tensor)\n",
    "rnn_test_loader = DataLoader(rnn_test, batch_size=RNN_BATCH_SIZE, drop_last=True, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "class_weights = torch.FloatTensor(\n",
    "    class_weight.compute_class_weight(\n",
    "        \"balanced\", classes=np.unique(y_wv_20_words_train), y=y_wv_20_words_train\n",
    "    )\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free Some Memory\n",
    "del X_wv_20_words_train, y_wv_20_words_train, X_wv_20_words_test, y_rnn_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs, n_hidden_states):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_outputs = n_outputs\n",
    "        self.n_hidden_size = n_hidden_states\n",
    "        self.n_layers = 1\n",
    "        self.rnn = torch.nn.RNN(self.n_inputs, self.n_hidden_size, self.n_layers, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(self.n_hidden_size, n_outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.n_hidden_size)\n",
    "        return hidden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleRNN(\n",
       "  (rnn): RNN(300, 20, batch_first=True)\n",
       "  (fc): Linear(in_features=20, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_rnn_model = SimpleRNN(n_inputs=300, n_outputs=5, n_hidden_states=20)\n",
    "simple_rnn_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function and Optimizer\n",
    "criterion_simple_rnn = nn.CrossEntropyLoss(weight=class_weights).to(device)\n",
    "\n",
    "optimizer_simple_rnn = torch.optim.Adam(simple_rnn_model.parameters(), lr=128e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.401129\n",
      "Epoch: 2 \tTraining Loss: 1.290815\n",
      "Epoch: 3 \tTraining Loss: 1.262726\n",
      "Epoch: 4 \tTraining Loss: 1.249236\n",
      "Epoch: 5 \tTraining Loss: 1.236278\n",
      "Epoch: 6 \tTraining Loss: 1.245102\n",
      "Epoch: 7 \tTraining Loss: 1.225701\n",
      "Epoch: 8 \tTraining Loss: 1.219416\n",
      "Epoch: 9 \tTraining Loss: 1.217325\n",
      "Epoch: 10 \tTraining Loss: 1.211375\n",
      "Epoch: 11 \tTraining Loss: 1.211957\n",
      "Epoch: 12 \tTraining Loss: 1.208538\n",
      "Epoch: 13 \tTraining Loss: 1.205574\n",
      "Epoch: 14 \tTraining Loss: 1.212909\n",
      "Epoch: 15 \tTraining Loss: 1.203798\n",
      "Epoch: 16 \tTraining Loss: 1.203189\n",
      "Epoch: 17 \tTraining Loss: 1.206479\n",
      "Epoch: 18 \tTraining Loss: 1.204533\n",
      "Epoch: 19 \tTraining Loss: 1.198724\n",
      "Epoch: 20 \tTraining Loss: 1.207651\n",
      "Epoch: 21 \tTraining Loss: 1.205054\n",
      "Epoch: 22 \tTraining Loss: 1.199495\n",
      "Epoch: 23 \tTraining Loss: 1.204661\n",
      "Epoch: 24 \tTraining Loss: 1.199945\n",
      "Epoch: 25 \tTraining Loss: 1.195570\n",
      "Epoch: 26 \tTraining Loss: 1.197134\n",
      "Epoch: 27 \tTraining Loss: 1.195752\n",
      "Epoch: 28 \tTraining Loss: 1.205070\n",
      "Epoch: 29 \tTraining Loss: 1.195826\n",
      "Epoch: 30 \tTraining Loss: 1.205218\n",
      "Epoch: 31 \tTraining Loss: 1.209259\n",
      "Epoch: 32 \tTraining Loss: 1.206904\n"
     ]
    }
   ],
   "source": [
    "simple_rnn = train_model(\n",
    "    rnn_train_loader, simple_rnn_model, n_epochs=32, optimizer=optimizer_simple_rnn, criterion=criterion_simple_rnn\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_rnn_true, y_rnn_pred = predict(simple_rnn, rnn_test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.60      0.57      3992\n",
      "           1       0.35      0.34      0.35      3996\n",
      "           2       0.38      0.37      0.38      3996\n",
      "           3       0.42      0.41      0.41      3992\n",
      "           4       0.62      0.60      0.61      3992\n",
      "\n",
      "    accuracy                           0.47     19968\n",
      "   macro avg       0.46      0.47      0.46     19968\n",
      "weighted avg       0.46      0.47      0.46     19968\n",
      " \n",
      "Accuracy Score:  0.46509415064102566\n"
     ]
    }
   ],
   "source": [
    "# Print the Metrics\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "print(classification_report(y_rnn_true, y_rnn_pred), \"\\nAccuracy Score: \", accuracy_score(y_rnn_true, y_rnn_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model\n",
    "torch.save(simple_rnn.state_dict(), f\"{MODEL_PATH}/{RNN_TOP_20_WV_MODEL_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "del simple_rnn, criterion_simple_rnn, optimizer_simple_rnn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report accuracy values on the testing split for your RNN model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you conclude by comparing accuracy values you obtain with those obtained with feedforward neural network models?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gated Recurrent Unit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs, n_hidden_states, n_layers=1):\n",
    "        super(GRU, self).__init__()\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_outputs = n_outputs\n",
    "        self.n_hidden_states = n_hidden_states\n",
    "        self.n_layers = n_layers\n",
    "        self.rnn = torch.nn.GRU(self.n_inputs, self.n_hidden_states, self.n_layers, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(self.n_hidden_states, self.n_outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.n_hidden_states).to(device)\n",
    "        return hidden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working on GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the top 20 word vectors dataset\n",
    "import pickle as pkl\n",
    "\n",
    "X_wv_20_words_train, y_wv_20_words_train, X_wv_20_words_test, y_rnn_test = None, None, None, None\n",
    "with open(f\"{DATA_PATH}/{WORDS_20_WORD_VECTORS_DATA_FILE}\", mode=\"rb\") as file:\n",
    "    X_wv_20_words_train, y_wv_20_words_train, X_wv_20_words_test, y_rnn_test = pkl.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRU_BATCH_SIZE = 16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_wv_20_words_train_tensor = torch.FloatTensor(X_wv_20_words_train).to(device)\n",
    "y_wv_20_words_train_tensor = torch.LongTensor(y_wv_20_words_train).to(device)\n",
    "X_wv_20_words_test_tensor = torch.FloatTensor(X_wv_20_words_test).to(device)\n",
    "y_wv_20_words_test_tensor = torch.LongTensor(y_rnn_test).to(device)\n",
    "\n",
    "gru_train = TensorDataset(X_wv_20_words_train_tensor, y_wv_20_words_train_tensor)\n",
    "gru_train_loader = DataLoader(gru_train, batch_size=GRU_BATCH_SIZE, drop_last=True, shuffle=True)\n",
    "\n",
    "gru_test = TensorDataset(X_wv_20_words_test_tensor, y_wv_20_words_test_tensor)\n",
    "gru_test_loader = DataLoader(gru_test, batch_size=GRU_BATCH_SIZE, drop_last=True, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GRU(\n",
       "  (rnn): GRU(300, 20, batch_first=True)\n",
       "  (fc): Linear(in_features=20, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gru_model = GRU(n_inputs=300, n_outputs=5, n_hidden_states=20)\n",
    "gru_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "class_weights = torch.FloatTensor(\n",
    "    class_weight.compute_class_weight(\n",
    "        \"balanced\", classes=np.unique(y_wv_20_words_train), y=y_wv_20_words_train\n",
    "    )\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function and Optimizer\n",
    "criterion_gru = nn.CrossEntropyLoss(weight=class_weights).to(device)\n",
    "\n",
    "optimizer_gru = torch.optim.Adam(gru_model.parameters(), lr=16e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free Some Memory\n",
    "del X_wv_20_words_train, y_wv_20_words_train, X_wv_20_words_test, y_rnn_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.423860\n",
      "Epoch: 2 \tTraining Loss: 1.250046\n",
      "Epoch: 3 \tTraining Loss: 1.206539\n",
      "Epoch: 4 \tTraining Loss: 1.184161\n",
      "Epoch: 5 \tTraining Loss: 1.167547\n",
      "Epoch: 6 \tTraining Loss: 1.155109\n",
      "Epoch: 7 \tTraining Loss: 1.144839\n",
      "Epoch: 8 \tTraining Loss: 1.136778\n",
      "Epoch: 9 \tTraining Loss: 1.129797\n",
      "Epoch: 10 \tTraining Loss: 1.123304\n",
      "Epoch: 11 \tTraining Loss: 1.117963\n",
      "Epoch: 12 \tTraining Loss: 1.113396\n",
      "Epoch: 13 \tTraining Loss: 1.108805\n",
      "Epoch: 14 \tTraining Loss: 1.104589\n",
      "Epoch: 15 \tTraining Loss: 1.100702\n",
      "Epoch: 16 \tTraining Loss: 1.097047\n",
      "Epoch: 17 \tTraining Loss: 1.093883\n",
      "Epoch: 18 \tTraining Loss: 1.090560\n",
      "Epoch: 19 \tTraining Loss: 1.087260\n",
      "Epoch: 20 \tTraining Loss: 1.085175\n",
      "Epoch: 21 \tTraining Loss: 1.082275\n",
      "Epoch: 22 \tTraining Loss: 1.079674\n",
      "Epoch: 23 \tTraining Loss: 1.077481\n",
      "Epoch: 24 \tTraining Loss: 1.075076\n",
      "Epoch: 25 \tTraining Loss: 1.072198\n",
      "Epoch: 26 \tTraining Loss: 1.070574\n",
      "Epoch: 27 \tTraining Loss: 1.067724\n",
      "Epoch: 28 \tTraining Loss: 1.066097\n",
      "Epoch: 29 \tTraining Loss: 1.063814\n",
      "Epoch: 30 \tTraining Loss: 1.062477\n",
      "Epoch: 31 \tTraining Loss: 1.059752\n",
      "Epoch: 32 \tTraining Loss: 1.058247\n"
     ]
    }
   ],
   "source": [
    "gru_model = train_model(gru_train_loader, gru_model, n_epochs=32, optimizer=optimizer_gru, criterion=criterion_gru)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_gru_true, y_gru_pred = predict(gru_model, gru_test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.58      0.60      3996\n",
      "           1       0.42      0.36      0.39      3994\n",
      "           2       0.43      0.46      0.45      3997\n",
      "           3       0.46      0.45      0.45      3992\n",
      "           4       0.63      0.73      0.68      3989\n",
      "\n",
      "    accuracy                           0.52     19968\n",
      "   macro avg       0.51      0.52      0.51     19968\n",
      "weighted avg       0.51      0.52      0.51     19968\n",
      " \n",
      "Accuracy Score:  0.5161258012820513\n"
     ]
    }
   ],
   "source": [
    "# Print Metrics\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "print(classification_report(y_gru_true, y_gru_pred), \"\\nAccuracy Score: \", accuracy_score(y_gru_true, y_gru_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model\n",
    "torch.save(gru_model.state_dict(), f\"{MODEL_PATH}/{GRU_TOP_20_WV_MODEL_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "del (\n",
    "    X_wv_20_words_train_tensor,\n",
    "    y_wv_20_words_train_tensor,\n",
    "    X_wv_20_words_test_tensor,\n",
    "    y_wv_20_words_test_tensor,\n",
    "    gru_train,\n",
    "    gru_train_loader,\n",
    "    gru_test,\n",
    "    gru_test_loader,\n",
    "    gru_model,\n",
    "    optimizer_gru,\n",
    "    criterion_gru,\n",
    "    y_gru_true,\n",
    "    y_gru_pred,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you conclude by comparing accuracy values you obtain with those obtained using simple RNN?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Answer\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "e78917e7f90f3892e7e12462ef46781cf5994bd706032ea53be00d0b1f29dcb9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
